(Files content cropped to 300k characters)

================================================
FILE: README.adoc
================================================

= The Internet Computer Protocol (ICP)
:toc: macro


ifdef::env-github[]
++++
<p align="center">
  <img width="800" src="/readme-icp3.png">
</p>
++++
endif::[]


The Internet Computer is the world’s first blockchain that runs at web speed and can increase its capacity without bound. Like the Internet (which is composed of many machines adhering to TCP/IP protocol) and blockchain protocols (such as Bitcoin and Ethereum).

:toc-title:
toc::[]

== Resources on the Internet Computer

=== R&D documentation

You can learn more about the Internet Computer's Protocol, features, and designs here, here are some helpful resources:

Protocol Documentation:

- https://medium.com/dfinity/a-technical-overview-of-the-internet-computer-f57c62abc20f[A Technical Overview of the Internet Computer (blog post)]
- https://medium.com/dfinity/software-canisters-an-evolution-of-smart-contracts-internet-computer-f1f92f1bfffb[Canisters, an Evolution of Smart Contracts]
- https://medium.com/dfinity/applied-crypto-one-public-key-for-the-internet-computer-ni-dkg-4af800db869d[Noninteractive Distributed Key Generation]
- https://medium.com/dfinity/the-internet-computers-token-economics-an-overview-29e238bd1d83[The Internet Computer’s Token Economics: An Overview]
- https://medium.com/dfinity/understanding-the-internet-computers-network-nervous-system-neurons-and-icp-utility-tokens-730dab65cae8[Understanding the Internet Computer’s Network Nervous System, Neurons, and ICP Utility Tokens]
- https://github.com/dfinity/nns-proposals[List of NNS Proposals]
- https://medium.com/dfinity/achieving-consensus-on-the-internet-computer-ee9fbfbafcbc[Consensus protocol]

Engineering

- https://sdk.dfinity.org/docs/developers-guide/concepts/what-is-ic[What is the Internet Computer?]
- https://sdk.dfinity.org/docs/quickstart/quickstart-intro.html[Tutorials, SDKs, and sample apps to get started]
- https://docs.dfinity.org/[Rust Cargo docs for the replica]




=== ICP Dashboard

You can observe the state of the Internet Computer's infrastructure (Nodes, data centers, subnets) and traditional blockchain metrics (blocks/second, Token Supply, etc)

- https://dashboard.internetcomputer.org

=== The community

To interact with the community, check out the developer forum: https://forum.dfinity.org/

=== Rust implementation of the ICP

This repo contains many different pieces (including testing and other infrastructure components), but the most important one is the source code for the Rust implementation of the "*replica*" (read: "client" in some blockchains) that is compiled and run by the machines that together make up the Internet Computer.

=== DFINITY Foundation

The https://dfinity.org/[DFINITY Foundation] is a Swiss not-for-profit organization based in Zurich, Switzerland, which oversees research centers in Palo Alto, San Francisco, and Zurich. Its goal is to further the design, development, and adoption of the Internet Computer Protocol.

== Getting Started

=== Who should be using this code

- *If you are an app developer*, and your intent is to build apps so you want a local Internet Computer replica in your machine to deploy to, you are better off using the https://sdk.dfinity.org/docs/quickstart/quickstart-intro.html[Canister SDK] written by the DFINITY Foundation. It is optimized for this and much more lightweight (less than 2 minutes to get started). It will build and run a local replica and you do not need to get into systems code to run it.

- *If you are a blockchain enthusiast*, and your intent is to understand the protocol (not an implementation), you may be better off going to the https://medium.com/dfinity/achieving-consensus-on-the-internet-computer-ee9fbfbafcbc[Consensus protocol] and https://sdk.dfinity.org/docs/interface-spec/index.html[IC Interface Specification]. This content (by the DFINITY research team) is tailor made for understanding the protocol and design.

- *If you are a blockchain miner*, you should know that the Internet Computer Protocol (while it is a blockchain) does not have the traditional mining or validating you may come to expect from blockchain projects. The Internet Computer Protocol is designed using new and novel cryptography that does not require "mining"... but it does require independent node providers, which may include yourself. You can of course check out the source code in this repo, but a better resource may be this: https://wiki.internetcomputer.org/wiki/Internet_Computer_wiki#For_Node_Providers[Internet Computer Wiki - For Node Providers]

- *If you are an engineer looking to build a new SDK, oracle, wallet or any part that enables and improves the Internet Computer ecosystem*, you should take a look at the https://sdk.dfinity.org/docs/interface-spec/index.html[Interface Specification] which is for low-level interaction with the Internet Computer.

- *If you are a systems engineer, security engineer or cryptographer*, and your intent is to see what is going on under the hood by digging through source and building this locally, *_then you are in the right place_*.

== Building the code

*System requirements*

- x86-64 based system (minimum: 16 GB MEM/SWAP, 100 GB available disk space)
- Ubuntu 22.04 or newer
- https://podman.io/getting-started/installation[Podman]

For detailed information on building IC-OS images, please refer to the link:ic-os/README.adoc[IC-OS README]

Alternatively, to build all IC-OS images using a simple, containerized environment, run:

  $ ./ci/container/build-ic.sh -i

To build only the binaries and canisters, use the `-b` and/or `-c` flags:

  $ ./ci/container/build-ic.sh -b -c

All built artifacts will be located in the top-level artifacts/ directory.

== Verifying Releases

=== Overview

The Internet Computer provides a robust system for verifying the build reproducibility of IC-OS update images. Each https://dashboard.internetcomputer.org/releases[release proposal] includes detailed verification instructions.

=== Prerequisites

* Ubuntu 22.04 or higher
* Python 3.x
* curl (`sudo apt install curl`)
* About 100 GB of free disk space in `$HOME` (or another volume, specifed via
  option `--cache-dir`)

=== Quick Start

To verify an IC OS Version Election proposal:

IMPORTANT: Always use versioned repro-check URLs (replace `{COMMIT_ID}` with the actual commit hash) instead of `master` to ensure compatibility between the repro-check script and the build system for that specific commit.

[source,bash]
----
# Verify by proposal number
curl -fsSL https://raw.githubusercontent.com/dfinity/ic/{COMMIT_ID}/ci/scripts/repro-check | \
    python3 - -p <proposal_number>

# Verify by git commit
curl -fsSL https://raw.githubusercontent.com/dfinity/ic/{COMMIT_ID}/ci/scripts/repro-check | \
    python3 - -c {COMMIT_ID}
----

=== Component-Specific Verification

You can verify specific components individually:

[source,bash]
----
# Verify GuestOS only
curl -fsSL https://raw.githubusercontent.com/dfinity/ic/{COMMIT_ID}/ci/scripts/repro-check | \
    python3 - -c {COMMIT_ID} --guestos

# Verify HostOS only
curl -fsSL https://raw.githubusercontent.com/dfinity/ic/{COMMIT_ID}/ci/scripts/repro-check | \
    python3 - -c {COMMIT_ID} --hostos

# Verify SetupOS only
curl -fsSL https://raw.githubusercontent.com/dfinity/ic/{COMMIT_ID}/ci/scripts/repro-check | \
    python3 - -c {COMMIT_ID} --setupos
----

== Contributing

=== The Network Nervous System
Thank you for taking the time to learn more about the Internet Computer Protocol. You can contribute to either, but it is important to note that the Internet Computer is governed by a decentralized system called the Network Nervous System (NNS). You can learn more here:

- https://medium.com/dfinity/understanding-the-internet-computers-network-nervous-system-neurons-and-icp-utility-tokens-730dab65cae8[Understanding the Internet Computer’s Network Nervous System, Neurons, and ICP Utility Tokens]
- https://github.com/dfinity/nns-proposals[List of NNS Proposals]

=== Open Source Policy

The DFINITY Foundation makes the code of the Internet Computer available to
the public.

This is important so that the community can review the code that defines the
behaviour of the Internet Computer. Furthermore, the community will be able to build the code
and verify that it derives from the same binary image that is referenced in
upgrade proposals published via the Network Nervous System (NNS).

All code of the Internet Computer is be licensed under the Apache 2.0 license, except for a
few components licensed under the link:licenses/IC-1.0.txt[Internet Computer Community Source License] and link:licenses/IC-shared-1.0.txt[Internet Computer Shared Community Source License] which are more restrictive than the Apache 2.0 license to protect the Intellectual Property (IP) of the DFINITY Foundation.

While we adapt our development processes and security reviews for a world of developing with our code in the open, we are not accepting any pull requests at this time. For now, please join our developer community at https://forum.dfinity.org. If you discover any bugs and vulnerabilities, please follow the procedure at https://dfinity.org/vulnerability-disclosure-program/.

=== Rust Dependency Policy

To make the mono repository a success, there needs to be some basic rules to make development faster.

* When adding a new external crate dependency please make sure it is necessary. Check that
** There isn't another already imported crate with similar functionality.
** The crate is well maintained and comes from reputable authors.
* When bumping the semantic version of an external crate, please do it for the whole repository. Avoid importing the same crate with multiple versions.
* Keep the rust-lang up-to-date for Bazel and Cargo.
* Use Cargo workspace for inferring external crate versions by adding the new crate to the section `[workspace.dependencies]` of the workspace `Cargo.toml` and adding `new-crate = { workspace = true }` to each package-specific `Cargo.toml` that needs it.



================================================
FILE: buf.yaml
================================================
version: v1beta1
build:
  roots:
    - rs/bitcoin/service/proto
    - rs/crypto/internal/crypto_service_provider/proto
    - rs/https_outcalls/service/proto
    - rs/monitoring/adapter_metrics_service/proto
    - rs/nervous_system/proto/proto
    - rs/nns/common/proto
    - rs/nns/gtc/proto Temporarily removed because a PR was reverted and then un-reverted
    - rs/nns/handlers/root/impl/proto
    - rs/nns/governance/proto
    - rs/protobuf/def
    - rs/ledger_suite/icp/proto
    - rs/sns/governance/proto
    - rs/sns/root/proto
    - rs/sns/swap/proto
    - rs/types/base_types/proto
lint:
  use:
    - DEFAULT
breaking:
  ignore:
  use:
    - WIRE
  except:
    # Allow renaming of optional scalar types (`buf` implements these as the single
    # member of a `oneof` whose name is derived from the field name). It will also
    # allow silently moving a field from one `oneof` to another, but this is highly
    # unlikely to happen (for starters, it requires two `oneofs` in a message).
    - FIELD_SAME_ONEOF



================================================
FILE: BUILD.bazel
================================================
load("@bazel_skylib//rules:common_settings.bzl", "string_setting")
load("@gazelle//:def.bzl", "gazelle")
load("@rules_python//python:pip.bzl", "compile_pip_requirements")
load("//ci/src/artifacts:upload.bzl", "upload_artifacts")

package(default_visibility = ["//visibility:public"])

# WARNING! .git is the directory, not a regular file! only consume it in your rules if you know how exactly bazel works and understand implications!
exports_files([
    ".git",
    "buf.yaml",
    "clippy.toml",
    "rustfmt.toml",
    "WORKSPACE.bazel",
    "mainnet-canister-revisions.json",
    "mainnet-icos-images.bzl",
])

alias(
    name = "buildifier",
    actual = "//bazel:buildifier",
)

alias(
    name = "ruff-format",
    actual = "//pre-commit:ruff-format",
)

alias(
    name = "protobuf-format",
    actual = "//pre-commit:protobuf-format",
)

alias(
    name = "shfmt-format",
    actual = "//pre-commit:shfmt-format",
)

alias(
    name = "rustfmt",
    actual = "@rules_rust//:rustfmt",
)

alias(
    name = "ormolu-format",
    actual = "//pre-commit:ormolu-format",
)

alias(
    name = "gen_rust_project",
    actual = "@rules_rust//tools/rust_analyzer:gen_rust_project",
)

# See https://github.com/bazelbuild/bazel-gazelle#running-gazelle-with-bazel
# gazelle:prefix github.com/dfinity/ic
# gazelle:proto disable
gazelle(
    name = "gazelle",
)

gazelle(
    name = "gazelle-update-repos",
    args = [
        "-from_file=go.mod",
        "-to_macro=go_deps.bzl%go_dependencies",
        "-prune",
    ],
    command = "update-repos",
)

alias(
    name = "gobin",
    actual = "@rules_go//go",
    visibility = ["//visibility:public"],
)

# Builds python dependencies. To update the lockfile:
# $ bazel run //:python-requirements.update
compile_pip_requirements(
    name = "python-requirements",
    timeout = "moderate",
    src = "requirements.in",
    requirements_txt = "requirements.txt",
)

test_suite(
    name = "single_large_node",  # the "_test" postfix is dropped on purpose since this target is meant for interactive use.
    tags = ["manual"],
    tests = ["//rs/tests/testnets:single_large_node"],
)

### Bitcoind

alias(
    name = "bitcoind",
    actual = select({
        "@bazel_tools//src/conditions:darwin_arm64": "@bitcoin_core_darwin_arm64//:bitcoind",
        "@bazel_tools//src/conditions:darwin_x86_64": "@bitcoin_core_darwin_x86//:bitcoind",
        "@bazel_tools//src/conditions:linux_aarch64": "@bitcoin_core_linux_aarch64//:bitcoind",
        "@bazel_tools//src/conditions:linux_x86_64": "@bitcoin_core_linux_x86//:bitcoind",
    }),
)

genrule(
    name = "dogecoind_stub",
    outs = ["dogecoind_stub.txt"],
    cmd = "echo 'Not supported on this platform.' > $@",
)

alias(
    name = "dogecoind",
    actual = select({
        "@bazel_tools//src/conditions:linux_x86_64": "@dogecoin_core_linux_x86//:dogecoind",
        "//conditions:default": "//:dogecoind_stub",
    }),
)

### rclone, used for uploading artifacts to S3-compatible storage

alias(
    name = "rclone",
    actual = select({
        "@platforms//os:osx": "@rclone-x86_64-darwin//:rclone",  # No arm64 build for this version, use amd64 for now.
        "@bazel_tools//src/conditions:linux_x86_64": "@rclone-x86_64-linux//:rclone",
        "//conditions:default": "@platforms//:incompatible",
    }),
)

### Pocket IC

# The pocket-ic server binary. Use this as a test dependency if the test
# does not require a specific pocket-ic version (see ":pocket-ic-server-variant"
# for details).
# By default returns the pocket-ic server from the source tree to ensure
# consistency within the source tree. See 'pocket_ic_mainnet_test' for
# overrides.
alias(
    name = "pocket-ic-server",
    actual = select({
        ":pocket-ic-server-variant-mainnet": "//:pocket-ic-mainnet",
        ":pocket-ic-server-variant-head": "//rs/pocket_ic_server:pocket-ic-server",
        "//conditions:default": "//rs/pocket_ic_server:pocket-ic-server",
    }),
)

# A setting to switch between different variants of pocket-ic. The
# default pocket-ic variant/version (head) is the one as in the
# source tree.
string_setting(
    name = "pocket-ic-server-variant",
    build_setting_default = "head",
    visibility = ["//visibility:public"],
)

config_setting(
    name = "pocket-ic-server-variant-head",
    flag_values = {
        ":pocket-ic-server-variant": "head",
    },
)

# A "mainnet" variant of the pocket-ic server which represents a
# released version of pocket-ic.
config_setting(
    name = "pocket-ic-server-variant-mainnet",
    flag_values = {
        ":pocket-ic-server-variant": "mainnet",
    },
)

# The pocket-ic as released; use this for tests that need to ensure consistency
# with a release pocket-ic/replica.
genrule(
    name = "pocket-ic-mainnet",
    srcs = ["@pocket-ic-mainnet-gz//file"],
    outs = ["pocket-ic"],
    cmd = "gunzip -c $< > $@",
)

# Upload artifacts to the CDN
upload_artifacts(
    name = "upload-artifacts",
    testonly = True,
    inputs = [
        "//publish/binaries:bundle",
    ] + select({
        "@platforms//os:osx": [],
        "//conditions:default": [
            "//publish/binaries:bundle-legacy",  # avoid overwriting legacy artifacts
            "//publish/canisters:bundle",
            "//ic-os/guestos/envs/dev:bundle-disk",  # used by icos_deploy testnet script
            "//ic-os/guestos/envs/prod:bundle-update",
            "//ic-os/guestos/envs/dev:bundle-update",  # used by nested tests
            "//ic-os/hostos/envs/prod:bundle-update",
            "//ic-os/hostos/envs/dev:bundle-update",  # used by nested tests
            "//ic-os/setupos/envs/prod:bundle",
            "//ic-os/setupos/envs/dev:bundle",  # used by nested tests
            "//ic-os/guestos/envs/recovery:bundle-update",  # used for manual recovery
        ],
    }),
    visibility = ["//visibility:public"],
)

# Artifact uploader that can be run with `bazel run ... -- BUNDLE...`
upload_artifacts(
    name = "artifact-uploader",
)

# Test suite for super-fast tests (should only take a couple seconds to run).
test_suite(
    name = "insta_tests",
    tests = [
        "//bazel:buildifier_test",
        "//bazel:gazelle_test",
    ],
)



================================================
FILE: Cargo.toml
================================================
[workspace]

members = [
    "packages/canlog",
    "packages/canlog_derive",
    "packages/ic-dummy-getrandom-for-wasm",
    "packages/ic-ed25519",
    "packages/ic-error-types",
    "packages/ic-ethereum-types",
    "packages/ic-heap-bytes",
    "packages/ic-heap-bytes-derive",
    "packages/ic-hpke",
    "packages/ic-http-types",
    "packages/ic-ledger-hash-of",
    "packages/ic-metrics-assert",
    "packages/ic-pub-key",
    "packages/icrc-cbor",
    "packages/icrc-ledger-agent",
    "packages/icrc-ledger-client",
    "packages/icrc-ledger-client-cdk",
    "packages/icrc-ledger-types",
    "packages/ic-secp256k1",
    "packages/ic-secp256r1",
    "packages/ic-sha3",
    "packages/ic-signature-verification",
    "packages/pocket-ic",
    "packages/pocket-ic/test_canister",
    "rs/artifact_pool",
    "rs/backup",
    "rs/bitcoin/adapter",
    "rs/bitcoin/adapter/test_utils",
    "rs/bitcoin/checker",
    "rs/bitcoin/ckbtc/agent",
    "rs/bitcoin/ckbtc/minter",
    "rs/bitcoin/client",
    "rs/bitcoin/consensus",
    "rs/bitcoin/mock",
    "rs/bitcoin/replica_types",
    "rs/bitcoin/service",
    "rs/bitcoin/validation",
    "rs/boundary_node/canary_proxy",
    "rs/boundary_node/certificate_issuance/certificate_issuer",
    "rs/boundary_node/certificate_issuance/certificate_orchestrator",
    "rs/boundary_node/certificate_issuance/certificate_orchestrator_interface",
    "rs/boundary_node/certificate_issuance/create_acme_account",
    "rs/boundary_node/ic_boundary",
    "rs/boundary_node/rate_limits",
    "rs/boundary_node/rate_limits/api",
    "rs/boundary_node/rate_limits/canister_client",
    "rs/boundary_node/rate_limits/integration_tests",
    "rs/boundary_node/salt_sharing",
    "rs/boundary_node/salt_sharing/api",
    "rs/boundary_node/salt_sharing/integration_tests",
    "rs/boundary_node/systemd_journal_gatewayd_shim",
    "rs/canister_client",
    "rs/canister_client/read_state_response_parser",
    "rs/canister_client/sender",
    "rs/canister_sandbox",
    "rs/canonical_state",
    "rs/canonical_state/certification_version",
    "rs/canonical_state/tree_hash",
    "rs/canonical_state/tree_hash/test_utils",
    "rs/certification",
    "rs/certification/test-utils",
    "rs/config",
    "rs/consensus",
    "rs/consensus/certification",
    "rs/consensus/cup_utils",
    "rs/consensus/dkg",
    "rs/consensus/idkg",
    "rs/consensus/mocks",
    "rs/consensus/utils",
    "rs/consensus/vetkd",
    "rs/criterion_time",
    "rs/cross-chain/proposal-cli",
    "rs/crypto",
    "rs/crypto/for_verification_only",
    "rs/crypto/iccsa",
    "rs/crypto/interfaces/sig_verification",
    "rs/crypto/internal/crypto_lib/basic_sig/cose",
    "rs/crypto/internal/crypto_lib/basic_sig/der_utils",
    "rs/crypto/internal/crypto_lib/basic_sig/ecdsa_secp256k1",
    "rs/crypto/internal/crypto_lib/basic_sig/ecdsa_secp256r1",
    "rs/crypto/internal/crypto_lib/basic_sig/ed25519",
    "rs/crypto/internal/crypto_lib/basic_sig/iccsa",
    "rs/crypto/internal/crypto_lib/basic_sig/iccsa/test_utils",
    "rs/crypto/internal/crypto_lib/basic_sig/rsa_pkcs1",
    "rs/crypto/internal/crypto_lib/bls12_381/type",
    "rs/crypto/internal/crypto_lib/bls12_381/vetkd",
    "rs/crypto/internal/crypto_lib/hmac",
    "rs/crypto/internal/crypto_lib/multi_sig/bls12_381",
    "rs/crypto/internal/crypto_lib/seed",
    "rs/crypto/internal/crypto_lib/sha2",
    "rs/crypto/internal/crypto_lib/threshold_sig/bls12_381",
    "rs/crypto/internal/crypto_lib/threshold_sig/canister_threshold_sig",
    "rs/crypto/internal/crypto_lib/threshold_sig/canister_threshold_sig/fe-derive",
    "rs/crypto/internal/crypto_lib/threshold_sig/canister_threshold_sig/test_utils",
    "rs/crypto/internal/crypto_lib/tls",
    "rs/crypto/internal/crypto_lib/types",
    "rs/crypto/internal/crypto_service_provider",
    "rs/crypto/internal/crypto_service_provider/csp_proptest_utils",
    "rs/crypto/internal/crypto_service_provider/protobuf_generator",
    "rs/crypto/internal/csp_test_utils",
    "rs/crypto/internal/logmon",
    "rs/crypto/internal/test_vectors",
    "rs/crypto/node_key_generation",
    "rs/crypto/node_key_validation",
    "rs/crypto/node_key_validation/tls_cert_validation",
    "rs/crypto/prng",
    "rs/crypto/secrets_containers",
    "rs/crypto/sha2",
    "rs/crypto/standalone-sig-verifier",
    "rs/crypto/temp_crypto",
    "rs/crypto/temp_crypto/temp_vault",
    "rs/crypto/test_utils",
    "rs/crypto/test_utils/canister_sigs",
    "rs/crypto/test_utils/canister_threshold_sigs",
    "rs/crypto/test_utils/crypto_returning_ok",
    "rs/crypto/test_utils/csp",
    "rs/crypto/test_utils/keygen",
    "rs/crypto/test_utils/keys",
    "rs/crypto/test_utils/local_csp_vault",
    "rs/crypto/test_utils/metrics",
    "rs/crypto/test_utils/multi_sigs",
    "rs/crypto/test_utils/ni-dkg",
    "rs/crypto/test_utils/reproducible_rng",
    "rs/crypto/test_utils/root_of_trust",
    "rs/crypto/test_utils/tls",
    "rs/crypto/test_utils/vetkd",
    "rs/crypto/tls_interfaces",
    "rs/crypto/tls_interfaces/mocks",
    "rs/crypto/tree_hash",
    "rs/crypto/tree_hash/test_utils",
    "rs/crypto/utils/basic_sig",
    "rs/crypto/utils/canister_threshold_sig",
    "rs/crypto/utils/ni_dkg",
    "rs/crypto/utils/threshold_sig",
    "rs/crypto/utils/threshold_sig_der",
    "rs/crypto/utils/tls",
    "rs/cup_explorer",
    "rs/cycles_account_manager",
    "rs/depcheck",
    "rs/determinism_test",
    "rs/dogecoin/ckdoge/minter",
    "rs/embedders",
    "rs/embedders/benches/embedders_bench",
    "rs/ethereum/cketh/minter",
    "rs/ethereum/cketh/test_utils",
    "rs/ethereum/evm-rpc-client",
    "rs/ethereum/ledger-suite-orchestrator",
    "rs/ethereum/ledger-suite-orchestrator/test_utils",
    "rs/execution_environment",
    "rs/execution_environment/benches/lib",
    "rs/execution_environment/benches/management_canister/test_canister",
    "rs/execution_environment/tools",
    "rs/http_endpoints/async_utils",
    "rs/http_endpoints/metrics",
    "rs/http_endpoints/nns_delegation_manager",
    "rs/http_endpoints/public",
    "rs/http_endpoints/test_agent",
    "rs/http_endpoints/xnet",
    "rs/https_outcalls/adapter",
    "rs/https_outcalls/client",
    "rs/https_outcalls/consensus",
    "rs/https_outcalls/service",
    "rs/http_utils",
    "rs/ic_os/attestation",
    "rs/ic_os/attestation/testing",
    "rs/ic_os/build_tools/dflate",
    "rs/ic_os/build_tools/diroid",
    "rs/ic_os/build_tools/inject_files",
    "rs/ic_os/build_tools/partition_tools",
    "rs/ic_os/config",
    "rs/ic_os/config_types",
    "rs/ic_os/config_types/compatibility_tests",
    "rs/ic_os/deterministic_ips",
    "rs/ic_os/device",
    "rs/ic_os/dev_test_tools/setupos-disable-checks",
    "rs/ic_os/dev_test_tools/setupos-image-config",
    "rs/ic_os/fstrim_tool",
    "rs/ic_os/grub",
    "rs/ic_os/guest_upgrade/client",
    "rs/ic_os/guest_upgrade/server",
    "rs/ic_os/guest_upgrade/shared",
    "rs/ic_os/guest_upgrade/tests",
    "rs/ic_os/linux_kernel_command_line",
    "rs/ic_os/metrics_tool",
    "rs/ic_os/network",
    "rs/ic_os/nft_exporter",
    "rs/ic_os/nss_icos",
    "rs/ic_os/os_tools/guest_disk",
    "rs/ic_os/os_tools/guestos_tool",
    "rs/ic_os/os_tools/guest_vm_runner",
    "rs/ic_os/os_tools/hostos_tool",
    "rs/ic_os/os_tools/setupos_tool",
    "rs/ic_os/sev",
    "rs/ic_os/utils",
    "rs/ic_os/vsock/guest",
    "rs/ic_os/vsock/host",
    "rs/ic_os/vsock/vsock_lib",
    "rs/ingress_manager",
    "rs/interfaces",
    "rs/interfaces/adapter_client",
    "rs/interfaces/certified_stream_store",
    "rs/interfaces/certified_stream_store/mocks",
    "rs/interfaces/mocks",
    "rs/interfaces/registry",
    "rs/interfaces/registry/mocks",
    "rs/interfaces/state_manager",
    "rs/interfaces/state_manager/mocks",
    "rs/ledger_suite/common/ledger_canister_core",
    "rs/ledger_suite/common/ledger_core",
    "rs/ledger_suite/icp",
    "rs/ledger_suite/icp/archive",
    "rs/ledger_suite/icp/index",
    "rs/ledger_suite/icp/ledger",
    "rs/ledger_suite/icp/protobuf_generator",
    "rs/ledger_suite/icp/test_utils",
    "rs/ledger_suite/icrc1",
    "rs/ledger_suite/icrc1/archive",
    "rs/ledger_suite/icrc1/index-ng",
    "rs/ledger_suite/icrc1/ledger",
    "rs/ledger_suite/icrc1/test_utils",
    "rs/ledger_suite/icrc1/test_utils/icrc3_test_ledger",
    "rs/ledger_suite/icrc1/tokens_u256",
    "rs/ledger_suite/icrc1/tokens_u64",
    "rs/ledger_suite/tests/sm-tests",
    "rs/ledger_suite/tests/sm-tests/constants",
    "rs/ledger_suite/test_utils/in_memory_ledger",
    "rs/ledger_suite/test_utils/state_machine_helpers",
    "rs/limits",
    "rs/memory_tracker",
    "rs/messaging",
    "rs/migration_canister",
    "rs/monitoring/adapter_metrics/client",
    "rs/monitoring/adapter_metrics/server",
    "rs/monitoring/adapter_metrics/service",
    "rs/monitoring/logger",
    "rs/monitoring/metrics",
    "rs/monitoring/pprof",
    "rs/monitoring/tracing",
    "rs/monitoring/tracing/jaeger_exporter",
    "rs/monitoring/tracing/logging_layer",
    "rs/nervous_system/agent",
    "rs/nervous_system/candid_utils",
    "rs/nervous_system/canisters",
    "rs/nervous_system/chunks",
    "rs/nervous_system/clients",
    "rs/nervous_system/collections/union_multi_map",
    "rs/nervous_system/common",
    "rs/nervous_system/common/build_metadata",
    "rs/nervous_system/common/test_canister",
    "rs/nervous_system/common/test_keys",
    "rs/nervous_system/common/test_utils",
    "rs/nervous_system/common/validation",
    "rs/nervous_system/governance",
    "rs/nervous_system/histogram",
    "rs/nervous_system/humanize",
    "rs/nervous_system/initial_supply",
    "rs/nervous_system/instruction_stats",
    "rs/nervous_system/instruction_stats_update_attribute",
    "rs/nervous_system/integration_tests",
    "rs/nervous_system/linear_map",
    "rs/nervous_system/lock",
    "rs/nervous_system/long_message",
    "rs/nervous_system/neurons_fund",
    "rs/nervous_system/neurons_fund/nfplot",
    "rs/nervous_system/proto",
    "rs/nervous_system/proto/protobuf_generator",
    "rs/nervous_system/proxied_canister_calls_tracker",
    "rs/nervous_system/rate_limits",
    "rs/nervous_system/root",
    "rs/nervous_system/runtime",
    "rs/nervous_system/string",
    "rs/nervous_system/temporary",
    "rs/nervous_system/time_helpers",
    "rs/nervous_system/timers",
    "rs/nervous_system/timer_task",
    "rs/nervous_system/timestamp",
    "rs/nervous_system/tools/neuron-subaccount",
    "rs/nervous_system/tools/release-runscript",
    "rs/nervous_system/tools/submit-motion-proposal",
    "rs/nervous_system/tools/sync-with-released-nervous-system-wasms",
    "rs/nns/cmc",
    "rs/nns/common",
    "rs/nns/common/protobuf_generator",
    "rs/nns/constants",
    "rs/nns/governance",
    "rs/nns/governance/api",
    "rs/nns/governance/init",
    "rs/nns/governance/protobuf_generator",
    "rs/nns/gtc",
    "rs/nns/gtc_accounts",
    "rs/nns/gtc/protobuf_generator",
    "rs/nns/handlers/lifeline/impl",
    "rs/nns/handlers/lifeline/interface",
    "rs/nns/handlers/root/impl",
    "rs/nns/handlers/root/impl/protobuf_generator",
    "rs/nns/handlers/root/interface",
    "rs/nns/identity",
    "rs/nns/init",
    "rs/nns/integration_tests",
    "rs/nns/nns-ui",
    "rs/nns/sns-wasm",
    "rs/nns/sns-wasm/protobuf_generator",
    "rs/nns/test_utils",
    "rs/nns/test_utils/golden_nns_state",
    "rs/nns/test_utils_macros",
    "rs/nns/test_utils/prepare_golden_state",
    "rs/node_rewards/canister",
    "rs/node_rewards/canister/api",
    "rs/node_rewards/canister/protobuf_generator",
    "rs/node_rewards/rewards_calculation",
    "rs/orchestrator",
    "rs/orchestrator/dashboard",
    "rs/orchestrator/image_upgrader",
    "rs/orchestrator/registry_replicator",
    "rs/p2p/artifact_downloader",
    "rs/p2p/artifact_manager",
    "rs/p2p/consensus_manager",
    "rs/p2p/memory_transport",
    "rs/p2p/peer_manager",
    "rs/p2p/quic_transport",
    "rs/p2p/state_sync_manager",
    "rs/p2p/test_utils",
    "rs/phantom_newtype",
    "rs/pocket_ic_server",
    "rs/prep",
    "rs/protobuf",
    "rs/protobuf/generator",
    "rs/query_stats",
    "rs/recovery",
    "rs/recovery/subnet_splitting",
    "rs/registry/admin",
    "rs/registry/admin-derive",
    "rs/registry/canister",
    "rs/registry/canister/api",
    "rs/registry/canister/chunkify",
    "rs/registry/canister-client",
    "rs/registry/canister/protobuf_generator",
    "rs/registry/client",
    "rs/registry/fake",
    "rs/registry/fetch_large_record_test_canister",
    "rs/registry/helpers",
    "rs/registry/keys",
    "rs/registry/local_registry",
    "rs/registry/local_store",
    "rs/registry/local_store/artifacts",
    "rs/registry/nns_data_provider",
    "rs/registry/nns_data_provider_wrappers",
    "rs/registry/node_provider_rewards",
    "rs/registry/proto",
    "rs/registry/proto_data_provider",
    "rs/registry/proto/generator",
    "rs/registry/provisional_whitelist",
    "rs/registry/regedit",
    "rs/registry/routing_table",
    "rs/registry/subnet_features",
    "rs/registry/subnet_type",
    "rs/registry/transport",
    "rs/registry/transport/protobuf_generator",
    "rs/replay",
    "rs/replica",
    "rs/replica/setup_ic_network",
    "rs/replicated_state",
    "rs/replica_tests",
    "rs/rosetta-api/common/rosetta_core",
    "rs/rosetta-api/icp",
    "rs/rosetta-api/icp/client",
    "rs/rosetta-api/icp/ledger_canister_blocks_synchronizer",
    "rs/rosetta-api/icp/ledger_canister_blocks_synchronizer/test_utils",
    "rs/rosetta-api/icp/runner",
    "rs/rosetta-api/icp/tests/integration_tests",
    "rs/rosetta-api/icp/test_utils",
    "rs/rosetta-api/icp/test_utils/sender_canister",
    "rs/rosetta-api/icrc1",
    "rs/rosetta-api/icrc1/client",
    "rs/rosetta-api/icrc1/runner",
    "rs/rust_canisters/backtrace_canister",
    "rs/rust_canisters/call_loop_canister",
    "rs/rust_canisters/call_tree_test",
    "rs/rust_canisters/canister_creator",
    "rs/rust_canisters/canister_log",
    "rs/rust_canisters/canister_profiler",
    "rs/rust_canisters/canister_serve",
    "rs/rust_canisters/canister_test",
    "rs/rust_canisters/dfn_candid",
    "rs/rust_canisters/dfn_core",
    "rs/rust_canisters/dfn_http",
    "rs/rust_canisters/dfn_http_metrics",
    "rs/rust_canisters/dfn_json",
    "rs/rust_canisters/dfn_protobuf",
    "rs/rust_canisters/downstream_calls_test",
    "rs/rust_canisters/ecdsa",
    "rs/rust_canisters/load_simulator",
    "rs/rust_canisters/memory_test",
    "rs/rust_canisters/on_wire",
    "rs/rust_canisters/proxy_canister",
    "rs/rust_canisters/random_traffic_test",
    "rs/rust_canisters/response_payload_test",
    "rs/rust_canisters/stable_memory_integrity",
    "rs/rust_canisters/stable_reader",
    "rs/rust_canisters/stable_structures",
    "rs/rust_canisters/statesync_test",
    "rs/rust_canisters/tests",
    "rs/rust_canisters/xnet_test",
    "rs/rust_canisters/xrc_mock",
    "rs/sns/audit",
    "rs/sns/cli",
    "rs/sns/governance",
    "rs/sns/governance/api",
    "rs/sns/governance/api_helpers",
    "rs/sns/governance/proposal_criticality",
    "rs/sns/governance/proposals_amount_total_limit",
    "rs/sns/governance/protobuf_generator",
    "rs/sns/governance/token_valuation",
    "rs/sns/init",
    "rs/sns/init/protobuf_generator",
    "rs/sns/integration_tests",
    "rs/sns/root",
    "rs/sns/root/protobuf_generator",
    "rs/sns/swap",
    "rs/sns/swap/protobuf_generator",
    "rs/sns/swap/proto_library",
    "rs/sns/testing",
    "rs/sns/test_utils",
    "rs/sns/treasury_manager",
    "rs/sns/treasury_manager/mock",
    "rs/state_layout",
    "rs/state_machine_tests",
    "rs/state_manager",
    "rs/state_tool",
    "rs/sys",
    "rs/tests/boundary_nodes",
    "rs/tests/boundary_nodes/integration_test_common",
    "rs/tests/boundary_nodes/performance_test_common",
    "rs/tests/boundary_nodes/utils",
    "rs/tests/ckbtc",
    "rs/tests/consensus",
    "rs/tests/consensus/backup",
    "rs/tests/consensus/catch_up_test_common",
    "rs/tests/consensus/liveness_test_common",
    "rs/tests/consensus/orchestrator",
    "rs/tests/consensus/subnet_recovery",
    "rs/tests/consensus/tecdsa",
    "rs/tests/consensus/tecdsa/utils",
    "rs/tests/consensus/upgrade",
    "rs/tests/consensus/utils",
    "rs/tests/consensus/vetkd",
    "rs/tests/cross_chain",
    "rs/tests/crypto",
    "rs/tests/dre",
    "rs/tests/dre/utils",
    "rs/tests/driver",
    "rs/tests/execution",
    "rs/tests/financial_integrations",
    "rs/tests/financial_integrations/rosetta",
    "rs/tests/financial_integrations/rosetta/rosetta_test_lib",
    "rs/tests/httpbin-rs",
    "rs/tests/idx",
    "rs/tests/message_routing",
    "rs/tests/message_routing/common",
    "rs/tests/message_routing/rejoin_test_lib",
    "rs/tests/message_routing/xnet",
    "rs/tests/message_routing/xnet/slo_test_lib",
    "rs/tests/nested",
    "rs/tests/nested/nns_recovery",
    "rs/tests/networking",
    "rs/tests/networking/canister_http",
    "rs/tests/networking/canisters",
    "rs/tests/networking/subnet_update_workload",
    "rs/tests/nns",
    "rs/tests/nns/cycles_minting",
    "rs/tests/nns/nns_dapp",
    "rs/tests/nns/sns",
    "rs/tests/nns/sns/lib",
    "rs/tests/node",
    "rs/tests/query_stats/lib",
    "rs/tests/research",
    "rs/tests/research/spec_compliance",
    "rs/tests/sdk",
    "rs/tests/test_canisters/http_counter",
    "rs/tests/test_canisters/kv_store",
    "rs/tests/test_canisters/message",
    "rs/tests/test_canisters/signer",
    "rs/tests/testnets",
    "rs/test_utilities",
    "rs/test_utilities/artifact_pool",
    "rs/test_utilities/compare_dirs",
    "rs/test_utilities/consensus",
    "rs/test_utilities/embedders",
    "rs/test_utilities/execution_environment",
    "rs/test_utilities/identity",
    "rs/test_utilities/in_memory_logger",
    "rs/test_utilities/io",
    "rs/test_utilities/load_wasm",
    "rs/test_utilities/logger",
    "rs/test_utilities/metrics",
    "rs/test_utilities/registry",
    "rs/test_utilities/serialization",
    "rs/test_utilities/state",
    "rs/test_utilities/time",
    "rs/test_utilities/tmpdir",
    "rs/test_utilities/types",
    "rs/tla_instrumentation/local_key",
    "rs/tla_instrumentation/tla_instrumentation",
    "rs/tla_instrumentation/tla_instrumentation_proc_macros",
    "rs/tree_deserializer",
    "rs/types/base_types",
    "rs/types/base_types/protobuf_generator",
    "rs/types/exhaustive_derive",
    "rs/types/management_canister_types",
    "rs/types/types",
    "rs/types/types_test_utils",
    "rs/types/wasm_types",
    "rs/universal_canister/impl",
    "rs/universal_canister/lib",
    "rs/utils",
    "rs/utils/ensure",
    "rs/utils/lru_cache",
    "rs/utils/rustfmt",
    "rs/utils/thread",
    "rs/utils/validate_eq",
    "rs/utils/validate_eq_derive",
    "rs/validator",
    "rs/validator/http_request_arbitrary",
    "rs/validator/http_request_test_utils",
    "rs/validator/ingress_message",
    "rs/validator/ingress_message/test_canister",
    "rs/wasm_transform",
    "rs/xnet/hyper",
    "rs/xnet/payload_builder",
    "rs/xnet/uri",
    "rs/ic_os/remote_attestation/shared",
    "rs/ic_os/remote_attestation/server",
]

resolver = "2"

exclude = ["universal_canister/impl"]

[workspace.package]
version = "0.9.0"
authors = ["The Internet Computer Project Developers"]
description = "Autonomous serverless cloud functionality for the public Internet."
documentation = "https://internetcomputer.org/docs/"
edition = "2024"

[profile.release]
# Add debug information to the release build (does NOT reduce the level of optimization!)
# Makes flamegraphs more readable.
# https://doc.rust-lang.org/cargo/reference/manifest.html#the-profile-sections
debug = true

[profile.release-stripped]
inherits = "release"
lto = "thin"
debug = false

[profile.release-lto]
inherits = "release"
# Enable "thin" LTO to reduce both the compilation time and the binary size.
# See: https://doc.rust-lang.org/cargo/reference/profiles.html#lto
lto = "thin"

[profile.canister-release]
inherits = "release"
debug = false
lto = true
opt-level = 'z'

[profile.dev.package.curve25519-dalek]
opt-level = 3

[profile.dev.package.ic_bls12_381]
opt-level = 3

[profile.dev.package.k256]
opt-level = 3

[profile.dev.package.sha2]
opt-level = 3

[profile.dev.package.p256]
opt-level = 3

[workspace.dependencies]
actix-rt = "2.10.0"
actix-web = "4.9.0"
actix-web-prom = "0.9.0"
aide = { version = "^0.14.2", features = ["axum", "axum-json"] }
anyhow = "1.0.93"
arbitrary = { version = "1.3.2", features = ["derive"] }
arrayvec = "0.7.4"
askama = { version = "0.12.1", features = ["serde-json"] }
assert_cmd = "2.0.16"
assert_matches = "1.5.0"
async-recursion = "1.0.5"
async-stream = "0.3.6"
async-trait = "0.1.83"
axum = { version = "0.8.4", features = ["ws"] }
axum-extra = { version = "0.10.1", features = ["typed-header"] }
axum-server = { version = "0.7.2", features = ["tls-rustls-no-provider"] }
backoff = "0.4"
base64 = { version = "0.13.1" }
bincode = "1.3.3"
bitcoin = { git = "https://github.com/dfinity/rust-dogecoin", rev = "cda2b5ec270017c82abd6ef2e71b7fe583a133fd", features = [
    "default",
    "rand",
    "serde",
] }
# build-info and build-info-build MUST be kept in sync!
build-info = { git = "https://github.com/dfinity-lab/build-info", rev = "701a696844fba5c87df162fbbc1ccef96f27c9d7" }
build-info-build = { git = "https://github.com/dfinity-lab/build-info", rev = "701a696844fba5c87df162fbbc1ccef96f27c9d7", default-features = false }
bytes = "1.9.0"
canbench-rs = "0.2.1"
candid = { version = "0.10.17" }
candid_parser = { version = "0.1.2" }
chrono = { version = "0.4.38", default-features = false, features = [
    "alloc",
    "clock",
    "serde",
] }
ciborium = "0.2.1"
clap = { version = "4.5.20", features = ["derive", "string"] }
# cloudflare v0.12 is broken, master is partly fixed but unreleased yet.
# see:
# - https://github.com/cloudflare/cloudflare-rs/issues/222
# - https://github.com/cloudflare/cloudflare-rs/issues/236
cloudflare = { git = "https://github.com/dfinity/cloudflare-rs.git", rev = "8b011d170d9d61eaad77bb9645371f6219285104", default-features = false }
criterion = { version = "0.5.1", features = ["html_reports", "async_tokio"] }
crossbeam-channel = "0.5.15"
curve25519-dalek = { version = "4.1.3", features = [
    "group",
    "precomputed-tables",
] }
der = { version = "0.7", default-features = false, features = ["derive"] }
derive-new = "0.7.0"
devicemapper = "0.34"
dfx-core = { version = "0.1.4" }
ed25519-dalek = { version = "2.1.1", features = [
    "std",
    "zeroize",
    "digest",
    "batch",
    "pkcs8",
    "pem",
    "hazmat",
] }
ethnum = { version = "1.3.2", features = ["serde"] }
evm_rpc_types = { version = "2.0.0" }
flate2 = "1.0.31"
fs_extra = "1.2.0"
futures = "0.3.31"
futures-util = "0.3.31"
get_if_addrs = "0.5.3"
getrandom = { version = "0.2", features = ["custom"] }
goldenfile = "1.8.0"
gpt = "4.1"
hex = { version = "0.4.3", features = ["serde"] }
hkdf = "^0.12"
http = "1.3.1"
http-body = "1.0.1"
http-body-util = "0.1.3"
humantime = "2.2"
humantime-serde = "1.1.1"
hyper = { version = "1.6.0", features = ["full"] }
hyper-rustls = { version = "0.27.5", default-features = false, features = [
    "http1",
    "http2",
    "native-tokio",
    "ring",
    "tls12",
] }
hyper-socks2 = { version = "0.9.1", default-features = false }
hyper-util = { version = "0.1.12", features = ["full"] }
ic0 = "1.0.0"
ic-agent = { version = "0.40.1", features = ["pem", "ring"] }
ic-bn-lib = { git = "https://github.com/dfinity/ic-bn-lib", rev = "620fb49a238b3d8a2caa436b5742ed7ca7012098", features = [
    "acme_alpn",
] }
ic-btc-interface = "0.2.3"
ic-canister-sig-creation = "1.3.0"
ic-cbor = "3"
ic-cdk = "0.18.7"
ic-cdk-timers = "0.12.2"
ic-certificate-verification = "3"
ic-certification = "3"
ic-ed25519 = "0.4.0"
ic-gateway = { git = "https://github.com/dfinity/ic-gateway", rev = "b78562340bd00f05f9c055dcba3ec0f74758c927", default-features = false }
ic-http-certification = "3.0.3"
ic-http-gateway = "0.3.0"
ic-identity-hsm = "0.40.1"
ic-management-canister-types = "0.3.0"
ic-response-verification = "3"
ic-secp256k1 = "0.3.0"
ic-sha3 = "1.0.0"
ic-stable-structures = "0.6.8"
ic-transport-types = { version = "0.40.1" }
ic-utils = { version = "0.40.1", features = ["raw"] }
ic-vetkeys = "0.4.0"
ic_bls12_381 = { version = "0.10.1", default-features = false, features = [
    "groups",
    "pairings",
    "alloc",
    "experimental",
    "zeroize",
] }
ic_principal = { version = "0.1.1", default-features = false }
idna = "1.0.2"
indexmap = "2.10.0"
indoc = "1.0.9"
inferno = "0.12.0"
ipnet = { version = "2.10.1", features = ["serde"] }
itertools = "0.12.0"
jsonrpc = { version = "0.18.0", features = ["minreq_http"] }
k256 = { version = "0.13.4", default-features = false, features = [
    "arithmetic",
    "ecdsa",
    "pem",
    "pkcs8",
    "precomputed-tables",
    "schnorr",
    "std",
] }
lazy_static = "1.4.0"
leb128 = "0.2.5"
libc = "0.2.158"
libcryptsetup-rs = "0.13"
libflate = "2.1.0"
libnss = "0.5.0"
lmdb-rkv = { git = "https://github.com/dfinity-lab/lmdb-rs", rev = "4d952c8f1dca79de855af892b444d7112567b58d" }
lmdb-rkv-sys = { git = "https://github.com/dfinity-lab/lmdb-rs", rev = "4d952c8f1dca79de855af892b444d7112567b58d" }
local-ip-address = "0.5.6"
loopdev-3 = "0.5"
macaddr = "1.0"
memmap2 = "0.9.5"
minicbor = { version = "0.19.1", features = ["alloc", "derive"] }
minicbor-derive = "0.13.0"
mockall = "0.13.0"
mockito = "1.6.1"
nftables = "0.4"
nix = { version = "0.24.3", features = ["ptrace", "user"] }
num-bigint = "0.4.6"
num-traits = { version = "0.2.12", features = ["libm"] }
num_cpus = "1.16.0"
object = "0.37.3"
opentelemetry = { version = "0.27.0", features = ["metrics", "trace"] }
opentelemetry-otlp = { version = "0.27.0", features = ["grpc-tonic"] }
opentelemetry_sdk = { version = "0.27.0", features = ["trace", "rt-tokio"] }
p256 = { version = "0.13.2", default-features = false, features = [
    "arithmetic",
    "ecdsa",
    "pem",
    "pkcs8",
] }
p384 = "0.13"
pairing = "0.23"
parking_lot = "0.12.3"
paste = "1.0.15"
pem = { version = "^1.0.1", default-features = false }
ping = "0.5.0"
pkcs8 = "0.10.2"
pprof = { version = "0.14.0", default-features = false, features = [
    "criterion",
    "flamegraph",
    "prost-codec",
] }
predicates = "3.1.2"
pretty_assertions = "1.4.0"
proc-macro2 = "1.0.89"
prometheus = { version = "0.13.4", features = ["process"] }
prometheus-parse = { version = "0.2.4" }
proptest = "1.5.0"
proptest-derive = "0.5.0"
prost = "0.13.3"
prost-build = "0.13.3"
protobuf = "2.28.0"
ring = { version = "^0.17.7", features = ["std"] }
quinn = { version = "0.11.5", default-features = false, features = [
    "ring",
    "log",
    "runtime-tokio",
    "rustls",
] }
quinn-udp = "0.5.5"
quote = "1.0.37"
rand = { version = "0.8.5", features = ["small_rng"] }
rand_chacha = "0.3.1"
raw-cpuid = "11.5"
rayon = "1.10.0"
rcgen = { version = "0.13.1", features = ["zeroize"] }
regex = "1.11.0"
serde_regex = "1.1.0"
reqwest = { version = "0.12.15", default-features = false, features = [
    "blocking",
    "http2",
    "json",
    "multipart",
    "rustls-tls",
    "rustls-tls-native-roots",
    "socks",
    "stream",
] }
rolling-file = "0.2.0"
rsa = { version = "0.9.6", features = ["sha2", "getrandom"] }
rstest = "0.19.0"
rust-ini = "0.21.1"
rustc-demangle = { version = "0.1.16" }
rustls = { version = "0.23.18", default-features = false, features = [
    "ring",
    "std",
    "brotli",
    "tls12",
] }
schemars = { version = "0.8.21", features = ["derive"] }
scopeguard = "1.2.0"
semver = { version = "1.0.9", features = ["serde"] }
serde = { version = "1.0.203", features = ["derive"] }
serde_bytes = "0.11.15"
serde_cbor = "0.11.2"
serde_json = { version = "^1.0.107" }
serde_with = "1.14.0"
serde_yaml = "0.9.33"
sev = { version = "6.2", default-features = false, features = [
    "crypto_nossl",
    "snp",
] }
sha2 = "0.10.9"
sha3 = "0.10.8"
signature = "2.2.0"
simple_asn1 = "0.6.2"
slog = { version = "2.7.0", features = [
    "max_level_trace",
    "nested-values",
    "release_max_level_trace",
] }
slog-async = { version = "2.8.0", features = ["nested-values"] }
slog-scope = "4.4.0"
slog-term = "2.9.1"
slotmap = "1.0.7"
socket2 = { version = "0.5.7", features = ["all"] }
ssh2 = "0.9.4"
static_assertions = "1.1.0"
strum = { version = "0.26.3", features = ["derive"] }
strum_macros = "0.26.4"
subtle = "2.6.1"
syn = { version = "1.0.109", features = ["fold", "full"] }
sys-mount = "3.0"
systemd = "0.10"
tar = "0.4.39"
tempfile = "3.20"
thiserror = "2.0.3"
threadpool = "1.8.1"
tikv-jemalloc-ctl = { version = "0.6", features = ["stats"] }
tikv-jemallocator = "0.6"
time = { version = "0.3.36", features = ["formatting"] }
tokio = { version = "1.42.0", features = ["full"] }
tokio-metrics = "0.4.0"
tokio-rustls = { version = "0.26.0", default-features = false, features = [
    "ring",
] }
tokio-stream = "0.1.17"
tokio-test = "0.4.4"
tokio-util = { version = "0.7.13", features = ["full"] }
tonic = "0.12.3"
tonic-build = "0.12.3"
tower = { version = "0.5.2", features = ["full"] }
tower-http = { version = "0.6.4", features = [
    "add-extension",
    "cors",
    "limit",
    "trace",
    "request-id",
    "util",
    "compression-full",
    "tracing",
] }
tracing = "0.1.41"
tracing-appender = "0.2.3"
tracing-flame = "0.2.0"
tracing-opentelemetry = "0.28.0"
tracing-subscriber = { version = "0.3.19", features = [
    "env-filter",
    "fmt",
    "json",
    "time",
] }
turmoil = "0.6.4"
url = { version = "2.5.3", features = ["serde"] }
# DO NOT upgrade to >=1.13 unless you are ready to deal with problems.
# This breaks `wasm32-unknown-unknown` compatibility.
# Read https://github.com/uuid-rs/uuid/releases/tag/1.13.0
uuid = { version = "=1.12.1", features = ["v4", "serde"] }
virt = "0.4"
walkdir = "2.3.3"
walrus = "0.23.3"
wasm-encoder = { version = "0.235.0", features = ["wasmparser"] }
wasmparser = "0.235.0"
wasmprinter = "0.235.0"
wast = "235.0.0"
wat = "1.235.0"
which = "6.0.3"
wirm = { version = "2.1.0", features = ["parallel"] }
x509-cert = { version = "0.2.5", features = ["builder", "hazmat"] }
x509-parser = { version = "0.16.0" }
zeroize = { version = "1.8.1", features = ["zeroize_derive"] }
zstd = "0.13.2"

[workspace.dependencies.ic-wasm]
default-features = false
features = ["exe"]
version = "^0.8.1"



================================================
FILE: clippy.toml
================================================
too-many-arguments-threshold = 12
allow-expect-in-tests = true
allow-unwrap-in-tests = true
disallowed-methods = [
    { path = "bincode::deserialize_from", reason = "bincode::deserialize_from() is not safe to use on untrusted data, since the method will read a u64 length value from the first 8 bytes of the serialized payload and will then attempt to allocate this number of bytes without any validation." },
    { path = "std::io::Write::write", reason = "`Write::write()` may not write the entire buffer. Use `Write::write_all()` instead. Or, if you are intentionally using `Write::write()`, use `#[allow(clippy::disallowed_methods)]` to locally disable this check." },
    { path = "tokio::io::AsyncWriteExt::write", reason = "`AsyncWriteExt::write()` may not write the entire buffer. Use `AsyncWriteExt::write_all()` instead. Or, if you are intentionally using `Write::write()`, use `#[allow(clippy::disallowed_methods)]` to locally disable this check." },
    { path = "tokio::task::block_in_place", reason = "`block_in_place()` almost always signals that there is an issue with the overall design. Furthermore, `block_in_place()` panics unless the Tokio scheduler has enough available threads to move tasks. If you are intentionally using `block_in_place()`, use `#[allow(clippy::disallowed_methods)]` to locally disable this check." },
    # unbounded channels are for expert use only
    { path = "tokio::sync::mpsc::unbounded_channel", reason = "Using an unbounded channel can lead to unbounded memory growth. Please use a bounded channel instead. If you are intentionally using an unbounded channel, use `#[allow(clippy::disallowed_methods)]` to locally disable this check." },
    { path = "futures::channel::mpsc::unbounded", reason = "Using an unbounded channel most likely will read to unbounded memory growth. Please use a bounded channel instead. If you are intentionally using unbounded channel, use `#[allow(clippy::disallowed_methods)]` to locally disable this check." },
    { path = "futures_channel::mpsc::unbounded", reason = "Using an unbounded channel most likely will read to unbounded memory growth. Please use a bounded channel instead. If you are intentionally using unbounded channel, use `#[allow(clippy::disallowed_methods)]` to locally disable this check." },
    { path = "crossbeam::channel::unbounded", reason = "Using an unbounded channel most likely will read to unbounded memory growth. Please use a bounded channel instead. If you are intentionally using unbounded channel, use `#[allow(clippy::disallowed_methods)]` to locally disable this check." },
    { path = "crossbeam_channel::unbounded", reason = "Using an unbounded channel most likely will read to unbounded memory growth. Please use a bounded channel instead. If you are intentionally using unbounded channel, use `#[allow(clippy::disallowed_methods)]` to locally disable this check." },
    { path = "ic_cdk::futures::spawn", reason = "This method doesn't poll immediately. For safe migration from `ic_cdk::spawn`, use `ic_cdk::futures::spawn_017_compat`. Use `#[allow(clippy::disallowed_methods)]` to locally disable this check if you understand the semantics." },
]
disallowed-types = [
    { path = "tokio::sync::Mutex", reason = "You should only use an asynchronous lock if you need to .await something while the lock is locked. Usually, this is not necessary, and you should avoid using an asynchronous lock when you can. Asynchronous locks are a lot slower than blocking locks. Please read how to share state effectively across async tasks https://tokio.rs/tokio/tutorial/shared-state. If you are intentionally using an async Mutex, use `#[allow(clippy::disallowed_types)]` to locally disable this check." },
    { path = "tokio::sync::RwLock", reason = "You should only use an asynchronous lock if you need to .await something while the lock is locked. Usually, this is not necessary, and you should avoid using an asynchronous lock when you can. Asynchronous locks are a lot slower than blocking locks. Please read how to share state effectively across async tasks https://tokio.rs/tokio/tutorial/shared-state. If you are intentionally using an async RwLock, use `#[allow(clippy::disallowed_types)]` to locally disable this check." },
    { path = "protobuf::coded_input_stream::CodedInputStream", reason = "The type allows uncontrolled recursion while parsing unknown fields via `skip_group`. Please refer to RUSTSEC-2024-0437 for more details."},
]



================================================
FILE: deny.toml
================================================
# This template contains all of the possible sections and their default values

# Note that all fields that take a lint level have these possible values:
# * deny - An error will be produced and the check will fail
# * warn - A warning will be produced, but the check will not fail
# * allow - No warning or error will be produced, though in some cases a note
# will be

# The values provided in this template are the default values that will be used
# when any section or field is not specified in your own configuration

# Root options

# The graph table configures how the dependency graph is constructed and thus
# which crates the checks are performed against
[graph]
# If 1 or more target triples (and optionally, target_features) are specified,
# only the specified targets will be checked when running `cargo deny check`.
# This means, if a particular package is only ever used as a target specific
# dependency, such as, for example, the `nix` crate only being used via the
# `target_family = "unix"` configuration, that only having windows targets in
# this list would mean the nix crate, as well as any of its exclusive
# dependencies not shared by any other crates, would be ignored, as the target
# list here is effectively saying which targets you are building for.
targets = [
    # The triple can be any string, but only the target triples built in to
    # rustc (as of 1.40) can be checked against actual config expressions
    #"x86_64-unknown-linux-musl",
    # You can also specify which target_features you promise are enabled for a
    # particular target. target_features are currently not validated against
    # the actual valid features supported by the target architecture.
    #{ triple = "wasm32-unknown-unknown", features = ["atomics"] },
]
# When creating the dependency graph used as the source of truth when checks are
# executed, this field can be used to prune crates from the graph, removing them
# from the view of cargo-deny. This is an extremely heavy hammer, as if a crate
# is pruned from the graph, all of its dependencies will also be pruned unless
# they are connected to another crate in the graph that hasn't been pruned,
# so it should be used with care. The identifiers are [Package ID Specifications]
# (https://doc.rust-lang.org/cargo/reference/pkgid-spec.html)
#exclude = []
# If true, metadata will be collected with `--all-features`. Note that this can't
# be toggled off if true, if you want to conditionally enable `--all-features` it
# is recommended to pass `--all-features` on the cmd line instead
all-features = false
# If true, metadata will be collected with `--no-default-features`. The same
# caveat with `all-features` applies
no-default-features = false
# If set, these feature will be enabled when collecting metadata. If `--features`
# is specified on the cmd line they will take precedence over this option.
#features = []

# The output table provides options for how/if diagnostics are outputted
[output]
# When outputting inclusion graphs in diagnostics that include features, this
# option can be used to specify the depth at which feature edges will be added.
# This option is included since the graphs can be quite large and the addition
# of features from the crate(s) to all of the graph roots can be far too verbose.
# This option can be overridden via `--feature-depth` on the cmd line
feature-depth = 1

# This section is considered when running `cargo deny check advisories`
# More documentation for the advisories section can be found here:
# https://embarkstudios.github.io/cargo-deny/checks/advisories/cfg.html
[advisories]
# The path where the advisory databases are cloned/fetched into
#db-path = "$CARGO_HOME/advisory-dbs"
# The url(s) of the advisory databases to use
#db-urls = ["https://github.com/rustsec/advisory-db"]
# A list of advisory IDs to ignore. Note that ignored advisories will still
# output a note when they are encountered.
ignore = [
    #"RUSTSEC-0000-0000",
    #{ id = "RUSTSEC-0000-0000", reason = "you can specify a reason the advisory is ignored" },
    #"a-crate-that-is-yanked@0.1.1", # you can also ignore yanked crate versions if you wish
    #{ crate = "a-crate-that-is-yanked@0.1.1", reason = "you can specify why you are ignoring the yanked crate" },
]
# If this is true, then cargo deny will use the git executable to fetch advisory database.
# If this is false, then it uses a built-in git library.
# Setting this to true can be helpful if you have special authentication requirements that cargo-deny does not support.
# See Git Authentication for more information about setting up git authentication.
#git-fetch-with-cli = true

# This section is considered when running `cargo deny check licenses`
# More documentation for the licenses section can be found here:
# https://embarkstudios.github.io/cargo-deny/checks/licenses/cfg.html
[licenses]
# List of explicitly allowed licenses
# See https://spdx.org/licenses/ for list of possible licenses
# [possible values: any SPDX 3.11 short identifier (+ optional exception)].
allow = [
    "Apache-2.0",
    "Apache-2.0 WITH LLVM-exception",
    "BlueOak-1.0.0",
    "BSD-2-Clause",
    "BSD-3-Clause",
    "0BSD",
    "CC0-1.0",
    "CDDL-1.0",
    "ISC",
    "MIT",
    "MPL-2.0",
    "Unicode-DFS-2016",
    "Zlib",
    "LGPL-3.0",
]
# The confidence threshold for detecting a license from license text.
# The higher the value, the more closely the license text must be to the
# canonical license text of a valid SPDX license file.
# [possible values: any between 0.0 and 1.0].
confidence-threshold = 0.8
# Allow 1 or more licenses on a per-crate basis, so that particular licenses
# aren't accepted for every possible crate as with the normal allow list
exceptions = [
    # Each entry is the crate and version constraint, and its specific allow
    # list
    #{ allow = ["Zlib"], crate = "adler32" },
]

# Some crates don't have (easily) machine readable licensing information,
# adding a clarification entry for it allows you to manually specify the
# licensing information
#[[licenses.clarify]]
# The package spec the clarification applies to
#crate = "ring"
# The SPDX expression for the license requirements of the crate
#expression = "MIT AND ISC AND OpenSSL"
# One or more files in the crate's source used as the "source of truth" for
# the license expression. If the contents match, the clarification will be used
# when running the license check, otherwise the clarification will be ignored
# and the crate will be checked normally, which may produce warnings or errors
# depending on the rest of your configuration
#license-files = [
# Each entry is a crate relative path, and the (opaque) hash of its contents
#{ path = "LICENSE", hash = 0xbd0eed23 }
#]

[licenses.private]
# If true, ignores workspace crates that aren't published, or are only
# published to private registries.
# To see how to mark a crate as unpublished (to the official registry),
# visit https://doc.rust-lang.org/cargo/reference/manifest.html#the-publish-field.
ignore = false
# One or more private registries that you might publish crates to, if a crate
# is only published to private registries, and ignore is true, the crate will
# not have its license(s) checked
registries = [
    #"https://sekretz.com/registry
]

# This section is considered when running `cargo deny check bans`.
# More documentation about the 'bans' section can be found here:
# https://embarkstudios.github.io/cargo-deny/checks/bans/cfg.html
[bans]
# Lint level for when multiple versions of the same crate are detected
multiple-versions = "allow"
# Lint level for when a crate version requirement is `*`
wildcards = "allow"
# The graph highlighting used when creating dotgraphs for crates
# with multiple versions
# * lowest-version - The path to the lowest versioned duplicate is highlighted
# * simplest-path - The path to the version with the fewest edges is highlighted
# * all - Both lowest-version and simplest-path are used
highlight = "all"
# The default lint level for `default` features for crates that are members of
# the workspace that is being checked. This can be overridden by allowing/denying
# `default` on a crate-by-crate basis if desired.
workspace-default-features = "allow"
# The default lint level for `default` features for external crates that are not
# members of the workspace. This can be overridden by allowing/denying `default`
# on a crate-by-crate basis if desired.
external-default-features = "allow"
# List of crates that are allowed. Use with care!
allow = [
    #"ansi_term@0.11.0",
    #{ crate = "ansi_term@0.11.0", reason = "you can specify a reason it is allowed" },
]
# List of crates to deny
deny = [
    # TODO: deny openssl
    # { name = "openssl" },
    #"ansi_term@0.11.0",
    #{ crate = "ansi_term@0.11.0", reason = "you can specify a reason it is banned" },
    # Wrapper crates can optionally be specified to allow the crate when it
    # is a direct dependency of the otherwise banned crate
    #{ crate = "ansi_term@0.11.0", wrappers = ["this-crate-directly-depends-on-ansi_term"] },
]

# List of features to allow/deny
# Each entry the name of a crate and a version range. If version is
# not specified, all versions will be matched.
#[[bans.features]]
#crate = "reqwest"
# Features to not allow
#deny = ["json"]
# Features to allow
#allow = [
#    "rustls",
#    "__rustls",
#    "__tls",
#    "hyper-rustls",
#    "rustls",
#    "rustls-pemfile",
#    "rustls-tls-webpki-roots",
#    "tokio-rustls",
#    "webpki-roots",
#]
# If true, the allowed features must exactly match the enabled feature set. If
# this is set there is no point setting `deny`
#exact = true

# Certain crates/versions that will be skipped when doing duplicate detection.
skip = [
    #"ansi_term@0.11.0",
    #{ crate = "ansi_term@0.11.0", reason = "you can specify a reason why it can't be updated/removed" },
]
# Similarly to `skip` allows you to skip certain crates during duplicate
# detection. Unlike skip, it also includes the entire tree of transitive
# dependencies starting at the specified crate, up to a certain depth, which is
# by default infinite.
skip-tree = [
    #"ansi_term@0.11.0", # will be skipped along with _all_ of its direct and transitive dependencies
    #{ crate = "ansi_term@0.11.0", depth = 20 },
]

# This section is considered when running `cargo deny check sources`.
# More documentation about the 'sources' section can be found here:
# https://embarkstudios.github.io/cargo-deny/checks/sources/cfg.html
[sources]
# Lint level for what to happen when a crate from a crate registry that is not
# in the allow list is encountered
unknown-registry = "deny"
# Lint level for what to happen when a crate from a git repository that is not
# in the allow list is encountered
unknown-git = "deny"
# List of URLs for allowed crate registries. Defaults to the crates.io index
# if not specified. If it is specified but empty, no registries are allowed.
allow-registry = ["https://github.com/rust-lang/crates.io-index"]
# List of URLs for allowed Git repositories
allow-git = []

[sources.allow-org]
# 1 or more github.com organizations to allow git sources for
# TODO(NET-1607): remove apoelstra and dfinity-lab from list
github = ["dfinity-lab", "dfinity", "apoelstra"]
# 1 or more gitlab.com organizations to allow git sources for
# gitlab = [""]
# 1 or more bitbucket.org organizations to allow git sources for
# bitbucket = [""]



================================================
FILE: go.mod
================================================
module github.com/dfinity/ic

go 1.19

require (
	github.com/fatih/color v1.13.0
	github.com/google/go-cmp v0.5.9
	github.com/schollz/closestmatch v2.1.0+incompatible
	github.com/spf13/cobra v1.6.1
	github.com/stretchr/testify v1.8.1
)

require (
	github.com/davecgh/go-spew v1.1.1 // indirect
	github.com/inconshreveable/mousetrap v1.1.0 // indirect
	github.com/mattn/go-colorable v0.1.11 // indirect
	github.com/mattn/go-isatty v0.0.14 // indirect
	github.com/pmezard/go-difflib v1.0.0 // indirect
	github.com/spf13/pflag v1.0.5 // indirect
	golang.org/x/sys v0.0.0-20220728004956-3c1f35247d10 // indirect
	gopkg.in/yaml.v3 v3.0.1 // indirect
)



================================================
FILE: go.sum
================================================
github.com/cpuguy83/go-md2man/v2 v2.0.2/go.mod h1:tgQtvFlXSQOSOSIRvRPT7W67SCa46tRHOmNcaadrF8o=
github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=
github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/fatih/color v1.13.0 h1:8LOYc1KYPPmyKMuN8QV2DNRWNbLo6LZ0iLs8+mlH53w=
github.com/fatih/color v1.13.0/go.mod h1:kLAiJbzzSOZDVNGyDpeOxJ47H46qBXwg5ILebYFFOfk=
github.com/google/go-cmp v0.5.9 h1:O2Tfq5qg4qc4AmwVlvv0oLiVAGB7enBSJ2x2DqQFi38=
github.com/google/go-cmp v0.5.9/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=
github.com/inconshreveable/mousetrap v1.0.1/go.mod h1:vpF70FUmC8bwa3OWnCshd2FqLfsEA9PFc4w1p2J65bw=
github.com/inconshreveable/mousetrap v1.1.0 h1:wN+x4NVGpMsO7ErUn/mUI3vEoE6Jt13X2s0bqwp9tc8=
github.com/inconshreveable/mousetrap v1.1.0/go.mod h1:vpF70FUmC8bwa3OWnCshd2FqLfsEA9PFc4w1p2J65bw=
github.com/mattn/go-colorable v0.1.9/go.mod h1:u6P/XSegPjTcexA+o6vUJrdnUu04hMope9wVRipJSqc=
github.com/mattn/go-colorable v0.1.11 h1:nQ+aFkoE2TMGc0b68U2OKSexC+eq46+XwZzWXHRmPYs=
github.com/mattn/go-colorable v0.1.11/go.mod h1:u5H1YNBxpqRaxsYJYSkiCWKzEfiAb1Gb520KVy5xxl4=
github.com/mattn/go-isatty v0.0.12/go.mod h1:cbi8OIDigv2wuxKPP5vlRcQ1OAZbq2CE4Kysco4FUpU=
github.com/mattn/go-isatty v0.0.14 h1:yVuAays6BHfxijgZPzw+3Zlu5yQgKGP2/hcQbHb7S9Y=
github.com/mattn/go-isatty v0.0.14/go.mod h1:7GGIvUiUoEMVVmxf/4nioHXj79iQHKdU27kJ6hsGG94=
github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
github.com/russross/blackfriday/v2 v2.1.0/go.mod h1:+Rmxgy9KzJVeS9/2gXHxylqXiyQDYRxCVz55jmeOWTM=
github.com/schollz/closestmatch v2.1.0+incompatible h1:Uel2GXEpJqOWBrlyI+oY9LTiyyjYS17cCYRqP13/SHk=
github.com/schollz/closestmatch v2.1.0+incompatible/go.mod h1:RtP1ddjLong6gTkbtmuhtR2uUrrJOpYzYRvbcPAid+g=
github.com/spf13/cobra v1.6.1 h1:o94oiPyS4KD1mPy2fmcYYHHfCxLqYjJOhGsCHFZtEzA=
github.com/spf13/cobra v1.6.1/go.mod h1:IOw/AERYS7UzyrGinqmz6HLUo219MORXGxhbaJUqzrY=
github.com/spf13/pflag v1.0.5 h1:iy+VFUOCP1a+8yFto/drg2CJ5u0yRoB7fZw3DKv/JXA=
github.com/spf13/pflag v1.0.5/go.mod h1:McXfInJRrz4CZXVZOBLb0bTZqETkiAhM9Iw0y3An2Bg=
github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=
github.com/stretchr/objx v0.4.0/go.mod h1:YvHI0jy2hoMjB+UWwv71VJQ9isScKT/TqJzVSSt89Yw=
github.com/stretchr/objx v0.5.0/go.mod h1:Yh+to48EsGEfYuaHDzXPcE3xhTkx73EhmCGUpEOglKo=
github.com/stretchr/testify v1.7.1/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=
github.com/stretchr/testify v1.8.0/go.mod h1:yNjHg4UonilssWZ8iaSj1OCr/vHnekPRkoO+kdMU+MU=
github.com/stretchr/testify v1.8.1 h1:w7B6lhMri9wdJUVmEZPGGhZzrYTPvgJArz7wNPgYKsk=
github.com/stretchr/testify v1.8.1/go.mod h1:w2LPCIKwWwSfY2zedu0+kehJoqGctiVI29o6fzry7u4=
golang.org/x/sys v0.0.0-20200116001909-b77594299b42/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
golang.org/x/sys v0.0.0-20200223170610-d5e6a3e2c0ae/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
golang.org/x/sys v0.0.0-20210630005230-0f9fa26af87c/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.0.0-20210927094055-39ccf1dd6fa6/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.0.0-20220728004956-3c1f35247d10 h1:WIoqL4EROvwiPdUtaip4VcDdpZ4kha7wBWZrbVKCIZg=
golang.org/x/sys v0.0.0-20220728004956-3c1f35247d10/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405 h1:yhCVgyC4o1eVCa2tZl7eS0r+SDo693bJlVdllGtEeKM=
gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
gopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=



================================================
FILE: LICENSE
================================================
Copyright © 2021 DFINITY Foundation

Each file in this repository is licensed under the license as
described in the LICENSE file in the same directory that contains the
file or, if that doesn't exist, the first LICENSE file in any
higher-level directory.

Unless stated otherwise as described above, all files in this
directory are licensed under the Apache License, Version 2.0 (the
"License"); you may not use these files except in compliance with the
License. You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

Some files in this repository are licensed under the Internet Computer
Community Source License, Version 1.0 of which you can obtain a copy
of at:

  http://dfinity.org/licenses/IC-1.0

Some other files are licensed under the Internet Computer Shared
Community Source License, Version 1.0 of which you can obtain a copy
of at:

  http://dfinity.org/licenses/IC-shared-1.0

The Apache-2.0 license is also copied below:

                              Apache License
                        Version 2.0, January 2004
                     http://www.apache.org/licenses/

TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

1. Definitions.

   "License" shall mean the terms and conditions for use, reproduction,
   and distribution as defined by Sections 1 through 9 of this document.

   "Licensor" shall mean the copyright owner or entity authorized by
   the copyright owner that is granting the License.

   "Legal Entity" shall mean the union of the acting entity and all
   other entities that control, are controlled by, or are under common
   control with that entity. For the purposes of this definition,
   "control" means (i) the power, direct or indirect, to cause the
   direction or management of such entity, whether by contract or
   otherwise, or (ii) ownership of fifty percent (50%) or more of the
   outstanding shares, or (iii) beneficial ownership of such entity.

   "You" (or "Your") shall mean an individual or Legal Entity
   exercising permissions granted by this License.

   "Source" form shall mean the preferred form for making modifications,
   including but not limited to software source code, documentation
   source, and configuration files.

   "Object" form shall mean any form resulting from mechanical
   transformation or translation of a Source form, including but
   not limited to compiled object code, generated documentation,
   and conversions to other media types.

   "Work" shall mean the work of authorship, whether in Source or
   Object form, made available under the License, as indicated by a
   copyright notice that is included in or attached to the work
   (an example is provided in the Appendix below).

   "Derivative Works" shall mean any work, whether in Source or Object
   form, that is based on (or derived from) the Work and for which the
   editorial revisions, annotations, elaborations, or other modifications
   represent, as a whole, an original work of authorship. For the purposes
   of this License, Derivative Works shall not include works that remain
   separable from, or merely link (or bind by name) to the interfaces of,
   the Work and Derivative Works thereof.

   "Contribution" shall mean any work of authorship, including
   the original version of the Work and any modifications or additions
   to that Work or Derivative Works thereof, that is intentionally
   submitted to Licensor for inclusion in the Work by the copyright owner
   or by an individual or Legal Entity authorized to submit on behalf of
   the copyright owner. For the purposes of this definition, "submitted"
   means any form of electronic, verbal, or written communication sent
   to the Licensor or its representatives, including but not limited to
   communication on electronic mailing lists, source code control systems,
   and issue tracking systems that are managed by, or on behalf of, the
   Licensor for the purpose of discussing and improving the Work, but
   excluding communication that is conspicuously marked or otherwise
   designated in writing by the copyright owner as "Not a Contribution."

   "Contributor" shall mean Licensor and any individual or Legal Entity
   on behalf of whom a Contribution has been received by Licensor and
   subsequently incorporated within the Work.

2. Grant of Copyright License. Subject to the terms and conditions of
   this License, each Contributor hereby grants to You a perpetual,
   worldwide, non-exclusive, no-charge, royalty-free, irrevocable
   copyright license to reproduce, prepare Derivative Works of,
   publicly display, publicly perform, sublicense, and distribute the
   Work and such Derivative Works in Source or Object form.

3. Grant of Patent License. Subject to the terms and conditions of
   this License, each Contributor hereby grants to You a perpetual,
   worldwide, non-exclusive, no-charge, royalty-free, irrevocable
   (except as stated in this section) patent license to make, have made,
   use, offer to sell, sell, import, and otherwise transfer the Work,
   where such license applies only to those patent claims licensable
   by such Contributor that are necessarily infringed by their
   Contribution(s) alone or by combination of their Contribution(s)
   with the Work to which such Contribution(s) was submitted. If You
   institute patent litigation against any entity (including a
   cross-claim or counterclaim in a lawsuit) alleging that the Work
   or a Contribution incorporated within the Work constitutes direct
   or contributory patent infringement, then any patent licenses
   granted to You under this License for that Work shall terminate
   as of the date such litigation is filed.

4. Redistribution. You may reproduce and distribute copies of the
   Work or Derivative Works thereof in any medium, with or without
   modifications, and in Source or Object form, provided that You
   meet the following conditions:

   (a) You must give any other recipients of the Work or
       Derivative Works a copy of this License; and

   (b) You must cause any modified files to carry prominent notices
       stating that You changed the files; and

   (c) You must retain, in the Source form of any Derivative Works
       that You distribute, all copyright, patent, trademark, and
       attribution notices from the Source form of the Work,
       excluding those notices that do not pertain to any part of
       the Derivative Works; and

   (d) If the Work includes a "NOTICE" text file as part of its
       distribution, then any Derivative Works that You distribute must
       include a readable copy of the attribution notices contained
       within such NOTICE file, excluding those notices that do not
       pertain to any part of the Derivative Works, in at least one
       of the following places: within a NOTICE text file distributed
       as part of the Derivative Works; within the Source form or
       documentation, if provided along with the Derivative Works; or,
       within a display generated by the Derivative Works, if and
       wherever such third-party notices normally appear. The contents
       of the NOTICE file are for informational purposes only and
       do not modify the License. You may add Your own attribution
       notices within Derivative Works that You distribute, alongside
       or as an addendum to the NOTICE text from the Work, provided
       that such additional attribution notices cannot be construed
       as modifying the License.

   You may add Your own copyright statement to Your modifications and
   may provide additional or different license terms and conditions
   for use, reproduction, or distribution of Your modifications, or
   for any such Derivative Works as a whole, provided Your use,
   reproduction, and distribution of the Work otherwise complies with
   the conditions stated in this License.

5. Submission of Contributions. Unless You explicitly state otherwise,
   any Contribution intentionally submitted for inclusion in the Work
   by You to the Licensor shall be under the terms and conditions of
   this License, without any additional terms or conditions.
   Notwithstanding the above, nothing herein shall supersede or modify
   the terms of any separate license agreement you may have executed
   with Licensor regarding such Contributions.

6. Trademarks. This License does not grant permission to use the trade
   names, trademarks, service marks, or product names of the Licensor,
   except as required for reasonable and customary use in describing the
   origin of the Work and reproducing the content of the NOTICE file.

7. Disclaimer of Warranty. Unless required by applicable law or
   agreed to in writing, Licensor provides the Work (and each
   Contributor provides its Contributions) on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
   implied, including, without limitation, any warranties or conditions
   of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
   PARTICULAR PURPOSE. You are solely responsible for determining the
   appropriateness of using or redistributing the Work and assume any
   risks associated with Your exercise of permissions under this License.

8. Limitation of Liability. In no event and under no legal theory,
   whether in tort (including negligence), contract, or otherwise,
   unless required by applicable law (such as deliberate and grossly
   negligent acts) or agreed to in writing, shall any Contributor be
   liable to You for damages, including any direct, indirect, special,
   incidental, or consequential damages of any character arising as a
   result of this License or out of the use or inability to use the
   Work (including but not limited to damages for loss of goodwill,
   work stoppage, computer failure or malfunction, or any and all
   other commercial damages or losses), even if such Contributor
   has been advised of the possibility of such damages.

9. Accepting Warranty or Additional Liability. While redistributing
   the Work or Derivative Works thereof, You may choose to offer,
   and charge a fee for, acceptance of support, warranty, indemnity,
   or other liability obligations and/or rights consistent with this
   License. However, in accepting such obligations, You may act only
   on Your own behalf and on Your sole responsibility, not on behalf
   of any other Contributor, and only if You agree to indemnify,
   defend, and hold each Contributor harmless for any liability
   incurred by, or claims asserted against, such Contributor by reason
   of your accepting any such warranty or additional liability.

END OF TERMS AND CONDITIONS



================================================
FILE: mainnet-canister-revisions.json
================================================
{
  "archive": {
    "rev": "3ae3649a2366aaca83404b692fc58e4c6e604a25",
    "sha256": "5bf34cb029e437c4ccb990b1595876d4c869566d66b8b58059d0ee742891c219"
  },
  "bitcoin_testnet": {
    "sha256": "42af59ae4fd5041f30f8ac12f324e3a93533de0cb89cd2278100c2389cbfff65",
    "tag": "release/2025-07-02"
  },
  "ck_btc_archive": {
    "rev": "512cf412f33d430b79f42330518166d14fc6884e",
    "sha256": "649401fd06e58e61aea55747961d5144af673b5e70bccd005898a6da65c84c29"
  },
  "ck_btc_index": {
    "rev": "512cf412f33d430b79f42330518166d14fc6884e",
    "sha256": "a63b9628d45858b02eba1185c525c527c673746f4b57f6238822fd9f99907ae5"
  },
  "ck_btc_ledger": {
    "rev": "512cf412f33d430b79f42330518166d14fc6884e",
    "sha256": "901bc548f901145bd15a1156487eed703705794ad6a23787eaa04b1c7bbdcf48"
  },
  "ck_btc_ledger_v1": {
    "rev": "d4ee25b0865e89d3eaac13a60f0016d5e3296b31",
    "sha256": "a170bfdce5d66e751a3cc03747cb0f06b450af500e75e15976ec08a3f5691f4c"
  },
  "ck_btc_ledger_v2": {
    "rev": "e54d3fa34ded227c885d04e64505fa4b5d564743",
    "sha256": "3d808fa63a3d8ebd4510c0400aa078e99a31afaa0515f0b68778f929ce4b2a46"
  },
  "ck_btc_ledger_v2_noledgerversion": {
    "rev": "aba60ffbc46acfc8990bf4d5685c1360bd7026b9",
    "sha256": "67cfcbabb79e683b6fc855450d9972c9efaa7a1cd28c6387965616fbead191ea"
  },
  "ck_btc_ledger_v3": {
    "rev": "2190613d3b5bcd9b74c382b22d151580b8ac271a",
    "sha256": "25071c2c55ad4571293e00d8e277f442aec7aed88109743ac52df3125209ff45"
  },
  "ck_eth_archive": {
    "rev": "512cf412f33d430b79f42330518166d14fc6884e",
    "sha256": "3fafdd895c44886e38199882afcf06efb8e6e0b73af51eca327dcba4da7a0106"
  },
  "ck_eth_index": {
    "rev": "512cf412f33d430b79f42330518166d14fc6884e",
    "sha256": "02dc57b933ea8259e86ce51d10c067cf5939008ecf62e35a25276ff9fa1510b9"
  },
  "ck_eth_ledger": {
    "rev": "512cf412f33d430b79f42330518166d14fc6884e",
    "sha256": "b5a17d640743711184ac16e49608a6590c750d32cda70817b7d43a3a67e7cfdf"
  },
  "ck_eth_ledger_v1": {
    "rev": "d4ee25b0865e89d3eaac13a60f0016d5e3296b31",
    "sha256": "e6072806ae22868ee09c07923d093b1b0b687dba540d22cfc1e1a5392bfcca46"
  },
  "ck_eth_ledger_v2": {
    "rev": "e54d3fa34ded227c885d04e64505fa4b5d564743",
    "sha256": "98a7b7391608dc4a554d6964bad24157b6aaf890a05bbaad3fcc92033d9c7b02"
  },
  "ck_eth_ledger_v2_noledgerversion": {
    "rev": "aba60ffbc46acfc8990bf4d5685c1360bd7026b9",
    "sha256": "73d0c5f057aaf33004218ce588780e1b454c717c702b1cf47532f32c23515f1e"
  },
  "ck_eth_ledger_v3": {
    "rev": "2190613d3b5bcd9b74c382b22d151580b8ac271a",
    "sha256": "9637743e1215a4db376a62ee807a0986faf20833be2b332df09b3d5dbdd7339e"
  },
  "cycles-minting": {
    "rev": "0e4c8234a9e0508ae30c5b8a7498406294c25e95",
    "sha256": "da7dfd088371dc538c0bbde1a2f3e4e3795d2cb58dc16f6c7bf67247b9e51254"
  },
  "cycles_ledger": {
    "sha256": "ed99402535bb4f58e4ab469acc40c903f2fdeea409be16623d5c6a9131cbf120",
    "tag": "cycles-ledger-v1.0.6"
  },
  "cycles_ledger_index": {
    "rev": "83923a194d39835e8a7d9549f9f0831b962a60c2",
    "sha256": "6c406b9dc332f3dc58b823518ab2b2c481467307ad9e540122f17bd9b926c123"
  },
  "genesis-token": {
    "rev": "02571e8215fa3e77da791e693cc238b2de3beae9",
    "sha256": "f897870b7b6d6c15f657496bc731b0e341c34468ddbb8fa0722ec7beb6b51cee"
  },
  "governance": {
    "rev": "51dd253fd8b301f849dfc26f77cff6d15acd04c1",
    "sha256": "59672f1cb908142456a37676b997468d5313941c0d3077ace1bca1e79d9e9a59"
  },
  "governance-canister_test": {
    "rev": "51dd253fd8b301f849dfc26f77cff6d15acd04c1",
    "sha256": "0f8645b1e5419a0c298e8be782dcecf27ae5ede809c00b04d8530f6609313eb0"
  },
  "index": {
    "rev": "3ae3649a2366aaca83404b692fc58e4c6e604a25",
    "sha256": "b443df3315902404b142d60f3cfd2f580181683310f6e6321b52de297deffcda"
  },
  "internet_identity_test": {
    "sha256": "01d02df2fd171660283f6a084cb15badf09079241579bc531a2fd16e9c618fe0",
    "tag": "release-2025-10-14-3"
  },
  "ledger": {
    "rev": "69b755062f5ef0a7d6efc9a127172b46121420c8",
    "sha256": "51f4be010f23064137defacd627ffbec024c5133210c68ca3b80ab8f257101d6"
  },
  "lifeline": {
    "rev": "b5192581ccd35b67fe5a1f795ead9cbcd25956d6",
    "sha256": "8c8eb285de53ca5609abd7dc41ba3ec8eeb67708b81469311fd670e6738d7d0a"
  },
  "nns_dapp_test": {
    "sha256": "f1c12473fbce087ee54837ef6223ba378091f6148fa815e4b14c454baf27293e",
    "tag": "proposal-138910"
  },
  "node-rewards": {
    "rev": "ca959ab71539b277af5f9880e6a4c5779e135cad",
    "sha256": "095d056ef137d3a9b6c387756de2e9701c81dda43bfac94b2be628c8ebe43baa"
  },
  "registry": {
    "rev": "51dd253fd8b301f849dfc26f77cff6d15acd04c1",
    "sha256": "cfeaeae45097df10d9816384e34e4782cbb06adce20f22ade8e9594ae49dc914"
  },
  "root": {
    "rev": "2407385056dbcd1d5eb15b84a9c0cd5f511ff997",
    "sha256": "b296fa62644f46cd30f852835a562a47b9bcf79e5772aa26625ba7e1d878613b"
  },
  "sns-wasm": {
    "rev": "ff761f361981cff6a760b7d181f696551878b92a",
    "sha256": "5906d35741e4c69690b7a881073a7075dfb2525878b82017a92648e40fe3c934"
  },
  "sns_aggregator_test": {
    "sha256": "f4af69470afc76f50bbb845d001f42c135f4a7976c70bff9fe3ea4d86d2216aa",
    "tag": "proposal-137283-agg"
  },
  "sns_archive": {
    "rev": "3f3d9bfac750f82f424185ac5b32a756cfd45ad9",
    "sha256": "cb3f2ecc540f3b4c073f1a4de1fc4a9cef11cb1901405f49f9ef855a53b69e1c"
  },
  "sns_governance": {
    "rev": "ca94383ba3df22dacf7a005186b67303548eb144",
    "sha256": "78e522df253097189e91b97cc169a7fea6de4588884c37394fc80896884270db"
  },
  "sns_index": {
    "rev": "3f3d9bfac750f82f424185ac5b32a756cfd45ad9",
    "sha256": "73eb5d98d6e7020cd99a3430ef1284c05e2a708ae5274ca0387deca7551265e5"
  },
  "sns_ledger": {
    "rev": "3f3d9bfac750f82f424185ac5b32a756cfd45ad9",
    "sha256": "24e6b0b09ba44b1123453877994bb59ce75555c2e33f19f58163a3e0c6e62bd1"
  },
  "sns_ledger_v2": {
    "rev": "e54d3fa34ded227c885d04e64505fa4b5d564743",
    "sha256": "3d808fa63a3d8ebd4510c0400aa078e99a31afaa0515f0b68778f929ce4b2a46"
  },
  "sns_root": {
    "rev": "6ab18899e82ac202fea4a549dcc6c5a5191e4992",
    "sha256": "8ec6c66366c2cce843ae12dc0077a6a748724f848c37ddf8f1b670c99da818ad"
  },
  "swap": {
    "rev": "0e4c8234a9e0508ae30c5b8a7498406294c25e95",
    "sha256": "dad7d704719615f9dc5165ce195b57abfa75a712034d703c5d46a6aec267bdd0"
  }
}


================================================
FILE: mainnet-icos-revisions.json
================================================
{
  "guestos": {
    "subnets": {
      "tdb26-jop6k-aogll-7ltgs-eruif-6kk7m-qpktf-gdiqx-mxtrf-vb5e6-eqe": {
        "version": "45657852c1eca6728ff313808db29b47c862ad13",
        "update_img_hash": "84a17802d839e057727ff09e34f2cba47c129e7ca18f33ed38dbf99740809808",
        "update_img_hash_dev": "56218fa99bb02ef25c44852b66871ced678d683730527b3d0cd5c7733b9bb65b",
        "launch_measurements": {
          "guest_launch_measurements": [
            {
              "measurement": [
                185,
                206,
                172,
                199,
                131,
                227,
                32,
                125,
                143,
                85,
                37,
                42,
                45,
                121,
                121,
                172,
                68,
                118,
                124,
                113,
                51,
                10,
                206,
                70,
                211,
                98,
                168,
                243,
                25,
                111,
                13,
                181,
                111,
                219,
                173,
                241,
                100,
                88,
                123,
                43,
                149,
                139,
                226,
                140,
                253,
                184,
                159,
                28
              ],
              "metadata": {
                "kernel_cmdline": "root=/dev/vda5 console=ttyS0 nomodeset dfinity.system=A security=selinux selinux=1 enforcing=1 root_hash=f64ccef90a694ee56dcb65c0e2c819f4177ed792bef56b0ad5bd61ed5b8817cd"
              }
            },
            {
              "measurement": [
                37,
                162,
                225,
                85,
                106,
                122,
                136,
                6,
                208,
                159,
                181,
                168,
                88,
                123,
                4,
                83,
                60,
                45,
                140,
                221,
                16,
                176,
                26,
                54,
                137,
                176,
                212,
                110,
                143,
                249,
                100,
                58,
                75,
                67,
                98,
                123,
                41,
                201,
                99,
                213,
                198,
                230,
                4,
                91,
                79,
                114,
                44,
                12
              ],
              "metadata": {
                "kernel_cmdline": "root=/dev/vda8 console=ttyS0 nomodeset dfinity.system=B security=selinux selinux=1 enforcing=1 root_hash=f64ccef90a694ee56dcb65c0e2c819f4177ed792bef56b0ad5bd61ed5b8817cd"
              }
            }
          ]
        },
        "launch_measurements_dev": "unimplemented"
      },
      "io67a-2jmkw-zup3h-snbwi-g6a5n-rm5dn-b6png-lvdpl-nqnto-yih6l-gqe": {
        "version": "285897dae3a2cb60c50d9d8dd7327f18c3c372b9",
        "update_img_hash": "bfb66b0afd20458b5fc7ed6a1562c00b094849a6180b567939641aca842e065f",
        "update_img_hash_dev": "8395615628d1f4680d697430f1109e808da6770a6e60543c91140323e23fa9b8",
        "launch_measurements": {
          "guest_launch_measurements": [
            {
              "measurement": [
                8,
                60,
                248,
                64,
                162,
                239,
                140,
                135,
                45,
                127,
                79,
                23,
                18,
                192,
                41,
                160,
                110,
                55,
                97,
                105,
                14,
                109,
                0,
                129,
                110,
                44,
                201,
                37,
                64,
                175,
                184,
                124,
                237,
                82,
                187,
                159,
                248,
                42,
                190,
                108,
                105,
                150,
                248,
                105,
                72,
                17,
                181,
                63
              ],
              "metadata": {
                "kernel_cmdline": "root=/dev/vda5 console=ttyS0 nomodeset dfinity.system=A security=selinux selinux=1 enforcing=1 root_hash=ea37a6557daebe6f5ddbf7d9f126909e77d69802683110df31c2bd23ef8af995"
              }
            },
            {
              "measurement": [
                238,
                64,
                117,
                59,
                245,
                142,
                189,
                222,
                183,
                216,
                35,
                131,
                203,
                110,
                99,
                246,
                3,
                37,
                86,
                197,
                206,
                56,
                222,
                252,
                80,
                120,
                235,
                148,
                220,
                199,
                16,
                36,
                196,
                133,
                139,
                134,
                158,
                136,
                88,
                149,
                229,
                100,
                98,
                50,
                244,
                120,
                191,
                161
              ],
              "metadata": {
                "kernel_cmdline": "root=/dev/vda8 console=ttyS0 nomodeset dfinity.system=B security=selinux selinux=1 enforcing=1 root_hash=ea37a6557daebe6f5ddbf7d9f126909e77d69802683110df31c2bd23ef8af995"
              }
            }
          ]
        },
        "launch_measurements_dev": "unimplemented"
      }
    },
    "latest_release": {
      "version": "285897dae3a2cb60c50d9d8dd7327f18c3c372b9",
      "update_img_hash": "bfb66b0afd20458b5fc7ed6a1562c00b094849a6180b567939641aca842e065f",
      "update_img_hash_dev": "8395615628d1f4680d697430f1109e808da6770a6e60543c91140323e23fa9b8",
      "launch_measurements": {
        "guest_launch_measurements": [
          {
            "measurement": [
              8,
              60,
              248,
              64,
              162,
              239,
              140,
              135,
              45,
              127,
              79,
              23,
              18,
              192,
              41,
              160,
              110,
              55,
              97,
              105,
              14,
              109,
              0,
              129,
              110,
              44,
              201,
              37,
              64,
              175,
              184,
              124,
              237,
              82,
              187,
              159,
              248,
              42,
              190,
              108,
              105,
              150,
              248,
              105,
              72,
              17,
              181,
              63
            ],
            "metadata": {
              "kernel_cmdline": "root=/dev/vda5 console=ttyS0 nomodeset dfinity.system=A security=selinux selinux=1 enforcing=1 root_hash=ea37a6557daebe6f5ddbf7d9f126909e77d69802683110df31c2bd23ef8af995"
            }
          },
          {
            "measurement": [
              238,
              64,
              117,
              59,
              245,
              142,
              189,
              222,
              183,
              216,
              35,
              131,
              203,
              110,
              99,
              246,
              3,
              37,
              86,
              197,
              206,
              56,
              222,
              252,
              80,
              120,
              235,
              148,
              220,
              199,
              16,
              36,
              196,
              133,
              139,
              134,
              158,
              136,
              88,
              149,
              229,
              100,
              98,
              50,
              244,
              120,
              191,
              161
            ],
            "metadata": {
              "kernel_cmdline": "root=/dev/vda8 console=ttyS0 nomodeset dfinity.system=B security=selinux selinux=1 enforcing=1 root_hash=ea37a6557daebe6f5ddbf7d9f126909e77d69802683110df31c2bd23ef8af995"
            }
          }
        ]
      },
      "launch_measurements_dev": "unimplemented"
    }
  },
  "hostos": {
    "latest_release": {
      "version": "285897dae3a2cb60c50d9d8dd7327f18c3c372b9",
      "update_img_hash": "00295e6f99d3420f2ae3cd69e2f2481b9d484eb0ef8f2b6aa930cedb84864272",
      "update_img_hash_dev": "b00984f0133c85af0435cdab5b8ea483280b17c25d063900402d1ce39398feec",
      "launch_measurements": {
        "guest_launch_measurements": [
          {
            "measurement": [
              8,
              60,
              248,
              64,
              162,
              239,
              140,
              135,
              45,
              127,
              79,
              23,
              18,
              192,
              41,
              160,
              110,
              55,
              97,
              105,
              14,
              109,
              0,
              129,
              110,
              44,
              201,
              37,
              64,
              175,
              184,
              124,
              237,
              82,
              187,
              159,
              248,
              42,
              190,
              108,
              105,
              150,
              248,
              105,
              72,
              17,
              181,
              63
            ],
            "metadata": {
              "kernel_cmdline": "root=/dev/vda5 console=ttyS0 nomodeset dfinity.system=A security=selinux selinux=1 enforcing=1 root_hash=ea37a6557daebe6f5ddbf7d9f126909e77d69802683110df31c2bd23ef8af995"
            }
          },
          {
            "measurement": [
              238,
              64,
              117,
              59,
              245,
              142,
              189,
              222,
              183,
              216,
              35,
              131,
              203,
              110,
              99,
              246,
              3,
              37,
              86,
              197,
              206,
              56,
              222,
              252,
              80,
              120,
              235,
              148,
              220,
              199,
              16,
              36,
              196,
              133,
              139,
              134,
              158,
              136,
              88,
              149,
              229,
              100,
              98,
              50,
              244,
              120,
              191,
              161
            ],
            "metadata": {
              "kernel_cmdline": "root=/dev/vda8 console=ttyS0 nomodeset dfinity.system=B security=selinux selinux=1 enforcing=1 root_hash=ea37a6557daebe6f5ddbf7d9f126909e77d69802683110df31c2bd23ef8af995"
            }
          }
        ]
      },
      "launch_measurements_dev": "unimplemented"
    }
  }
}


================================================
FILE: MODULE.bazel
================================================
# Bazel modules for the IC build.
# NOTE: Some dependencies are still listed in the WORKSPACE file. See WORKSPACE
# for details.

module(
    name = "ic",
)

# General Bazel helpers
bazel_dep(name = "bazel_skylib", version = "1.7.1")
bazel_dep(name = "aspect_bazel_lib", version = "2.9.0")

bazel_dep(name = "buildifier_prebuilt", version = "8.2.0.2", dev_dependency = True)

# CC dependencies (for C libs like miracl-core, etc)
bazel_dep(name = "rules_cc", version = "0.0.13")
bazel_dep(name = "platforms", version = "0.0.11")
bazel_dep(name = "hermetic_cc_toolchain", version = "3.1.1")
archive_override(
    module_name = "hermetic_cc_toolchain",
    integrity = "sha256-kHdFv5FVX3foI0wLlTNx5srFunFdHPEv9kFJbdG86dE=",
    patch_strip = 1,
    patches = ["//bazel:hermetic_cc_toolchain.patch"],
    urls = ["https://github.com/uber/hermetic_cc_toolchain/releases/download/v3.1.1/hermetic_cc_toolchain-v3.1.1.tar.gz"],
)

# configure/make dependencies
bazel_dep(name = "rules_foreign_cc", version = "0.14.0")

register_toolchains(
    "@rules_foreign_cc//toolchains:preinstalled_pkgconfig_toolchain",
    "@rules_foreign_cc//toolchains:preinstalled_make_toolchain",
)

# Use HEAD to include this commit which is needed for preinstalled toolchains to work
# https://github.com/bazel-contrib/rules_foreign_cc/commit/d03f7ae79ddda0ad228b17048b9e2dc0efcc8e95
#
# Use a patch to work around determinism issues in make & pkgconfig toolchains
# https://github.com/bazel-contrib/rules_foreign_cc/issues/1313
archive_override(
    module_name = "rules_foreign_cc",
    integrity = "sha384-bTtlZejENu+3rnOsCg1nmSZJl54++7nB0zgzWT+jtZJ1QyMRwkV4ieOaeORQTdjY",
    patch_strip = 1,
    patches = ["//bazel:rules_foreign_cc.patch"],
    strip_prefix = "rules_foreign_cc-77d4483fadbb1b7bcace18ed8e8e87e8791050f6",
    urls = ["https://github.com/bazelbuild/rules_foreign_cc/archive/77d4483fadbb1b7bcace18ed8e8e87e8791050f6.tar.gz"],
)

# Misc tools

bazel_dep(name = "pigz", version = "2.8")  # (parallel) gzip

# Python dependencies

bazel_dep(name = "rules_python", version = "0.35.0")

python_version = "3.12"

python = use_extension("@rules_python//python/extensions:python.bzl", "python")
python.toolchain(python_version = python_version)

pip = use_extension("@rules_python//python/extensions:pip.bzl", "pip")
pip.parse(
    hub_name = "python_deps",
    python_version = python_version,
    requirements_lock = "//:requirements.txt",
)
use_repo(pip, "python_deps")

# Protobuf dependencies
bazel_dep(
    name = "protobuf",
    version = "28.2",
    # We rename it to keep the legacy name used throughout our bazel files
    repo_name = "com_google_protobuf",
)

# Go dependencies

bazel_dep(name = "rules_go", version = "0.50.1")
bazel_dep(name = "gazelle", version = "0.38.0")

go_deps = use_extension("@gazelle//:extensions.bzl", "go_deps")
go_deps.from_file(go_mod = "//:go.mod")

# All direct go deps
go_deps.module(
    path = "github.com/fatih/color",
    sum = "h1:8LOYc1KYPPmyKMuN8QV2DNRWNbLo6LZ0iLs8+mlH53w=",
    version = "v1.13.0",
)
go_deps.module(
    path = "github.com/spf13/cobra",
    sum = "h1:o94oiPyS4KD1mPy2fmcYYHHfCxLqYjJOhGsCHFZtEzA=",
    version = "v1.6.1",
)
use_repo(
    go_deps,
    "com_github_fatih_color",
    "com_github_google_go_cmp",
    "com_github_schollz_closestmatch",
    "com_github_spf13_cobra",
    "com_github_stretchr_testify",
)

# Docker/OCI & archive rules with image definitions
# Mirror image to GHCR if image is in DockerHub:
#   1st PR: add it to the list .github/workflows/container-mirror-images.json
#   2nd PR: add it below

bazel_dep(name = "rules_pkg", version = "1.0.1")
bazel_dep(name = "rules_oci", version = "2.0.0")

oci = use_extension("@rules_oci//oci:extensions.bzl", "oci")

# file server used in tests
oci.pull(
    name = "static-file-server",
    image = "ghcr.io/dfinity/halverneus/static-file-server@sha256:9e46688910b1cf9328c3b55784f08a63c53e70a276ccaf76bfdaaf2fbd0019fa",
    platforms = [
        "linux/amd64",
    ],
)
use_repo(oci, "static-file-server", "static-file-server_linux_amd64")

# bitcoin container used in test
oci.pull(
    name = "bitcoind",
    image = "ghcr.io/dfinity/kylemanna/bitcoind@sha256:17c7dd21690f3be34630db7389d2f0bff14649e27a964afef03806a6d631e0f1",
    platforms = [
        "linux/amd64",
    ],
)
use_repo(oci, "bitcoind", "bitcoind_linux_amd64")

# dogecoin container used in test
oci.pull(
    name = "dogecoind",
    image = "ghcr.io/dfinity/fiftysix/dogecoin-core@sha256:13c8b150b759eed9e11ebaf0edfdf998022bfd622164c8768d7057b43e4e156c",
    platforms = [
        "linux/amd64",
    ],
)
use_repo(oci, "dogecoind", "dogecoind_linux_amd64")

# IC HTTP Gateway container used in tests
oci.pull(
    name = "ic_gatewayd",
    image = "ghcr.io/dfinity/ic-gateway@sha256:0a25e9da712202966447919dfce145f9af5df62e282f2dfe8a7ea226a44b077a",
    platforms = [
        "linux/amd64",
    ],
)
use_repo(oci, "ic_gatewayd", "ic_gatewayd_linux_amd64")

# foundry container used in test
oci.pull(
    name = "foundry",
    image = "ghcr.io/foundry-rs/foundry@sha256:8f9dd6d4c498538b3aa3999758520bca24a41273163b0c7295ed53b1a6062f30",  # v0.3.0 https://github.com/foundry-rs/foundry/pkgs/container/foundry/391862899?tag=v0.3.0
    platforms = [
        "linux/amd64",
    ],
)
use_repo(oci, "foundry", "foundry_linux_amd64")

# nginx-proxy container used in test
oci.pull(
    name = "nginx-proxy",
    image = "ghcr.io/dfinity/nginxproxy/nginx-proxy@sha256:c9ba1ba8a93223305a8bce2ae09024060797698121cd01a48e5cd7462b22faa1",
    platforms = [
        "linux/amd64",
    ],
)
use_repo(oci, "nginx-proxy", "nginx-proxy_linux_amd64")

# Used by tests
oci.pull(
    name = "jaeger",
    image = "ghcr.io/dfinity/jaegertracing/all-in-one@sha256:836e9b69c88afbedf7683ea7162e179de63b1f981662e83f5ebb68badadc710f",
    platforms = [
        "linux/amd64",
    ],
)
use_repo(oci, "jaeger", "jaeger_linux_amd64")

oci.pull(
    name = "vector-with-log-fetcher",
    image = "ghcr.io/dfinity/dre/log-fetcher@sha256:61696bcce605f4ef7b86f606077b47675191ed099f7559b7746281d3bb2a3fba",
    platforms = [
        "linux/amd64",
    ],
)
use_repo(oci, "vector-with-log-fetcher", "vector-with-log-fetcher_linux_amd64")

# Used by tests
oci.pull(
    name = "minica",
    image = "ghcr.io/dfinity/ryantk/minica@sha256:c67e2c1885d438b5927176295d41aaab8a72dd9e1272ba85054bfc78191d05b0",
    platforms = ["linux/amd64"],
)
use_repo(oci, "minica", "minica_linux_amd64")

# used by rosetta image
oci.pull(
    name = "rust_base",
    image = "gcr.io/distroless/cc-debian12@sha256:3310655aac0d85eb9d579792387af1ff3eb7a1667823478be58020ab0e0d97a8",
    platforms = ["linux/amd64"],
)
use_repo(oci, "rust_base", "rust_base_linux_amd64")

# used in various places as base
oci.pull(
    name = "ubuntu_base",
    image = "ghcr.io/dfinity/ubuntu@sha256:6015f66923d7afbc53558d7ccffd325d43b4e249f41a6e93eef074c9505d2233",
    platforms = ["linux/amd64"],
)
use_repo(oci, "ubuntu_base", "ubuntu_base_linux_amd64")

# used by boundary node tests
oci.pull(
    name = "coredns",
    image = "ghcr.io/dfinity/coredns/coredns@sha256:be7652ce0b43b1339f3d14d9b14af9f588578011092c1f7893bd55432d83a378",
    platforms = ["linux/amd64"],
)
use_repo(oci, "coredns", "coredns_linux_amd64")

# used by custom domains tests
oci.pull(
    name = "pebble",
    image = "ghcr.io/dfinity/letsencrypt/pebble@sha256:fc5a537bf8fbc7cc63aa24ec3142283aa9b6ba54529f86eb8ff31fbde7c5b258",
    platforms = ["linux/amd64"],
)
use_repo(oci, "pebble", "pebble_linux_amd64")

oci.pull(
    name = "python3",
    image = "ghcr.io/dfinity/library/python@sha256:0a56f24afa1fc7f518aa690cb8c7be661225e40b157d9bb8c6ef402164d9faa7",
    platforms = ["linux/amd64"],
)
use_repo(oci, "python3", "python3_linux_amd64")

oci.pull(
    name = "alpine_openssl",
    image = "ghcr.io/dfinity/alpine/openssl@sha256:cf89651f07a33d2faf4499f72e6f8b0ee2542cd40735d51c7e75b8965c17af0e",
    platforms = ["linux/amd64"],
)
use_repo(oci, "alpine_openssl", "alpine_openssl_linux_amd64")

# Ubuntu snapshots

bazel_dep(name = "rules_distroless", version = "0.3.8")

apt = use_extension("@rules_distroless//apt:extensions.bzl", "apt")

# Packageset based on an Ubuntu noble snapshot, see manifest file
# for details
# To update, comment out the `lock` field below and run:
#   bazel run @noble//:lock
apt.install(
    name = "noble",
    lock = "//bazel:noble.lock.json",
    manifest = "//bazel:noble.yaml",
)
use_repo(apt, "noble")

# Haskell toolchain for spec_compliance tests

bazel_dep(
    name = "rules_haskell",
    version = "1.0",
)

# patched to work around https://github.com/tweag/rules_haskell/issues/2254
archive_override(
    module_name = "rules_haskell",
    integrity = "sha384-bL1ZOn6DZpfsl4+a3PA9T4OAOJnioHXUhoRR9jChk4oy9ykbMAjCBpZ+AxRdRu7V",
    patch_strip = 1,
    patches = ["//bazel:rules_haskell.patch"],
    strip_prefix = "rules_haskell-1.0",
    urls = ["https://github.com/tweag/rules_haskell/releases/download/v1.0/rules_haskell-1.0.tar.gz"],
)

haskell_toolchains = use_extension(
    "@rules_haskell//extensions:haskell_toolchains.bzl",
    "haskell_toolchains",
)
haskell_toolchains.bindists(version = "9.6.6")

rules_haskell_dependencies = use_extension(
    "@rules_haskell//extensions:rules_haskell_dependencies.bzl",
    "rules_haskell_dependencies",
)
use_repo(
    rules_haskell_dependencies,
    "Cabal",
)

# Stack snapshot. To update, run
# bazel run @stackage-unpinned//:pin
stack_snapshot = use_extension(
    "@rules_haskell//extensions:stack_snapshot.bzl",
    "stack_snapshot",
)
use_repo(
    stack_snapshot,
    "stackage",
    "stackage-unpinned",
)

stack_snapshot.stack_snapshot_json(label = "//:stackage_snapshot.json")
stack_snapshot.snapshot(name = "lts-22.30")

[
    stack_snapshot.package(
        name = pkg,
    )
    for pkg in [
        "QuickCheck",
        "aeson",
        "array",
        "asn1-encoding",
        "asn1-types",
        "async",
        "atomic-write",
        "base",
        "base16",
        "base32",
        "base64-bytestring",
        "binary",
        "bytes",
        "bytestring",
        "case-insensitive",
        "cborg",
        "cereal",
        "conduit",
        "constraints",
        "containers",
        "crypton-connection",
        "crypton-x509",
        "crypton-x509-store",
        "crypton-x509-validation",
        "cryptonite",
        "data-default-class",
        "deepseq",
        "digest",
        "directory",
        "dlist",
        "ed25519",
        "either",
        "entropy",
        "file-embed",
        "filepath",
        "hashable",
        "hex-text",
        "hspec",
        "http-client",
        "http-client-tls",
        "http-types",
        "leb128-cereal",
        "megaparsec",
        "memory",
        "mtl",
        "murmur3",
        "network",
        "network-uri",
        "optparse-applicative",
        "parallel",
        "parser-combinators",
        "prettyprinter",
        "primitive",
        "process",
        "quickcheck-io",
        "random",
        "row-types",
        "safe",
        "scientific",
        "secp256k1-haskell",
        "serialise",
        "split",
        "splitmix",
        "string-conversions",
        "tasty",
        "tasty-ant-xml",
        "tasty-html",
        "tasty-hunit",
        "tasty-quickcheck",
        "tasty-rerun",
        "template-haskell",
        "temporary",
        "text",
        "time",
        "tls",
        "transformers",
        "uglymemo",
        "unordered-containers",
        "utf8-string",
        "vector",
        "wai",
        "wai-cors",
        "wai-extra",
        "warp",
        "wide-word",
        "word8",
        "zlib",
    ]
]

stack_snapshot.package(
    name = "attoparsec",
    components =
        [
            # attoparsec contains an internal library which is not exposed publicly,
            # but required to build the public library, hence the declaration of
            # those 2 components, as well as the explicit declaration of the
            # dependency between them.
            "lib",
            "lib:attoparsec-internal",
        ],
    components_dependencies = {
        "lib:attoparsec": ["lib:attoparsec-internal"],
    },
)

# Single files & archives that are not bazel modules

http_archive = use_repo_rule("@bazel_tools//tools/build_defs/repo:http.bzl", "http_archive")

http_file = use_repo_rule("@bazel_tools//tools/build_defs/repo:http.bzl", "http_file")

http_jar = use_repo_rule("@bazel_tools//tools/build_defs/repo:http.bzl", "http_jar")

# TLA+ tools
http_jar(
    name = "tlaplus_community_modules",
    sha256 = "109e0828d192c33703d5cbc50b5b6e128acd816565616e27b3855949c7baba9c",
    url = "https://github.com/tlaplus/CommunityModules/releases/download/202302091937/CommunityModules-202302091937.jar",
)

http_jar(
    name = "tlaplus_community_modules_deps",
    sha256 = "762c4bdc25a0cb67043411c7f4f062cc2c038631c9c569539df880e0e78d5cf4",
    url = "https://github.com/tlaplus/CommunityModules/releases/download/202302091937/CommunityModules-deps-202302091937.jar",
)

# Apalache model checker for TLA+
http_archive(
    name = "tla_apalache",
    build_file_content = """
package(default_visibility = ["//visibility:public"])

exports_files([ "bin/apalache-mc" ])
    """,
    sha256 = "173a683707c2a639c955328746461cad39e4b8a5adff95e156ce3bf376ba293b",
    strip_prefix = "apalache-0.44.11",
    url = "https://github.com/apalache-mc/apalache/releases/download/v0.44.11/apalache-0.44.11.tgz",
)

# Official WebAssembly test suite.
# To be used for testing libraries that handle canister Wasm code.
http_archive(
    name = "wasm_spec_testsuite",
    build_file = "@//third_party:BUILD.wasmtestsuite.bazel",
    sha256 = "9afc0e7c250b5f0dcf32e9a95860b99a392ab78a653fcf3705778e8a9357f3c4",
    strip_prefix = "testsuite-4f77306bb63151631d84f58dedf67958eb9911b9",
    url = "https://github.com/WebAssembly/testsuite/archive/4f77306bb63151631d84f58dedf67958eb9911b9.tar.gz",
)

# Asset canister

http_file(
    name = "asset_canister",
    downloaded_file_path = "assetstorage.wasm.gz",
    sha256 = "1286960c50eb7a773cfb5fdd77cc238588f39e21f189cc3eb0f35199a99b9c7e",
    url = "https://github.com/dfinity/sdk/raw/0.14.2/src/distributed/assetstorage.wasm.gz",
)

# Asset canister that certifies long assets chunk-wise

http_file(
    name = "long_asset_canister",
    downloaded_file_path = "http_gateway_canister_custom_assets.wasm.gz",
    sha256 = "eedcbf986c67fd4ebe3042094604a9a5703e825e56433e2509a6a4d0384ccf95",
    url = "https://github.com/dfinity/http-gateway/raw/refs/heads/main/examples/http-gateway/canister/http_gateway_canister_custom_assets.wasm.gz",
)

# Old version of wallet canister

http_file(
    name = "wallet_canister_0.7.2",
    downloaded_file_path = "wallet.wasm",
    sha256 = "1404b28b1c66491689b59e184a9de3c2be0dbdd75d952f29113b516742b7f898",
    url = "https://github.com/dfinity/sdk/raw/0.7.2/src/distributed/wallet.wasm",
)

# Bitcoin canister

http_file(
    name = "btc_canister",
    downloaded_file_path = "ic-btc-canister.wasm.gz",
    sha256 = "f18d28cbebf49cbd2b6a3dba8f6da1399b95714c7cf100d3fb31ba9c33941daa",
    url = "https://github.com/dfinity/bitcoin-canister/releases/download/release%2F2024-07-28/ic-btc-canister.wasm.gz",
)

# Bitcoin Adapter Mainnet Data for Integration Test

# The files have been generated by syncing bitcoind client, followed
# by requesting all the desired data using a bash script, such as:
#
# declare -a headers=()
# for h in {0..800000}
# do
#     hash=$(bitcoin-cli getblockhash $h)
#     header=$(bitcoin-cli getblockheader $hash | jq '{version, prev_blockhash:.previousblockhash, merkle_root:.merkleroot, time, bits, nonce}')
#     headers+=("$header")
# done
# echo "[" $(IFS=,; echo "${headers[*]}") "]"

# Contains the first 800_000 headers of the Bitcoin mainnet blockchain.
http_file(
    name = "bitcoin_adapter_mainnet_headers",
    downloaded_file_path = "mainnet_headers_800k.json.gz",
    sha256 = "fb58cd3c0e2efe298eeb96751ae2276e938b39e2f1d9760ea47a9f3a8288d214",
    url = "https://download.dfinity.systems/testdata/mainnet_headers_800k.json.gz",
)

# Contains the first 800_000 headers of the Dogecoin mainnet blockchain with auxiliary proof-of-work.
http_file(
    name = "doge_headers_800k_mainnet_auxpow",
    downloaded_file_path = "doge_headers_800k_mainnet_auxpow.json.gz",
    sha256 = "e138d7c59d237d0eea70943b60038b8755d9aa1f04bc94f315a74775cf8bfc2d",
    url = "http://download.dfinity.systems/testdata/doge/doge_headers_800k_mainnet_auxpow.json.gz",
)

# Contains blocks 350_990 to 350_999 (inclusive) of the Bitcoin mainnet blockchain.
http_file(
    name = "bitcoin_adapter_mainnet_blocks",
    downloaded_file_path = "blocks.json.gz",
    sha256 = "4ba1e7d0b4a2fea5692bb4c79a4c9e077325312a50facef13256ba8d45bc8f2a",
    url = "https://download.dfinity.systems/testdata/blocks.json.gz",
)

# Contains the first 800_000 headers of the Bitcoin testnet blockchain.
http_file(
    name = "bitcoin_adapter_testnet_headers",
    downloaded_file_path = "testnet_headers.json.gz",
    sha256 = "c01542d816d9631a7e59210aea29c1d46ed805b0bcbfd9e067d8d53a5e5919c8",
    url = "https://download.dfinity.systems/testdata/testnet_headers.json.gz",
)

# Contains blocks 350_990 to 350_999 (inclusive) of the Bitcoin testnet blockchain.
http_file(
    name = "bitcoin_adapter_testnet_blocks",
    downloaded_file_path = "testnet_blocks.json.gz",
    sha256 = "ae57b5b58d8a1e89545c0caedeb114d0dd179e0c5fd4a44e43321b2cdbac74fb",
    url = "https://download.dfinity.systems/testdata/testnet_blocks.json.gz",
)

# Contains blocks 0 to 700_000 (inclusive) of the Dogecoin mainnet blockchain.
http_file(
    name = "dogecoin_adapter_mainnet_headers",
    downloaded_file_path = "doge_headers_mainnet_0_700000_raw.csv",
    sha256 = "42c9f72499308f847312cae8fdb5148dcaed71120041a60d336f5628dd93e1fc",
    url = "https://download.dfinity.systems/testdata/doge/doge_headers_mainnet_0_700000_raw.csv",
)

# Contains blocks 1 to 15_000 (inclusive) of the Dogecoin mainnet blockchain in parsed format.
http_file(
    name = "dogecoin_adapter_mainnet_headers_parsed",
    downloaded_file_path = "doge_headers_mainnet_1_15000_parsed.csv",
    sha256 = "a76d47758cf8251e1a524510cd2372287888784f2587cea0834b6af9582b6448",
    url = "https://download.dfinity.systems/testdata/doge/doge_headers_mainnet_1_15000_parsed.csv",
)

# Contains blocks 521_337 to 536_336 (inclusive) of the Dogecoin mainnet blockchain in parsed format.
# Contains 14_955 AuxPow headers out of 15,000 headers.
http_file(
    name = "dogecoin_adapter_mainnet_headers_auxpow_parsed",
    downloaded_file_path = "doge_headers_mainnet_521337_536336_auxpow_parsed.csv",
    sha256 = "248e36cf7962cb0d55f9e8f20d2beecee9611045713d118a6ad6ff8c98cd2070",
    url = "https://download.dfinity.systems/testdata/doge/doge_headers_mainnet_521337_536336_auxpow_parsed.csv",
)

# Contains blocks 0 to 2_000_000 (inclusive) of the Dogecoin testnet blockchain.
http_file(
    name = "dogecoin_adapter_testnet_headers",
    downloaded_file_path = "doge_headers_testnet_0_2000000_raw.csv",
    sha256 = "feefd751b10c174622f9a81bb0777eac035936a6a6f4ec3604fdcfa1201b0e42",
    url = "https://download.dfinity.systems/testdata/doge/doge_headers_testnet_0_2000000_raw.csv",
)

# Contains blocks 1 to 15_000 (inclusive) of the Dogecoin testnet blockchain in parsed format.
http_file(
    name = "dogecoin_adapter_testnet_headers_parsed",
    downloaded_file_path = "doge_headers_testnet_1_15000_parsed.csv",
    sha256 = "39d12178923b95e4f98257aaf537336e42ce1940b6b6c69277164c8c556fd15e",
    url = "https://download.dfinity.systems/testdata/doge/doge_headers_testnet_1_15000_parsed.csv",
)

# Contains blocks 293_100 to 308_099 (inclusive) of the Dogecoin testnet blockchain in parsed format.
# Contains 14_746 AuxPow headers out of 15_000 headers.
http_file(
    name = "dogecoin_adapter_testnet_headers_auxpow_parsed",
    downloaded_file_path = "doge_headers_testnet_293100_308099_auxpow_parsed.csv",
    sha256 = "2aeb78d9564153607900a8e86d2e0fae72fb9fda362c2ce475e60c5d3e43f5e2",
    url = "https://download.dfinity.systems/testdata/doge/doge_headers_testnet_293100_308099_auxpow_parsed.csv",
)

# Internet Identity canister (test build)

http_file(
    name = "ii_dev_canister",
    downloaded_file_path = "internet_identity_dev.wasm.gz",
    sha256 = "2357d822cd451f25c0edab3e45db52ab140a2ac8c4b0170201c78acc5bc11779",
    url = "https://github.com/dfinity/internet-identity/releases/download/release-2024-05-13/internet_identity_dev.wasm.gz",
)

# NNS frontend dapp canister

http_file(
    name = "nns_dapp_canister",
    downloaded_file_path = "nns_dapp_canister.wasm.gz",
    sha256 = "46e5cdad3c77465e938c6ddcc5f41f0ed0b22fd0efc4c9832c3bbd5eee656b0e",
    url = "https://github.com/dfinity/nns-dapp/releases/download/nightly-2025-09-03/nns-dapp_test.wasm.gz",
)

# SNS aggregator canister

http_file(
    name = "sns_aggregator",
    downloaded_file_path = "sns_aggregator_dev.wasm.gz",
    sha256 = "ad1b1a3faa5f5db553198ee68f36361b173cf6cfdc064f7727b2c02a81020c7b",
    url = "https://github.com/dfinity/nns-dapp/releases/download/nightly-2025-09-03/sns_aggregator_dev.wasm.gz",
)

# KongSwap backend canister

http_file(
    name = "kong_backend_canister",
    downloaded_file_path = "kong_backend.wasm.gz",
    sha256 = "bed9a193158348843c8661616c65b152f57f875863f75dad3b75627ad6a2b4fc",
    url = "https://github.com/KongSwap/kong/raw/4bf8f99df53dbd34bef0e55ab6364d85bb31c71a/wasm/kong_backend.wasm.gz",
)

# SNS-KongSwap Adaptor canister (an SNS extension of the TreasuryManager kind)

http_file(
    name = "kongswap-adaptor-canister",
    downloaded_file_path = "kongswap-adaptor-canister.wasm.gz",
    sha256 = "1c07ceba560e7bcffa43d1b5ae97db81151854f068b707c1728e213948212a6c",
    url = "https://github.com/dfinity/sns-kongswap-adaptor/releases/download/v1.0.0/kongswap-adaptor-canister.wasm.gz",
)

# Cycles Ledger canister

http_file(
    name = "cycles-ledger.wasm.gz",
    downloaded_file_path = "cycles-ledger.wasm.gz",
    sha256 = "d2aacbd214f20d752fd1696c2e36d7eceaafe07b932b3ae9e7e5564d1bda0178",
    url = "https://github.com/dfinity/cycles-ledger/releases/download/cycles-ledger-v1.0.3/cycles-ledger.wasm.gz",
)

# Subnet Rental Canister

http_file(
    name = "subnet_rental_canister",
    downloaded_file_path = "subnet_rental_canister.wasm.gz",
    sha256 = "860d30bc74facd4287b7af766004344e4cea4989ae0bf5408b4fb70ff337bc42",
    url = "https://github.com/dfinity/subnet-rental-canister/releases/download/0.3.0/subnet_rental_canister.wasm.gz",
)

# Financial Integration artifacts for upgrade testing

# ic-icrc1-ledger releases without ICRC-3
http_file(
    name = "ic-icrc1-ledger-wo-icrc-3.wasm.gz",
    sha256 = "dc42ffd30d6616068b2dd10023ab0ff8d49b0cbce7582c4673c18caa7412dd3a",
    url = "https://download.dfinity.systems/ic/300dc603a92b5f70dae79229793c902f346af3cc/canisters/ic-icrc1-ledger.wasm.gz",
)

http_file(
    name = "ic-icrc1-ledger-wo-icrc-3-u256.wasm.gz",
    sha256 = "8730fd2aa3b9fe67468fc5e853436f618d0b16be34ff30d7e6492eac55f78d90",
    url = "https://download.dfinity.systems/ic/300dc603a92b5f70dae79229793c902f346af3cc/canisters/ic-icrc1-ledger-u256.wasm.gz",
)

# XC artifacts for testing

# EVM RPC canister

http_file(
    name = "evm_rpc.wasm.gz",
    sha256 = "53ff4625ad3990f22ab8ee1cee85b6ab43cb623e0ca28d3162c41cfac55bd1a6",
    url = "https://github.com/internet-computer-protocol/evm-rpc-canister/releases/download/v2.4.0/evm_rpc.wasm.gz",
)

http_archive(
    name = "haskell-candid",
    sha256 = "bd9d67c3f719ac4cfb5ba339b52dd8ee985b11029a53e97b3cd555334f28d1e3",
    strip_prefix = "haskell-candid-b4ebdea36ad0b7cbf6f69ddbbfffc73434ecb222",
    urls = ["https://github.com/nomeata/haskell-candid/archive/b4ebdea36ad0b7cbf6f69ddbbfffc73434ecb222.tar.gz"],
)

http_archive(
    name = "miracl-core",
    build_file = "@//third_party:BUILD.miracl-core.bazel",
    patch_args = ["-p1"],
    patches = ["//hs/spec_compliance:miracl-core.patch"],
    sha256 = "b93a14c35f56eca8dddaab95dea94294d51ca680a7d6bb1c1f048e1cd71550cd",
    strip_prefix = "core-ec77b2817a0e360b1893affe4cfa44d19391efc7",
    urls = ["https://github.com/miracl/core/archive/ec77b2817a0e360b1893affe4cfa44d19391efc7.tar.gz"],
)

http_archive(
    name = "bitcoin_core_linux_x86",
    build_file_content = """
package(default_visibility = ["//visibility:public"])
filegroup(
    name = "bitcoind",
    srcs = ["bin/bitcoind"],
)
""",
    sha256 = "2a6974c5486f528793c79d42694b5987401e4a43c97f62b1383abf35bcee44a8",
    strip_prefix = "bitcoin-27.0",
    urls = [
        "https://bitcoin.org/bin/bitcoin-core-27.0/bitcoin-27.0-x86_64-linux-gnu.tar.gz",
        "https://bitcoincore.org/bin/bitcoin-core-27.0/bitcoin-27.0-x86_64-linux-gnu.tar.gz",
    ],
)

http_archive(
    name = "bitcoin_core_linux_aarch64",
    build_file_content = """
package(default_visibility = ["//visibility:public"])
filegroup(
    name = "bitcoind",
    srcs = ["bin/bitcoind"],
)
""",
    sha256 = "cb35e250ae9d0328aa90e7aad0b877ed692597420a1092e8ab1a5dd756209722",
    strip_prefix = "bitcoin-27.0",
    urls = [
        "https://bitcoin.org/bin/bitcoin-core-27.0/bitcoin-27.0-aarch64-linux-gnu.tar.gz",
        "https://bitcoincore.org/bin/bitcoin-core-27.0/bitcoin-27.0-aarch64-linux-gnu.tar.gz",
    ],
)

http_archive(
    name = "bitcoin_core_darwin_x86",
    build_file_content = """
package(default_visibility = ["//visibility:public"])
filegroup(
    name = "bitcoind",
    srcs = ["bin/bitcoind"],
)
""",
    sha256 = "e1efd8c4605b2aabc876da93b6eee2bedd868ce7d1f02b0220c1001f903b3e2c",
    strip_prefix = "bitcoin-27.0",
    urls = [
        "https://bitcoin.org/bin/bitcoin-core-27.0/bitcoin-27.0-x86_64-apple-darwin.tar.gz",
        "https://bitcoincore.org/bin/bitcoin-core-27.0/bitcoin-27.0-x86_64-apple-darwin.tar.gz",
    ],
)

http_archive(
    name = "bitcoin_core_darwin_arm64",
    build_file_content = """
package(default_visibility = ["//visibility:public"])
filegroup(
    name = "bitcoind",
    srcs = ["bin/bitcoind"],
)
""",
    sha256 = "1d9d9b837297a73fc7a3b1cfed376644e3fa25c4e1672fbc143d5946cb52431d",
    strip_prefix = "bitcoin-27.0",
    urls = [
        "https://bitcoin.org/bin/bitcoin-core-27.0/bitcoin-27.0-arm64-apple-darwin.tar.gz",
        "https://bitcoincore.org/bin/bitcoin-core-27.0/bitcoin-27.0-arm64-apple-darwin.tar.gz",
    ],
)

http_archive(
    name = "dogecoin_core_linux_x86",
    build_file_content = """
package(default_visibility = ["//visibility:public"])
filegroup(
    name = "dogecoind",
    srcs = ["bin/dogecoind"],
)
""",
    sha256 = "4f227117b411a7c98622c970986e27bcfc3f547a72bef65e7d9e82989175d4f8",
    strip_prefix = "dogecoin-1.14.9",
    url = "https://github.com/dogecoin/dogecoin/releases/download/v1.14.9/dogecoin-1.14.9-x86_64-linux-gnu.tar.gz",
)

http_archive(
    name = "openssl",
    build_file = "@//third_party:BUILD.openssl.bazel",
    integrity = "sha256-4V3agv4v6BOdwqwho21MoB1TE8dfmfRsTooncJtylL8=",
    strip_prefix = "openssl-3.4.0",
    urls = ["https://github.com/openssl/openssl/releases/download/openssl-3.4.0/openssl-3.4.0.tar.gz"],
)

http_archive(
    name = "libfuzzer",
    build_file = "@//third_party:BUILD.libfuzzer.bazel",
    integrity = "sha256-CLw4JzN3fdo8liWeNzL/lsHfmNBHDE+FsWMnTq5of08=",
    strip_prefix = "llvm-project-llvmorg-20.1.0/compiler-rt/lib/fuzzer",
    urls = ["https://github.com/llvm/llvm-project/archive/refs/tags/llvmorg-20.1.0.tar.gz"],
)

http_archive(
    name = "jemalloc",
    build_file = "@//third_party:BUILD.jemalloc.bazel",
    sha256 = "ef6f74fd45e95ee4ef7f9e19ebe5b075ca6b7fbe0140612b2a161abafb7ee179",
    strip_prefix = "jemalloc-5.3.0",
    urls = [
        "https://github.com/jemalloc/jemalloc/archive/refs/tags/5.3.0.tar.gz",
    ],
)

http_archive(
    name = "lmdb",
    build_file = "@//third_party:BUILD.lmdb.bazel",
    sha256 = "d424c1eb841d0b78b91994b6ddef31aa6a3300727b9d9e7868033edfca0f142c",
    strip_prefix = "openldap-OPENLDAP_REL_ENG_2_5_9/libraries/liblmdb",
    urls = [
        "https://github.com/openldap/openldap/archive/refs/tags/OPENLDAP_REL_ENG_2_5_9.zip",
    ],
)

http_file(
    name = "bitcoin_example_canister",
    downloaded_file_path = "basic_bitcoin.wasm.gz",
    sha256 = "e5560c9f7ca3f06c62b0169ffdc5b568ce0eb48e600a1ab893fdeb7414117dc2",
    url = "https://github.com/dfinity/examples/releases/download/rust-basic-bitcoin-25-10-10/basic_bitcoin.wasm.gz",
)

http_file(
    name = "pocket-ic-mainnet-gz",
    # Calculate this hash using:
    # git checkout $revision
    # ci/container/container-run.sh
    # bazel build //publish/binaries:pocket-ic.gz
    # sha256sum bazel-bin/publish/binaries/pocket-ic.gz
    sha256 = "237272216498074e5250a0685813b96632963ff9abbc51a7030d9b625985028d",
    url = "https://github.com/dfinity/pocketic/releases/download/9.0.1/pocket-ic-x86_64-linux.gz",
)

http_archive(
    name = "rclone-x86_64-darwin",
    build_file = "@//third_party:BUILD.rclone.bazel",
    sha256 = "0a9a107ba911bf6bd3c61ad5160a379ac7c81a1b63498e12efcccb4676c65361",
    strip_prefix = "rclone-v1.69.2-osx-amd64",
    url = "https://github.com/rclone/rclone/releases/download/v1.69.2/rclone-v1.69.2-osx-amd64.zip",
)

http_archive(
    name = "rclone-x86_64-linux",
    build_file = "@//third_party:BUILD.rclone.bazel",
    sha256 = "14aaed7163df57894c96f8aca94757f19065f9cb3cb8a84ff9c33234271e1d69",
    strip_prefix = "rclone-v1.69.2-linux-amd64",
    url = "https://github.com/rclone/rclone/releases/download/v1.69.2/rclone-v1.69.2-linux-amd64.zip",
)

# Management canister candid interface

http_file(
    name = "management_canister_did",
    downloaded_file_path = "ic.did",
    sha256 = "0e92d8b9c2cf3d3fca166b76b2d3b8a2464d9b2b61117d8b2f63222b388d8dd1",
    url = "https://raw.githubusercontent.com/dfinity/portal/78c93aa37ef17dc67484079d1a4bf58a10a63106/docs/references/_attachments/ic.did",
)

# Mozilla CA certificate store in PEM format
http_file(
    name = "mozilla_root_ca_store",
    downloaded_file_path = "cacert.pem",
    sha256 = "bb1782d281fe60d4a2dcf41bc229abe3e46c280212597d4abcc25bddf667739b",
    url = "https://curl.se/ca/cacert-2024-11-26.pem",
)

mainnet_icos_versions = use_repo_rule("//bazel:mainnet-icos-versions.bzl", "mainnet_icos_versions")

mainnet_icos_versions(
    name = "mainnet_icos_versions",
    path = "//:mainnet-icos-revisions.json",
)

os_info = use_repo_rule("//bazel:os_info.bzl", "os_info")

os_info(name = "os_info")

# Set up hermetic cc toolchains for binaries and canisters
toolchains = use_extension("@hermetic_cc_toolchain//toolchain:ext.bzl", "toolchains")

# The use_hermetic_cc setting is added to each toolchain to allow us to opt out
# in favor of system toolchains, when needed.
toolchains.extra_settings(settings = ["//bazel:use_hermetic_cc"])
use_repo(toolchains, "zig_sdk")

register_toolchains(
    # Linux toolchains
    "@zig_sdk//toolchain:linux_amd64_gnu.2.31",

    # macOS toolchains
    # Do not use hermetic toolchains for macOS until we have had a chance to
    # wrap the various system libraries.
    # "@zig_sdk//toolchain:darwin_amd64",
    # "@zig_sdk//toolchain:darwin_arm64",

    # wasm toolchains
    "@zig_sdk//toolchain:none_wasm",

    # These toolchains are only registered locally.
    dev_dependency = True,
)

new_local_repository = use_repo_rule("@bazel_tools//tools/build_defs/repo:local.bzl", "new_local_repository")

# Use libsystemd from the host environment
# Ideally this is pulled hermetically, but as this is only used by infogetty
# within IC images, we have less compatibility to worry about, and packaging it
# this way is easier.
new_local_repository(
    name = "libsystemd",
    build_file_content = """
cc_import(
    name = "libsystemd-internal",
    hdrs = glob(["include/systemd/*.h"]),
    interface_library = "lib/x86_64-linux-gnu/libsystemd.so",
    system_provided = True,
    visibility = ["//visibility:private"],
)

# Use an extra cc_library to hide the depth of the include folder
cc_library(
    name = "libsystemd",
    includes = ["include"],
    deps = ["libsystemd-internal"],
    visibility = ["//visibility:public"],
)
""",
    path = "/usr",
)

# Use libvirt from the host environment
# Used for managing virtual machines programmatically.
new_local_repository(
    name = "libvirt",
    build_file_content = """
cc_import(
    name = "libvirt",
    hdrs = glob(["include/libvirt/*.h"]),
    shared_library = "lib/x86_64-linux-gnu/libvirt.so",
    visibility = ["//visibility:public"],
)
""",
    path = "/usr",
)



================================================
FILE: PULL_REQUEST_BAZEL_TARGETS
================================================
# What?
# This file specifies which additional bazel targets to test for which specific file changes.
#
# Why?
# On PRs not tagged with CI_ALL_BAZEL_TARGETS, which is the default,
# CI only tests bazel targets which have modified inputs
# and excludes tests tagged as 'long_test'.
#
# This is problematic for targets that don't explicitly depend
# on modified inputs like the shell format check //pre-commit:shfmt-check
# for example which implicitly depends on the whole workspace.
#
# Furthermore, from experience we know that modifications to certain
# files or directories have a high risk of breaking certain long_tests.
# In those cases folks have traditionally labelled their PRs with CI_ALL_BAZEL_TARGETS
# to run *all* long_tests. This however is often overkill and often forgotten.
#
# To address both issues this file can specify which bazel
# targets to explicitly test based on which files were modified
# disregarding whether they're long_tests.
#
# How?
# The syntax is similar but not the same as gitignore
# (https://git-scm.com/docs/gitignore#_pattern_format):
# * A blank line matches no files, so it can serve as a separator for readability.
# * A line starting with # serves as a comment.
#   Put a backslash ("\") in front of the first hash for patterns that begin with a hash.
# * A pattern is separated by whitespace from its set of targets which are whitespace separated.
# * Patterns and targets don't support spaces in them. This might be added later if desired.
# * Targets can span multiple lines for readability.
#   So the targets on a line starting with whitespace
#   are considered part of the previous set of targets.
# * Unlike gitignore, patterns use simple globbing semantics:
#   https://docs.python.org/3/library/fnmatch.html
#   So there's no negation of patterns or special directory handling
#   like gitignore wildmatch patterns.
#
# Tip: to test which files matches <PATTERN> you can use the following:
# git ls-files | python -c 'import sys, fnmatch; print("\n".join(fnmatch.filter(sys.stdin.read().splitlines(),"<PATTERN>")))'
#
# Run `ci/scripts/targets.py check` to see the matches for every pattern
# and to check the correctness of this file. This is also run automatically by
# the check-pull-request-bazel-targets job on CI.

*.sh        //pre-commit:shfmt-check
*.py        //pre-commit:ruff-lint
*.hs        //pre-commit:ormolu-lint
*.proto     //pre-commit:protobuf-format-check
            //pre-commit:buf-breaking

mainnet-icos-revisions.json
  //rs/tests/consensus/backup:backup_manager_downgrade_test_colocate
  //rs/tests/consensus/backup:backup_manager_upgrade_test_colocate
  //rs/tests/consensus/upgrade:upgrade_downgrade_app_subnet_test_colocate

ic-os/guestos/context/docker-base.* //rs/tests/node:kill_start_long_test

# Run most nested tests for any ic-os/ or rs/ic_os/ changes:
*ic[-_]os/* //rs/tests/nested:guestos_upgrade_smoke_test
            //rs/tests/nested:guestos_upgrade_from_latest_release_to_current
            //rs/tests/nested:hostos_upgrade_smoke_test
            //rs/tests/nested:hostos_upgrade_from_latest_release_to_current
            //rs/tests/nested:registration

# Full rejoin test for state sync changes:
*state_sync* //rs/tests/message_routing:rejoin_test

# XNet compatibility test for changes to certifications
*canonical_state/* //rs/tests/message_routing/xnet:xnet_compatibility

rs/nns/cmc/*.rs //rs/tests/nns:nns_cycles_minting_test

*nns_delegation* //rs/tests/networking:nns_delegation_mainnet_nns_version_test
                 //rs/tests/networking:nns_delegation_branch_nns_version_test

rs/backup/*.rs //rs/tests/consensus/backup/...

rs/orchestrator/*.rs //rs/tests/consensus/orchestrator/...

*guestos-recovery-engine* //rs/tests/consensus:guestos_recovery_engine_smoke_test
                          //rs/tests/nested/nns_recovery/...

*guestos-recovery-upgrader* //rs/tests/nested/nns_recovery/...

rs/recovery/src/*.rs //rs/tests/consensus/subnet_recovery/...
                     //rs/tests/consensus:subnet_splitting_test_colocate
                     //rs/tests/nested/nns_recovery/...

rs/recovery/subnet_splitting/*.rs //rs/tests/consensus:subnet_splitting_test_colocate

rs/bitcoin/* //rs/tests/ckbtc:btc_adapter_basics_test
             //rs/tests/ckbtc:doge_adapter_basics_test
             //rs/tests/execution:btc_get_balance_test_colocate

rs/boundary_node/rate_limits/* //rs/tests/boundary_nodes:rate_limit_canister_test



================================================
FILE: pyproject.toml
================================================
[tool.ruff]

# Exclude a variety of commonly ignored directories.
exclude = [
    ".bzr",
    ".direnv",
    ".eggs",
    ".git",
    ".hg",
    ".mypy_cache",
    ".nox",
    ".pants.d",
    ".pytype",
    ".ruff_cache",
    ".svn",
    ".tox",
    ".venv",
    "__pypackages__",
    "_build",
    "buck-out",
    "build",
    "dist",
    "node_modules",
    "venv",
    "env",
]

line-length = 120

target-version = "py37"

[tool.isort]
profile = "black"
remove_redundant_aliases = true

[tool.ruff.lint]

# Allow unused variables when underscore-prefixed.
dummy-variable-rgx = "^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$"

# Allow autofix for all enabled rules (when `--fix`) is provided.
fixable = [
    "A", 
    "B", 
    "C", 
    "D", 
    "E", 
    "F", 
    "G", 
    "I", 
    "N", 
    "Q", 
    "S", 
    "T", 
    "W", 
    "ANN", 
    "ARG", 
    "BLE", 
    "COM", 
    "DJ", 
    "DTZ", 
    "EM", 
    "ERA", 
    "EXE", 
    "FBT", 
    "ICN", 
    "INP", 
    "ISC", 
    "NPY", 
    "PD", 
    "PGH", 
    "PIE", 
    "PL", 
    "PT", 
    "PTH", 
    "PYI", 
    "RET", 
    "RSE", 
    "RUF", 
    "SIM", 
    "SLF", 
    "TCH", 
    "TID", 
    "TRY", 
    "UP", 
    "YTT",
]

unfixable = []

select = [
    "C9", # Mccabe
    "F", # Pyflakes
    "I", # Isort
    "N", # Pep-8 naming
    "D", # Pydocstyle
    "E", # Pydocstyle error
    "W", # Pydocstyle warning
]

ignore = [
    "C901",
    "D10",
    "D100",
    "D101", 
    "D102",
    "D103", 
    "D104", 
    "D107", 
    "D202", 
    "D203", 
    "D205", 
    "D212", 
    "D400", 
    "D401",
    "D407", 
    "D415",
    "D416",
    "D417",
    "E402",
    "E501",
    "E501",
    "E713",
    "F403",
    "N801",
    "N802",
    "N803",
    "N804",
    "N805",
    "N806",
    "N818",
    "N999",
]

[tool.ruff.lint.mccabe]
# Unlike Flake8, default to a complexity level of 10.
max-complexity = 10


================================================
FILE: requirements.in
================================================
ansible>=6.6.0
cbor>=1.0.0
configargparse
cryptography>=39.0.2
cvss>=2.5
fabric
GitPython>=3.1.24
https://github.com/rocklabs-io/ic-py/archive/53c375a1d6c1d09e8d24588142dece550b801cef.zip
idracredfishsupport>=0.0.8
invoke
jira>=3.10.5
junit-xml
loguru
# Remove matplotlib as it does not appear to be actively used, only for one-off plotting
# It requires a lot of complex dependencies so it is easier to just install it manually when necessary to plot something
# matplotlib>=3.6.2, <3.7
mypy
nested_lookup>=0.2.25
node_semver>=0.9.0
packaging>=21.3
paramiko>=2.11.0
parse>=1.19.0
pyjwt>=2.6.0
pytest
pytest-cov
PyYAML>=6.0.2
requests>=2.26.0
sev-snp-measure
simple-parsing
tqdm
uuid
PyGithub>=2.3.0
python_gitlab
# packages not directly used, but required by other packages and need to be manually version pinned for python 3.12
typing_extensions>=4.6.0



================================================
FILE: rust-toolchain.toml
================================================
[toolchain]
channel = "1.89.0"
targets = ["wasm32-unknown-unknown"]
profile = "default"
components = []



================================================
FILE: rustfmt.toml
================================================
edition = "2024"
style_edition = "2024"



================================================
FILE: SECURITY.adoc
================================================
# Security Policy

The security of the Internet Computer is essential to its success. If you
believe you have found a security vulnerability please report it to us as
described below.

## Reporting a Vulnerability

Please do not report security vulnerabilities and other bugs through public
GitLab issues. Instead, report them to DFINITY via email at:
bugs@dfinity.org. Please do not submit any third party code - only textual 
descriptions. Any comments, suggestions and recommendations you submit to 
us will be assigned to the DFINITY Foundation.

Thank you very much for taking the time to read and analyze the code of the
Internet Computer. This is very much appreciated.


================================================
FILE: SECURITY.md
================================================
# Security Policy

DFINITY takes the security of our software products seriously, which includes all source code repositories under the [DFINITY](https://github.com/dfinity) GitHub organization.

> [!IMPORTANT]
> [DFINITY Foundation](https://dfinity.org) has a [Internet Computer (ICP) Bug Bounty program](https://dfinity.org/bug-bounty/) that rewards researchers for finding and reporting vulnerabilities in the Internet Computer. Please check the scope and eligibility criteria outlined in the policy to see if the vulnerability you found qualifies for a reward.

## How to report a vulnerability

We appreciate your help in keeping our projects secure.
If you believe you have found a security vulnerability in any of our repositories, please report it responsibly to us as described below:

1. **Do not disclose the vulnerability publicly.** Public disclosure could be exploited by attackers before it can be fixed.
2. **Ideally, disclose the vulnerability through [Hackenproof](https://hackenproof.com/programs/internet-computer-protocol)**
    * Hackenproof facilitates disclosure and streamlines Bugbounty payouts.
4. **Alternatively, send an email to securitybugs@dfinity.org.** Please include the following information in your email:
    * A description of the vulnerability
    * Steps to reproduce the vulnerability
    * Risk rating of the vulnerability
    * Any other relevant information

We will respond to your report within 72 hours and work with you to fix the vulnerability as soon as possible.

### Security Updates

We are committed to fixing security vulnerabilities in a timely manner. Once a security vulnerability is reported, we will:

* Investigate the report and confirm the vulnerability.
* Develop a fix for the vulnerability.
* Release a new version of the project that includes the fix.
* Announce the security fix in the project's release notes.

## Preferred Language

We prefer all communications to be in English.

## Disclaimer

This security policy is subject to change at any time.



================================================
FILE: WORKSPACE.bazel
================================================
workspace(
    name = "ic",
)

load("@bazel_tools//tools/build_defs/repo:http.bzl", "http_archive")
load("//bazel:mainnet-canisters.bzl", "canisters")
load("//bazel:mainnet-icos-images.bzl", "get_mainnet_guestos_images", "get_mainnet_setupos_images")

# We cannot derive the Bazel repository names (e.g. @mainnet_nns_registry_canister) directly
# from the canister names because we use inconsistent repo names. Same goes for filenames.
# For each new canister defined in the source JSON file (see `path`) a mapping can be added
# from canister name (as per JSON) to filename (and to repo name, resp.).
canisters(
    name = "canisters",
    filenames = {
        "registry": "registry-canister.wasm.gz",
        "governance": "governance-canister.wasm.gz",
        "governance-canister_test": "governance-canister_test.wasm.gz",
        "ledger": "ledger-canister_notify-method.wasm.gz",
        "archive": "ledger-archive-node-canister.wasm.gz",
        "index": "ic-icp-index-canister.wasm.gz",
        "root": "root-canister.wasm.gz",
        "lifeline": "lifeline_canister.wasm.gz",
        "genesis-token": "genesis-token-canister.wasm.gz",
        "cycles-minting": "cycles-minting-canister.wasm.gz",
        "sns-wasm": "sns-wasm-canister.wasm.gz",
        "ck_btc_archive": "ic-icrc1-archive.wasm.gz",
        "ck_btc_ledger": "ic-icrc1-ledger.wasm.gz",
        "ck_btc_ledger_v1": "ic-icrc1-ledger.wasm.gz",
        "ck_btc_ledger_v2": "ic-icrc1-ledger.wasm.gz",
        "ck_btc_ledger_v2_noledgerversion": "ic-icrc1-ledger.wasm.gz",
        "ck_btc_ledger_v3": "ic-icrc1-ledger.wasm.gz",
        "ck_btc_index": "ic-icrc1-index-ng.wasm.gz",
        "ck_eth_archive": "ic-icrc1-archive-u256.wasm.gz",
        "ck_eth_ledger": "ic-icrc1-ledger-u256.wasm.gz",
        "ck_eth_ledger_v1": "ic-icrc1-ledger-u256.wasm.gz",
        "ck_eth_ledger_v2": "ic-icrc1-ledger-u256.wasm.gz",
        "ck_eth_ledger_v2_noledgerversion": "ic-icrc1-ledger-u256.wasm.gz",
        "ck_eth_ledger_v3": "ic-icrc1-ledger-u256.wasm.gz",
        "ck_eth_index": "ic-icrc1-index-ng-u256.wasm.gz",
        "sns_root": "sns-root-canister.wasm.gz",
        "sns_governance": "sns-governance-canister.wasm.gz",
        "swap": "sns-swap-canister.wasm.gz",
        "sns_ledger": "ic-icrc1-ledger.wasm.gz",
        "sns_ledger_v2": "ic-icrc1-ledger.wasm.gz",
        "sns_archive": "ic-icrc1-archive.wasm.gz",
        "sns_index": "ic-icrc1-index-ng.wasm.gz",
        "node-rewards": "node-rewards-canister.wasm.gz",
        "internet_identity_test": "internet_identity_dev.wasm.gz",
        "nns_dapp_test": "nns-dapp_test.wasm.gz",
        "sns_aggregator_test": "sns_aggregator_dev.wasm.gz",
        "cycles_ledger": "cycles-ledger.wasm.gz",
        "cycles_ledger_index": "ic-icrc1-index-ng-u256.wasm.gz",
        "bitcoin_testnet": "ic-btc-canister.wasm.gz",
    },
    path = "//:mainnet-canister-revisions.json",
    reponames = {
        "registry": "mainnet_nns_registry_canister",
        "governance": "mainnet_nns_governance_canister",
        "governance-canister_test": "nns_governance_canister_test_at_mainnet_commit",
        "ledger": "mainnet_icp_ledger_canister",
        "archive": "mainnet_icp_ledger-archive-node-canister",
        "index": "mainnet_icp_index_canister",
        "root": "mainnet_nns_root-canister",
        "lifeline": "mainnet_nns_lifeline_canister",
        "genesis-token": "mainnet_nns_genesis-token-canister",
        "cycles-minting": "mainnet_nns_cycles-minting-canister",
        "sns-wasm": "mainnet_nns_sns-wasm-canister",
        "ck_btc_archive": "mainnet_ckbtc_ic-icrc1-archive",
        "ck_btc_ledger": "mainnet_ckbtc_ic-icrc1-ledger",
        "ck_btc_ledger_v1": "mainnet_ckbtc_ic-icrc1-ledger-v1",
        "ck_btc_ledger_v2": "mainnet_ckbtc_ic-icrc1-ledger-v2",
        "ck_btc_ledger_v2_noledgerversion": "mainnet_ckbtc_ic-icrc1-ledger-v2-noledgerversion",
        "ck_btc_ledger_v3": "mainnet_ckbtc_ic-icrc1-ledger-v3",
        "ck_btc_index": "mainnet_ckbtc-index-ng",
        "ck_eth_archive": "mainnet_cketh_ic-icrc1-archive-u256",
        "ck_eth_ledger": "mainnet_cketh_ic-icrc1-ledger-u256",
        "ck_eth_ledger_v1": "mainnet_cketh_ic-icrc1-ledger-u256-v1",
        "ck_eth_ledger_v2": "mainnet_cketh_ic-icrc1-ledger-u256-v2",
        "ck_eth_ledger_v2_noledgerversion": "mainnet_cketh_ic-icrc1-ledger-u256-v2-noledgerversion",
        "ck_eth_ledger_v3": "mainnet_cketh_ic-icrc1-ledger-u256-v3",
        "ck_eth_index": "mainnet_cketh-index-ng",
        "sns_root": "mainnet_sns-root-canister",
        "sns_governance": "mainnet_sns-governance-canister",
        "swap": "mainnet_sns-swap-canister",
        "sns_ledger": "mainnet_ic-icrc1-ledger",
        "sns_ledger_v2": "mainnet_ic-icrc1-ledger-v2",
        "sns_archive": "mainnet_ic-icrc1-archive",
        "sns_index": "mainnet_ic-icrc1-index-ng",
        "node-rewards": "mainnet_node-rewards-canister",
        "internet_identity_test": "internet_identity_dev_at_mainnet_commit",
        "nns_dapp_test": "nns_dapp_test_at_mainnet_commit",
        "sns_aggregator_test": "sns_aggregator_test_at_mainnet_commit",
        "cycles_ledger": "mainnet_cycles_ledger_canister",
        "cycles_ledger_index": "mainnet_cycles_ledger_index",
        "bitcoin_testnet": "bitcoin_testnet_canister_at_mainnet_commit",
    },
    repositories = {
        "cycles_ledger": "dfinity/cycles-ledger",
        "internet_identity_test": "dfinity/internet-identity",
        "nns_dapp_test": "dfinity/nns-dapp",
        "sns_aggregator_test": "dfinity/nns-dapp",
        "bitcoin_testnet": "dfinity/bitcoin-canister",
    },
)

load("@canisters//:defs.bzl", "canister_deps")

canister_deps()

http_archive(
    name = "rules_rust",
    integrity = "sha256-2GH766nwQzOgrmnkSO6D1pF/JC3bt/41xo/CEqarpUY=",
    urls = ["https://github.com/bazelbuild/rules_rust/releases/download/0.64.0/rules_rust-0.64.0.tar.gz"],
)

load("@rules_rust//crate_universe:repositories.bzl", "crate_universe_dependencies")
load("@rules_rust//rust:repositories.bzl", "rules_rust_dependencies", "rust_register_toolchains")
load("@rules_rust//tools/rust_analyzer:deps.bzl", "rust_analyzer_dependencies")

rules_rust_dependencies()

rust_analyzer_dependencies()

crate_universe_dependencies()

rust_register_toolchains(
    edition = "2024",
    extra_rustc_flags = {
        # We need to use lld on aarch64-linux because the default gold linker runs into the following bug when linking pocket-ic-server:
        # /usr/bin/ld.gold: internal error in update_erratum_insn, at ../../gold/aarch64.cc:1003
        "aarch64-unknown-linux-gnu": [
            "-C",
            "link-arg=-fuse-ld=lld",
        ],
    },
    strip_level = {"x86_64-unknown-linux-gnu": {
        "dbg": "none",
        "fastbuild": "none",
        "opt": "none",
    }},
    versions = ["1.89.0"],
)

load("//bazel:external_crates.bzl", "external_crates_repository")

external_crates_repository(
    name = "crate_index",
    cargo_lockfile = "//:Cargo.Bazel.toml.lock",
    lockfile = "//:Cargo.Bazel.json.lock",
)

load("@crate_index//:defs.bzl", "crate_repositories")

crate_repositories()

# Motoko support

http_archive(
    name = "rules_motoko",
    sha256 = "f7cb0a906c8efe9d2ad8d27f0f6ac11f6409a771d74874f7e47d45959063dfe3",
    strip_prefix = "rules_motoko-0.2.1",
    urls = ["https://github.com/dfinity/rules_motoko/archive/refs/tags/v0.2.1.tar.gz"],
)

load("@rules_motoko//motoko:repositories.bzl", "rules_motoko_dependencies")

rules_motoko_dependencies()

# Rosetta CLI

load("//bazel:rosetta_cli.bzl", "rosetta_cli_repository")

rosetta_cli_repository(name = "rosetta-cli")

# trivy binary for vulnerability scanning
load("//bazel:trivy.bzl", "trivy_scan")

trivy_scan(name = "trivy")

# shfmt binary for fast shell formatting
load("//bazel:shfmt.bzl", "shfmt")

shfmt(name = "shfmt")

# ruff binary for fast python linting
load("//pre-commit:ruff.bzl", "ruff")

ruff(name = "ruff")

# ormolu binary for haskell linting
load("//pre-commit:ormolu.bzl", "ormolu")

ormolu(name = "ormolu")

# buf binary for protobuf linting
load("//pre-commit:buf.bzl", "buf")

buf(name = "buf")

# candid binary
load("//pre-commit:candid.bzl", "candid")

candid(name = "candid")

# dfx binary for haskell linting
load("//bazel:dfx.bzl", "dfx")

dfx(name = "dfx")

# sns-quill for SNS testing
load("//bazel:sns_quill.bzl", "sns_quill")

sns_quill(name = "sns_quill")

# idl2json for SNS testing
load("//bazel:idl2json.bzl", "idl_to_json")

idl_to_json(name = "idl2json")

# Mainnet ICOS image support
get_mainnet_setupos_images()

get_mainnet_guestos_images()



================================================
FILE: .bazelignore
================================================
target



================================================
FILE: .bazelrc
================================================
# import default bazelrc fragments, see bazel/conf/README.md
import %workspace%/bazel/conf/.bazelrc.build
import %workspace%/bazel/conf/.bazelrc.internal
try-import %workspace%/user.bazelrc



================================================
FILE: .bazelversion
================================================
7.6.1



================================================
FILE: .dockerignore
================================================
**/.git



================================================
FILE: .editorconfig
================================================
# See https://github.com/editorconfig/editorconfig/wiki/EditorConfig-Properties

root = true

[*]
charset = utf-8
end_of_line = lf
indent_style = space
insert_final_newline = true
trim_trailing_whitespace = true

# See https://doc.rust-lang.org/beta/style-guide/index.html#indentation-and-line-width
[*.rs]
indent_size = 4
max_line_length = 100

# See https://bazel.build/build/style-guide
[*.bazel]
indent_size = 4
quote_type = double

# See https://peps.python.org/pep-0008/
[*.py]
indent_size = 4

# See https://protobuf.dev/programming-guides/style/
[*.proto]
indent_size = 2
quote_type = double
max_line_length = 80

[Makefile]
indent_size = 8
indent_style = tab

[*.{json,yaml,yml}]
indent_style = space
indent_size = 2



================================================
FILE: .pre-commit-config.yaml
================================================
repos:
- repo: local
  hooks:
  - id: bazel_buildifier
    name: Auto format Bazel build files.
    stages: [commit]
    entry: bazel run //:buildifier
    files: \.bazel|\.bzl$
    language: system

  - id: bazel_shfmt_format
    name: Auto format shell files
    stages: [commit]
    entry: bazel run //:shfmt-format
    files: \.sh$
    language: system

  - id: bazel_ruff_format
    name: Auto format Python files
    stages: [commit]
    entry: bazel run //:ruff-format
    files: \.py$
    language: system

  - id: bazel_protobuf_format
    name: Auto format Protobuf files
    stages: [commit]
    entry: bazel run //:protobuf-format
    files: \.proto$
    language: system

  - id: bazel_ormolu_format
    name: Auto format Haskell files
    stages: [commit]
    entry: bazel run //:ormolu-format
    files: \.hs|\.lhs$
    language: system

  - id: bazel_rust_format
    name: Auto format Rust files
    stages: [commit]
    entry: bazel run //:rustfmt
    files: \.rs$
    language: system

  - id: bazel_smoke
    name: Run all bazel test smoke targets
    entry: bazel test --config=precommit //...
    pass_filenames: false
    language: system
    always_run: true
    verbose: true

  - id: bazel_rust_format_check
    name: Run rust format check
    entry: bazel build --aspects=@rules_rust//rust:defs.bzl%rustfmt_aspect --output_groups=rustfmt_checks //...
    pass_filenames: false
    language: system
    always_run: true
    verbose: true

- repo: https://github.com/ansible/ansible-lint.git
  rev: v24.5.0
  hooks:
    - id: ansible-lint
      always_run: false
      files: (^|/)testnet/ansible/.+\.(yaml|yml)$
      args: ['-i', 'testnet/ansible/.ansible-lint-ignore', 'testnet/ansible']



================================================
FILE: bazel/README.md
================================================
# Scope

This document helps developers understand how to start with Bazel and port
Cargo development workflows over to Bazel. While this guide focuses on Rust
development, Bazel is a polygot build system: See the “Language Specific
Guides” section below.

If you know C++ or Java, the best introductory reading material might be the
official guides for those languages (linked at the bottom of
[this](https://bazel.build/about/intro) page). Unfortunately, there is no Rust
analog, but the main concepts are language-independent, so it might be a good
idea to start with the C++ or Java guides.

# Setup

We recommend a Linux machine although MacOS works for Rust development but may
require some package installation.

**Do NOT use nix-shell.**

**Do NOT use direnv.**

**DO NOT use apt-get.**

To install Bazel follow the instructions on
[https://github.com/bazelbuild/bazelisk](https://github.com/bazelbuild/bazelisk).
IDX recommends and only supports `bazelisk` - do not install the `bazel`
package with native package managers such as apt get, homebrew, etc..

To build Rust and IC-OS code will require a minimal set of packages. On a Linux
host the
[`Dockerfile`](https://github.com/dfinity/ic/blob/master/ci/container/Dockerfile)
serves as a reference for the minimal apt installation set. Developers may
develop inside the build and development container with
`./ci/container/container-run.sh`.

```bash
bazel test //rs/crypto/sha2:all
```

Most targets should build on the host machine. However, the IC-OS image only
builds inside the canonical container (`ic-build-bazel:$TAG`). To enter this
docker container run `./ci/container/container-run.sh`. This container
is only available in x86-64 environments.

# Building Blocks

## Bazel Commands

The three most common commands for developers are `build`, `test` and `run`.

### Bazel Build

`bazel build //path:target_name`

Just builds the target binary, library or test; but doesn’t do anything with it.
Outputs it to the `bazel-bin` directory at the root of the workspace.

*Examples (see*
[https://docs.bazel.build/versions/main/guide.html#specifying-targets-to-build](https://docs.bazel.build/versions/main/guide.html#specifying-targets-to-build))

`bazel build //rs/crypto/sha:sha`

`cd rs/crypto; bazel build :sha`

`bazel build //rs/crypto/sha:all`

`bazel build //rs/crypto/...`: build all rule targets in all packages beneath
the directory `crypto`.

### Bazel Test

`*bazel test //path:target_name*`

Builds an executable test binary [or binaries] and runs them, outputting and
caching the results. Cached results will not be rerun. Bazel only caches passed
test results. The flag `--cache_test_results=no` will make Bazel rerun cached
tests.

To see log output from failed tests add `--test_output=errors`. To see all log
output add `--test_output=all`. To see output of non-failing Rust tests,
additionally add `--test_arg=--nocapture`.

To run only a specific test add `--test_arg=$TEST_NAME`

*Examples*

`bazel test //rs/log_analyzer:tests`

`cd rs/log_analyzer; bazel test :tests`

`bazel test //rs/log_analyzer:all`

`bazel test //rs/log_analyzer/...`

### Bazel Run

`*bazel run //path:target_name*`

Builds executable binary and then executes it on the host machine.

*Examples*

`bazel run //rs/log_analyzer:log_analyzer_bench`
`cd rs/log_analyzer; bazel run :log_analyzer_bench -- --bench`

All tests are also binaries that can be run:

`bazel run //rs/registry/canister:registry_canister_canister_test -- --help`

## BUILD.bazel Files

BUILD.bazel files define targets and their dependencies. You can see a complete
list of Rust build rules [here](https://bazelbuild.github.io/rules_rust/flatten.html). For Rust, the
most common targets are:

### `rust_library`

Builds a rust library crate. You can build this with `bazel build` command
described above.

### `rust_binary`

Builds a rust executable. You can build this with `bazel build` and run it on
your host machine with `bazel run`

### `rust_test`

Builds a Rust crate tests. You can build this with `bazel build` and execute the
test suite with `bazel test`

### `rust_doc_test`

Builds a Rust doc tests tests. You can build this with `bazel build` and execute
the test suite with `bazel test`

## WORKSPACE.bazel and external_crates.bzl

[WORKSPACE.bazel](https://github.com/dfinity/ic/blob/master/WORKSPACE.bazel)

The workspace file defines the root of the Bazel workspace and defines which
external dependencies to pull into the build.

[bazel/external_crates.bzl](https://github.com/dfinity/ic/blob/master/bazel/external_crates.bzl)

The workspace file pulls Rust crate dependencies from
`bazel/external_crates.bzl` . The crate dependencies live in a separate file to
facilitate integration with automation.

Changes to the crate_repository require regeneration of `Cargo.Bazel.*.lock`
with `./bin/bazel-pin.sh`

Below is an example of adding a new third-party crate.

1.  The main thing is adding an entry to bazel/external_crates.bzl . In this
example, I used/added the egg-mode crate in `bazel/external_crates.bzl`

```git
+ "egg-mode": crate.spec(
+   version = "^0.16.0",
+ ),
```

2.  `./bin/bazel-pin.sh`

3.  Don't worry about the changes to the two Cargo.Bazel.* files (in the root of the repo). Those are all generated by the repin command

## Example BUILD.Bazel File

[rs/crypto/sha/BUILD.bazel](https://github.com/dfinity/ic/blob/master/rs/crypto/sha/BUILD.bazel)

```bash
load("@rules_rust//rust:defs.bzl", "rust_doc_test", "rust_library", "rust_test")

package(default_visibility = ["//visibility:public"])

rust_library(
    name = "sha",
    srcs = glob(["src/**"]),
    crate_name = "ic_crypto_sha",
    version = "0.9.0",
    deps = ["//rs/crypto/internal/crypto_lib/sha2"],
)

rust_doc_test(
    name = "sha_doc_test",
    crate = ":sha",
)

rust_test(
    name = "sha224_test",
    srcs = ["tests/sha224.rs"],
    deps = [
        ":sha",
        "@crate_index//:openssl",
    ],
)

rust_test(
    name = "sha256_test",
    srcs = ["tests/sha256.rs"],
    deps = [
        ":sha",
        "@crate_index//:openssl",
    ],
)
```

## Python tests
Python bazel targets are built very similarly and also include `py_test`, `py_library`, `py_binary`. Note that `py_library` is a python module which tests can import and test against. Test deps can either be a `py_library` or a `requirement`, see example below:

```
load("@python_deps//:requirements.bzl", "requirement")

py_library(
    name = "my_module",
    srcs = ["my_module.py"],
    deps = requirement("numpy"),
)

py_test(
    name = "test_my_module",
    srcs = ["tests/test_my_module.py"],
    deps = [":my_module", requirement("pytest")],
)
```

Note that if a module is defined with a package dependency, then the test does not need to specify this dependency again, but can import the entire module. Similarly if packages depend on other packages, only the top-level package needs to be imported.

Some good examples for writing bazel tests can be found in `scalability/BUILD.bazel`.

To add python packages to the build container for use in a build or test target in bazel, follow [these instructions](https://github.com/dfinity/ic/blob/master/ci/src/docs/HowTo-Developer.adoc).


## Target Labels

In Bazel, target labels specify the absolute paths from the root of the
workspace to the BUILD.bazel file, or the target name from the current working
directory. Relative paths are not possible. Examples:

Example absolute path `bazel build //rs/crypto/sha:sha`

Example current working dir: `cd rs/crypto/sha; bazel build :sha`

Targets external to the workspace [e.g. third party crates] need to include the
repository name. For example `bazel build @crate_index//:openssl`

See
[https://docs.bazel.build/versions/main/build-ref.html#labels](https://docs.bazel.build/versions/main/build-ref.html#labels)
for Bazel’s nomenclature and more details.

### Wildcards

Bazel
[reference](https://docs.bazel.build/versions/main/guide.html#specifying-targets-to-build)

The `:all` target name specifies all targets in the path. For example `bazel
build //rs/log_analyzer:all` builds all libraries, executable binaries and
tests.

The `...` name specifies all targets in the workspace and recurses starting from
the current working directory. Note, there’s no colon. When used with `bazel
test`, also build all targets including those not referenced by any test.

*Examples*

`cd rs; bazel build ...` builds all Rust libraries, executable binaries and tests.

`cd rs; bazel test ...` runs all Rust tests and builds all Rust libraries and executables.

# Example Cargo to Bazel Conversion MRs

Take a look at the following example migration. It is instructive to compare and
contrast the `Cargo.toml` file with its associated `BAZEL.build` file.

- [Criterion Times](https://github.com/dfinity/ic/commit/83bafb9c102eb91b0afde7c5d2260532bc6874d5)
- [Log Analyzer](https://github.com/dfinity/ic/commit/63b176839c61ebe028cf93ed058f81d5b552efc8)

# Visualize and Share

Developers may inspect and share detailed build results, timings, logs and
artifacts with the buildfarm URL. Note the buildfarm URL emitted at the start and
end of the build.



# Flaky Tests

Bazel provides several tools to **mitigate** and **resolve** flaky tests.

## Mitigation

Mark the test as **flaky** to make Bazel will retry the test up to three times.

```bash
rust_test(
	name = "foo_test",
  # lines omitted
	flaky = True",  # flakiness rate of $f% over the last month on $date.
)
```

Where you can retrieve the flakiness rate $f from Superset.

Instruct rust to only run one test in parallel - this can help when multiple
concurrent test cases collide but may greatly increase the runtime of the tests.

```bash
rust_test(
	name = "foo_test",
  # lines omitted
  args = [
       "--test-threads",
      "1",
  ],
)
```

## Resolution

The test owners are responsible for eliminating flakiness in their tests.

Bazel provides a facility to reproduce flaky tests, `--runs_per_test <num>`
makes Bazel re-run a test multiple times and aggregate the result.

```bash
bazel test //rs/rust_canisters/memory_test:memory_test_integration_test
--runs_per_test 100
```

# Best Practices

[Minimize
visibility](https://docs-staging.bazel.build/2338/versions/main/visibility.html#best-practices).
Just like members of a class should generally be private, you should restrict
visibility (this is why private is the default visibility). The reason is that
it minimizes the amount of code that can depend on you. This is a Good Thing™️,
because if you later want to make an incompatible change (e.g. add a parameter
to a pub fn), there will be less affected downstream code that you’ll have to
update. The more people who can use an API, the harder it is to change. You will
find many BUILD.bazel files that do the “wrong thing”, in that [they contain the following line](https://sourcegraph.com/search?q=context:global+repo:%5Egithub%5C.com/dfinity/ic%24+package%28default_visibility+%3D+%5B%22//visibility:public%22%5D%29&patternType=standard&case=yes&sm=0):

```python
package(default_visibility = ["//visibility:public"])
```

We have many of these (despite this best practice), because we migrated from
Cargo. We didn’t hand-craft these BUILD.bazel files. Do not follow these
examples in new BUILD.bazel files. If you are feeling ambitious, it would be
nice if you updated this line in your BUILD.bazel files.

# FAQ

### How do I lint (i.e. run rustfmt, and clippy)?

Add `--config=lint` to your bazel command.

By default, clippy violations are just warnings, but formatting issues do not
generate warnings (just like what you’re probably used to from cargo).

Alternatively, if you only want one or the other, do `--config=fmt` or
`--config=clippy` instead (the latter maybe isn’t so useful, since you get
warnings by default anyway).

E.g.

```jsx
bazel build --config=lint //rs/sns/swap:all
```

### Crate contains data files

```jsx
crate/
├─ Cargo.toml
├─ BUILD.bazel
├─ data/
│  ├─ test.csv
├─ src/
│  ├─ lib.rsrs
```

With cargo, tests reference data file relative to the crate root directory.
However, Bazel runs all tests from the workspace root.

The following example adds a data file to the `BUILD.bazel` file under
`rust_test`.

```jsx
data = ["data/test.csv"],
env = {
	"CARGO_MANIFEST_DIR": "rs/bitcoin/validation",
},
```

Change how you access the file. This way we don’t break the cargo pipeline.

```rust
let rdr = Reader::from_path(
		PathBuf::from(std::env::var("CARGO_MANIFEST_DIR").unwrap())
			.join("tests/data/headers.csv"));
```

[Example MR](https://github.com/dfinity/ic/commit/592c25ea302be55f2ef745dc61700e8767909b6c)

### How to recalculate `Cargo.Bazel.*.lock` file?

Run `./bin/bazel-pin.sh` from the root of the repo to recalculate
`Cargo.Bazel.*.lock` files (should take about a minute or three)

You may run it inside of `./ci/container/container-run.sh` if you don’t
have `bazel` commands installed locally.

### rustfmt

`bazel run //:rustfmt`

### Clear Local Bazel Cache

`bazel/bazel_clean.sh`

You might also want to consider adding `--expunge`.

### How do I make debug vs. release builds?

Append `--config=dev` to your bazel command.

```bash
bazel build --config=dev :target
bazel test --config=dev :target
```

See `.bazelrc` at the root of the repo for details on other config options.

### How to run a unit test in just a single file?

Use the flag `--test_arg` to pass the test as an argument over to the Rust test runner. e.g.

```bash
bazel test //rs/registry/canister:registry_canister_test
--test_arg="registry::tests::test_apply_mutations_delta_too_large"
```

### How to run a group of unit tests and see their output?

Use `--test_output=all --test_arg=--nocapture` flags, and also
`--test_arg=heartbeat_` to match a group of tests.

```bash
bazel test --flaky_test_attempts=1 --test_output=all --test_arg=--nocapture
//rs/execution_environment:execution_environment_test --test_arg=heartbeat_
```

### How to find out where builds spend the most time?

Find the dashboard link. Bazel should output this immediate before and
after the Bazel command line invocation. Navigate to the “TIMING” section and
inspect the “Critical Path”

### How to measure the test coverage?

Bazel can collect test coverage information with the
[coverage](https://bazel.build/configure/coverage) command.

```bash
bazel coverage --combined_report=lcov //rs/my/test:target
```

You need to have `genhtml` tool installed to see the report. The tool comes with
the `lcov` package. You need to run `genhtml` in the repository root directory.

```bash
genhtml --ignore-errors source --output genhtml "$(bazel info output_path)/_coverage/_coverage_report.dat"
```

Open the `genhtml/index.html` file in a browser and navigate to the file of
interest to see the coverage.

### “Too many open files in system” error on MacOS

Add this line to `/etc/launchd.conf`:

`limit maxfiles 1000000 1000000`

### How do I auto-format my `.bazel` files?

Run `bazel run //:buildifier`

### Is there something equivalent to `cargo check` but for bazel?

TL;DR
Run `bazel build --config=check //rs/some/target`

The above command will try to only build metadata files for all the rust
libraries, which is almost exactly what `cargo check` does. As with cargo check,
this is not always possible, e.g. when a library depends on a macro.

This feature is still experimental, so please report any issues you encounter
with it, especially if they look something like `error[E0460]: found possibly
newer version of crate ...`.

### Is is possible to deploy a static testnet bazel?

Yes! Follow this procedure:
[testnet/tools/README.md](https://github.com/dfinity/ic/blob/master/testnet/tools/README.md)

### What is the official Bazel slack channel?

You can also find additional help in the Google Bazel documentation and in the
Google Bazel Slack organisation [[Invite link here](https://join.slack.com/t/bazelbuild/shared_invite/zt-18mwk19k1-cxoouSeqqGgkmiweHK35ag)].

# Language Specific Guides
## Go
### Overview

DFINITY uses Rust for systems programming. Rust offers strong safety guarantees
that benefit reliable systems programming; however the tradeoffs include slower
development cycles and a steeper learning curve. Therefore, for other domains
such as CLI tools, infrastructure automation, and scripting other languages
offer merits over Rust

- Faster development cycles
- Faster learning curve
- Mature community API client library support [e.g. GitLab API client libraries]

Historically, DFINITY used Python as an infrastructure or scripting languages.
The IDX team has written automation scripts and daemons in Python, and the Node
team has maintained Python scripts to build the IC-OS disk image. While Python
is ubiquitous and offers a fast learning curve, it notoriously “converts your
compile time errors into run-time errors” which can increase software
development times and cause unexpected breakages. Furthermore, while better than
shell scripts, Python codebases eventually become more difficult to read,
maintain and extend.

Therefore, IDX strongly recommends **Golang** over Python for CLIs,
infrastructure tools and scripting tasks. The Bazel build infrastructure
provides seamless Golang integration into the CI and build system - automation
tools handles the heavy lifting to generate BUILD files and manage external
dependencies.

### Getting Started

This tutorial will guide you through two programs that will acquaint you with Go
development in the IC repo.

- “Hello World Basic” will only use the Go standard library
- “Hello World Advanced” will bring in an external dependency.

Both programs will use [gazelle](https://github.com/bazelbuild/bazel-gazelle) -
a tool which generates build files and manages external dependencies.

### Hello World Basic

From a recent checkout of the IC repo:

```bash
**mkdir go-demo-basic
cd go-demo-basic**
**touch main.go**

# contents of main.go
package main

import "fmt"

func main() {
  fmt.Println("hello world")
}
# end contents of main.go
```

```bash
**bazel run //:gazelle**

**tree
.**
├── BUILD.bazel
└── main.go

**bazel run //go-demo-basic:go-demo-basic 2>/dev/null**

hello world
```

The command `bazel run //:gazelle` will generates the correct `BUILD.bazel`
file. You can then build and execute the Go binary with `bazel run //demo:demo`
. The CI system is batteries included. IDX’s infrastructure automatically
picks-up and builds these new targets on the CI merge requests pipelines.

### Hello World Advanced

```bash
**mkdir go-demo-advanced
cd go-demo-advanced
touch main.go**

# contents of main.go
package main

import("github.com/common-nighthawk/go-figure")

func main() {
  myFigure := figure.NewFigure("Hello World", "", true)
  myFigure.Print()
}
# end contents of main.go
```

```bash
**bazel run //:gazelle**

**tree**
.
├── BUILD.bazel
└── main.go
```

The new BUILD.bazel file will reference the external dependency [take a look at
the contents of the build file]. However, before Bazel can build and run the new
binary, Bazel needs to know how to fetch and provide that dependency and all its
transitive dependencies.

```bash
**bazel run //:gobin -- get github.com/common-nighthawk/go-figure**
**bazel run //:gazelle-update-repos

git status
# the above commands modified the following files: go.mod, go.sum, go_deps.bzl**
```

****And now Bazel can build and run the advanced hello world binary.

```bash
**bazel run //go-demo-advanced:go-demo-advanced 2>/dev/null

  _   _          _   _            __        __                 _       _
 | | | |   ___  | | | |   ___     \ \      / /   ___    _ __  | |   __| |
 | |_| |  / _ \ | | | |  / _ \     \ \ /\ / /   / _ \  | '__| | |  / _` |
 |  _  | |  __/ | | | | | (_) |     \ V  V /   | (_) | | |    | | | (_| |
 |_| |_|  \___| |_| |_|  \___/       \_/\_/     \___/  |_|    |_|  \__,_|**
```

### Real World Code Example

See [bazel/exporter](https://github.com/dfinity/ic/tree/master/bazel/exporter)
for a more advanced real world tool which requires several external dependencies
and Go protobuf definitions. The Bazel exporter runs at the end of every Bazel
CI job; reads Bazel build events protobufs; and exports telemetry to Honeycomb
to populate data.



================================================
FILE: bazel/bazel_clean.sh
================================================
#!/usr/bin/env bash

set -x

bazel clean --expunge
bazel shutdown
sudo rm -fr $HOME/.cache/bazel*
sudo rm -fr $HOME/.cache/buildbuddy*
rm -fr /private/var/tmp/_bazel_*



================================================
FILE: bazel/BUILD.bazel
================================================
load("@bazel_skylib//rules:common_settings.bzl", "bool_flag", "string_flag")
load("@buildifier_prebuilt//:rules.bzl", "buildifier")
load("@rules_python//python:defs.bzl", "py_test")
load("@rules_shell//shell:sh_binary.bzl", "sh_binary")
load("@rules_shell//shell:sh_test.bzl", "sh_test")
load("//bazel:defs.bzl", "write_info_file_var")

bool_flag(
    name = "enable_malicious_code",
    build_setting_default = False,
)

config_setting(
    name = "malicious_code_enabled",
    flag_values = {
        ":enable_malicious_code": "True",
    },
)

bool_flag(
    name = "enable_fuzzing_code",
    build_setting_default = False,
)

config_setting(
    name = "fuzzing_code_enabled",
    flag_values = {
        ":enable_fuzzing_code": "True",
    },
)

# Flag to enable uploading performance system test results to the ES database
# that is connected to the testnet Grafana dashboard.
bool_flag(
    name = "enable_upload_perf_systest_results",
    build_setting_default = False,
)

config_setting(
    name = "upload_perf_systest_results_enabled",
    flag_values = {
        ":enable_upload_perf_systest_results": "True",
    },
)

# Allow targets to opt out of hermetic toolchains, in favor of the one provided
# by the system
bool_flag(
    name = "hermetic_cc",
    build_setting_default = True,
)

config_setting(
    name = "use_hermetic_cc",
    flag_values = {
        ":hermetic_cc": "True",
    },
)

string_flag(
    name = "timeout_value",
    build_setting_default = "10m",
    visibility = ["//visibility:public"],
)

write_info_file_var(
    name = "version.txt",
    varname = "STABLE_VERSION",
    visibility = ["//visibility:public"],
)

write_info_file_var(
    name = "commit_date_iso_8601.txt",
    varname = "STABLE_COMMIT_DATE_ISO_8601",
    visibility = ["//visibility:public"],
)

exports_files(
    [
        "prost_generator.sh",
        "generic_rust_bench.sh",
        "canbench.sh",
        "file_size_test.sh",
    ],
    visibility = ["//visibility:public"],
)

BUILDIFIER_EXCLUDES = [
    "./.git/*",
    "./ci/src/git_changes/test_data/**",
]

buildifier(
    name = "buildifier",
    exclude_patterns = BUILDIFIER_EXCLUDES,
    lint_mode = "fix",
    mode = "fix",
    visibility = ["//visibility:public"],
)

buildifier(
    name = "buildifier.check",
    exclude_patterns = BUILDIFIER_EXCLUDES,
    lint_mode = "warn",
    mode = "diff",
)

py_test(
    name = "buildifier_test",
    size = "small",
    srcs = ["buildifier_test.py"],
    data = [
        ":buildifier.check",
        "//:WORKSPACE.bazel",
    ],
    env = {
        "BUILDIFIER_CHECK_BIN": "$(location :buildifier.check)",
        "WORKSPACE": "$(location //:WORKSPACE.bazel)",
    },
    tags = [
        "external",  # force test to be unconditionally executed.
        "local",  # precludes the action or test from being remotely cached, remotely executed, or run inside the sandbox.
        "smoke",  # it should be run before committing code changes into the version control system.
    ],
    visibility = ["//visibility:public"],
)

sh_test(
    name = "gazelle_test",
    srcs = ["gazelle_test.sh"],
    data = [
        "//:WORKSPACE.bazel",
        "//:gazelle",
    ],
    env = {
        "GAZELLE_BIN": "$(rootpath //:gazelle)",
        "WORKSPACE": "$(rootpath //:WORKSPACE.bazel)",
    },
    tags = [
        "external",  # force test to be unconditionally executed.
        "local",  # precludes the action or test from being remotely cached, remotely executed, or run inside the sandbox.
    ],
)

sh_binary(
    name = "upload_systest_dep",
    srcs = ["upload_systest_dep.sh"],
    visibility = ["//visibility:public"],
)



================================================
FILE: bazel/buildifier_test.py
================================================
#!/usr/bin/env python3
import os
import subprocess
import sys


def main():
    workspace_dir = os.path.dirname(os.path.realpath(os.environ["WORKSPACE"]))
    if not os.path.isdir(workspace_dir):
        sys.exit("WORKSPACE path '{}' is not directory".format(workspace_dir))

    res = subprocess.run(
        os.environ["BUILDIFIER_CHECK_BIN"],
        env={
            "BUILD_WORKSPACE_DIRECTORY": workspace_dir,
        },
    )

    if res.returncode != 0:
        print("\n\n        Please auto-format your changes with `bazel run //:buildifier`\n\n")
    sys.exit(res.returncode)


if __name__ == "__main__":
    main()



================================================
FILE: bazel/canbench.bzl
================================================
"""
This module defines functions to run benchmarks using canbench.
"""

load("@rules_rust//rust:defs.bzl", "rust_binary")
load("@rules_shell//shell:sh_binary.bzl", "sh_binary")
load("@rules_shell//shell:sh_test.bzl", "sh_test")
load("//bazel:canisters.bzl", "wasm_rust_binary_rule")

def rust_canbench(name, results_file, add_test = False, opt = "3", noise_threshold = None, data = [], env = {}, **kwargs):
    """ Run a Rust benchmark using canbench. 

    This creates 2 executable rules: :${name} for running the benchmark and :${name}_update for
    updating the results file and optionally a :${name}_test rule.

    Args:
        name: The name of the rule.
        results_file: The file used store the benchmark results for future comparison.
        add_test: If True add an additional :${name}_test rule that fails if canbench benchmark fails.
        opt: The optimization level to use for the rust_binary compilation.
        data: Additional data resources passthrough.
        env: Additional environment variables passthrough.
        **kwargs: Additional arguments to pass to rust_binary.
        noise_threshold: The noise threshold to use for the benchmark. If None, the default value from
            canbench is used.
    """

    rust_binary(
        name = name + "_bin",
        **kwargs
    )

    wasm_rust_binary_rule(
        name = name + "_wasm",
        binary = ":{name}_bin".format(name = name),
        opt = opt,
    )

    canbench_bin = "$(location @crate_index//:canbench__canbench)"
    wasm_path = "$(location :{name}_wasm)".format(name = name)
    pocket_ic_bin = "$(rootpath //:pocket-ic-mainnet)"
    data = data + [
        ":{name}_wasm".format(name = name),
        "@crate_index//:canbench__canbench",
        results_file,
        "//:WORKSPACE.bazel",
        "//:pocket-ic-mainnet",
    ]
    canbench_results_path = "$(rootpath {results_file})".format(results_file = results_file)
    env = env | {
        "CANBENCH_BIN": canbench_bin,
        "WASM_PATH": wasm_path,
        "CANBENCH_RESULTS_PATH": canbench_results_path,
        "POCKET_IC_BIN": pocket_ic_bin,
        # Hack to escape the sandbox and update the actual repository
        "WORKSPACE": "$(rootpath //:WORKSPACE.bazel)",
    }

    if noise_threshold:
        env["NOISE_THRESHOLD"] = str(noise_threshold)
    sh_binary(
        name = name,
        testonly = True,
        srcs = [
            "//bazel:canbench.sh",
        ],
        data = data,
        env = env,
    )
    sh_binary(
        name = name + "_update",
        testonly = True,
        srcs = [
            "//bazel:canbench.sh",
        ],
        data = data,
        env = env,
        args = ["--update"],
    )
    sh_binary(
        name = name + "_debug",
        testonly = True,
        srcs = [
            "//bazel:canbench.sh",
        ],
        data = data,
        env = env,
        args = ["--debug"],
    )

    if add_test:
        sh_test(
            name = name + "_test",
            srcs = [
                "//bazel:canbench.sh",
            ],
            data = data,
            env = env,
            args = ["--test"],
        )



================================================
FILE: bazel/canbench.sh
================================================
#!/bin/bash

# Runs canbench for benchmarking. Should only be invoked by bazel rules defined in canbench.bzl.
# Usage ./canbench.sh [--update|--test|--debug]
# - When `--update` is specified, the results file will be updated.
# - When `--test` is specified, significant changes will cause the script to exit with an error.
# - When `--debug` is specified, the benchmark will be run with instruction tracing.
#
# Environment variables:
# - CANBENCH_BIN: Path to the canbench binary.
# - CANBENCH_RESULTS_PATH: Path to the results file, which will be:
#     - updated if --update is specified.
#     - used for comparison if it's not empty.
# - WASM_PATH: Path to the wasm file to be benchmarked.
# - NOISE_THRESHOLD: The noise threshold in percentage. If the difference between the current
#     benchmark and the results file is above this threshold, the benchmark test will fail.
# - CANBENCH_STABLE_MEMORY_FILE: The file to use for the stable memory.
# - CANBENCH_PATTERN: The pattern to use for the benchmark. Only benchmarks matching this pattern
#   (name of the benchmark *contains* this string) will be run. Not applicable for `--update`.

set -eEuo pipefail

RUNFILES="$PWD"
REPO_PATH="$(dirname "$(readlink "$WORKSPACE")")"
REPO_RESULTS_PATH="${REPO_PATH}/${CANBENCH_RESULTS_PATH}"
CANBENCH_OUTPUT="$(mktemp -t canbench_output.txt.XXXX)"
NOISE_THRESHOLD_ARG="${NOISE_THRESHOLD:+--noise-threshold ${NOISE_THRESHOLD}}"
PATTERN_ARG="${CANBENCH_PATTERN:+${CANBENCH_PATTERN}}"

# Generates a canbench.yml dynamically to be used by canbench.
CANBENCH_YML="${RUNFILES}/canbench.yml"

echo "wasm_path:" >${CANBENCH_YML}
echo "  ${WASM_PATH}" >>${CANBENCH_YML}

if [ -s "${REPO_RESULTS_PATH}" ]; then
    echo "results_path:" >>${CANBENCH_YML}
    echo "  ${REPO_RESULTS_PATH}" >>${CANBENCH_YML}
fi

if [ -n "${CANBENCH_INIT_ARGS_HEX:-}" ]; then
    echo "init_args:" >>${CANBENCH_YML}
    echo "  hex: ${CANBENCH_INIT_ARGS_HEX}" >>${CANBENCH_YML}
fi

if [ -s "${CANBENCH_STABLE_MEMORY_FILE:-}" ]; then
    TMP_MEMORY_FILE=$(mktemp -p . XXXXXXX.mem)
    if [[ "${CANBENCH_STABLE_MEMORY_FILE}" =~ [.]gz$ ]]; then
        gunzip -c "${CANBENCH_STABLE_MEMORY_FILE}" >"$TMP_MEMORY_FILE"
    else
        cp "${CANBENCH_STABLE_MEMORY_FILE}" "$TMP_MEMORY_FILE"
    fi
    echo "stable_memory:" >>${CANBENCH_YML}
    echo "  file: ${TMP_MEMORY_FILE}" >>${CANBENCH_YML}
fi

if [ $# -eq 0 ]; then
    # Runs the benchmark without updating the results file.
    ${CANBENCH_BIN} ${PATTERN_ARG} --no-runtime-integrity-check --runtime-path ${POCKET_IC_BIN} ${NOISE_THRESHOLD_ARG}
elif [ "$1" = "--update" ]; then
    # Runs the benchmark while updating the results file.
    ${CANBENCH_BIN} --no-runtime-integrity-check --runtime-path ${POCKET_IC_BIN} ${NOISE_THRESHOLD_ARG} --persist

    # Since we cannot specify an empty results file for the first time, we need to copy the default
    # results file to the desired location.
    if [ ! -s ${REPO_RESULTS_PATH} ]; then
        cp "${RUNFILES}/canbench_results.yml" "${REPO_RESULTS_PATH}"
    fi
elif [ "$1" = "--test" ]; then
    # Runs the benchmark test that fails if the diffs are new or above the threshold.
    ${CANBENCH_BIN} ${PATTERN_ARG} --no-runtime-integrity-check --runtime-path ${POCKET_IC_BIN} ${NOISE_THRESHOLD_ARG} >$CANBENCH_OUTPUT
    if grep -q "(regress\|(improved by \|(new)" "$CANBENCH_OUTPUT"; then
        cat "$CANBENCH_OUTPUT"
        echo "**\`$REPO_RESULTS_PATH\` is not up to date ❌**
        If the performance change is expected, run \`_update\` target to save the updated benchmark results."
        exit 1
    else
        cat "$CANBENCH_OUTPUT"
        echo "**\`$REPO_RESULTS_PATH\` is up to date ✅**"
        exit 0
    fi
elif [ "$1" = "--debug" ]; then
    ${CANBENCH_BIN} ${PATTERN_ARG} --no-runtime-integrity-check --runtime-path ${POCKET_IC_BIN} ${NOISE_THRESHOLD_ARG} --instruction-tracing
else
    echo "Unknown command: $1"
    exit 1
fi



================================================
FILE: bazel/candid.bzl
================================================
"""
This module defines functions for checking backward compatibility of candid interfaces.
"""

def _did_git_test_impl(ctx):
    check_did = ctx.executable._check_did
    script = """#!/usr/bin/env bash

set -xeuo pipefail

# Note that MERGE_BASE_SHA is only set on Pull Requests.
# On other events we set the merge_base to HEAD which means we compare the
# did interface file against itself.
readonly merge_base=${{MERGE_BASE_SHA:-HEAD}}

readonly tmpfile=$(mktemp $TEST_TMPDIR/prev.XXXXXX)
readonly errlog=$(mktemp $TEST_TMPDIR/err.XXXXXX)

if ! git show $merge_base:{did_path} > $tmpfile 2> $errlog; then
    if grep -sq -- "exists on disk, but not in \\|does not exist in 'HEAD'" $errlog; then
        echo "{did_path} is a new file, skipping backwards compatibility check"
        exit 0
    else
        cat $errlog
        exit 1
    fi
fi

echo MERGE_BASE=$merge_base
echo DID_PATH={did_path}

{check_did} {did_path} "$tmpfile"
echo "{did_path} passed candid checks"

# In addition to the usual `didc check after.did before.did` it can be helpful to check the reverse as well.
# This is This is useful when it is expected that clients will "jump the gun", i.e. upgrade before servers.
# This is an unusual (but not unheard of) use case.
if [ {enable_also_reverse} = True ]; then
    echo "running also-reverse check"
    {check_did} "$tmpfile" {did_path}
fi
    """.format(check_did = check_did.short_path, did_path = ctx.file.did.path, enable_also_reverse = ctx.attr.enable_also_reverse)

    ctx.actions.write(output = ctx.outputs.executable, content = script)

    files = depset(direct = [check_did, ctx.file.did, ctx.file._git])
    runfiles = ctx.runfiles(files = files.to_list())

    return [
        DefaultInfo(runfiles = runfiles),
        RunEnvironmentInfo(inherited_environment = ["MERGE_BASE_SHA"]),
    ]

CHECK_DID = attr.label(
    default = Label("//rs/tools/check_did"),
    executable = True,
    allow_single_file = True,
    cfg = "exec",
)

_did_git_test = rule(
    implementation = _did_git_test_impl,
    attrs = {
        "did": attr.label(allow_single_file = True),
        "enable_also_reverse": attr.bool(default = False),
        "_check_did": CHECK_DID,
        "_git": attr.label(allow_single_file = True, default = "//:.git"),
    },
    test = True,
)

def did_git_test(name, did, **kwargs):
    """Defines a test checking whether a Candid interface evolves in a backward-compatible way.

    Args:
      name: the test name.
      did: the Candid file, must be a repository file.
      **kwargs: additional keyword arguments to pass to the test rule.
            enable_also_reverse (bool, optional): whether the test should also run candid checks in reverse order
    """
    tags = kwargs.pop("tags", [])
    for tag in ["local", "no-sandbox", "smoke", "didc"]:
        if not tag in tags:
            tags.append(tag)

    _did_git_test(name = name, did = did, tags = tags, **kwargs)



================================================
FILE: bazel/canisters.bzl
================================================
"""
This module defines utilities for building Rust canisters.
"""

load("@rules_motoko//motoko:defs.bzl", "motoko_binary")
load("@rules_rust//rust:defs.bzl", "rust_binary")
load("//bazel:candid.bzl", "did_git_test")

def _wasm_rust_transition_impl(_settings, attr):
    return {
        "//command_line_option:platforms": "@rules_rust//rust/platform:wasm",
        "@rules_rust//:extra_rustc_flags": [
            # rustc allocates a default stack size of 1MiB for Wasm, which causes stack overflow on certain
            # recursive workloads when compiled with 1.78.0+. Hence, we set the new stack size to 3MiB
            "-C",
            "link-args=-z stack-size=3145728",
            "-C",
            "linker-plugin-lto",
            "-C",
            "opt-level=" + attr.opt,
            "-C",
            "debug-assertions=no",
            "-C",
            "debuginfo=0",
            "-C",
            "lto",
            "-C",
            # If combined with -C lto, -C embed-bitcode=no will cause rustc to abort at start-up,
            # because the combination is invalid.
            # See: https://doc.rust-lang.org/rustc/codegen-options/index.html#embed-bitcode
            #
            # embed-bitcode is disabled by default by rules_rust.
            "embed-bitcode=yes",
            "-C",
            "target-feature=+bulk-memory",
        ],
    }

wasm_rust_transition = transition(
    implementation = _wasm_rust_transition_impl,
    inputs = [],
    outputs = [
        "//command_line_option:platforms",
        "@rules_rust//:extra_rustc_flags",
    ],
)

def _wasm_binary_impl(ctx):
    out = ctx.actions.declare_file(ctx.label.name + ".wasm")
    ctx.actions.run(
        executable = "cp",
        arguments = [ctx.files.binary[0].path, out.path],
        outputs = [out],
        inputs = ctx.files.binary,
    )

    return [DefaultInfo(files = depset([out]), runfiles = ctx.runfiles([out]))]

wasm_rust_binary_rule = rule(
    implementation = _wasm_binary_impl,
    attrs = {
        "binary": attr.label(mandatory = True, cfg = wasm_rust_transition),
        "_allowlist_function_transition": attr.label(default = "@bazel_tools//tools/allowlists/function_transition_allowlist"),
        "opt": attr.string(mandatory = True),
    },
)

def rust_canister(name, service_file, visibility = ["//visibility:public"], testonly = False, opt = "3", **kwargs):
    """Defines a Rust program that builds into a WebAssembly module.

    The following targets are generated:
        <name>.raw: the raw Wasm module as built by rustc
        <name>.wasm.gz: the Wasm module, shrunk, with metadata, gzipped.
        <name>_did_git_test: a test that checks the backwards-compatibility of the did service file from HEAD with the same file from the merge-base of the PR.

    Args:
      name: the name of the target that produces a Wasm module.
      service_file: the label pointing the canister candid interface file.
      visibility: visibility of the Wasm target
      opt: opt-level for the Wasm target
      testonly: testonly attribute for Wasm target
      **kwargs: additional arguments to pass a rust_binary.
    """

    # Tags for the wasm build (popped because not relevant to bin base build)
    tags = kwargs.pop("tags", [])
    tags.append("canister")

    # The option to keep the name section is only required for wasm finalization.
    keep_name_section = kwargs.pop("keep_name_section", False)

    # Sanity checking (no '.' in name)
    if name.count(".") > 0:
        fail("name '{}' should not include dots".format(name))

    # Rust binary build (not actually built by default, but transitioned & used in the
    # wasm build)
    # NOTE: '_wasm_' is a misnommer since it's not a wasm build but used for legacy
    # reasons (some targets depend on this)
    bin_name = "_wasm_" + name.replace(".", "_")
    rust_binary(
        name = bin_name,
        crate_type = "bin",
        tags = ["manual"],  # don't include in wildcards like //pkg/...
        visibility = ["//visibility:private"],  # shouldn't be used
        testonly = testonly,
        **kwargs
    )

    # The actual wasm build, unoptimized
    wasm_name = name + ".raw"
    wasm_rust_binary_rule(
        name = wasm_name,
        binary = ":" + bin_name,
        opt = opt,
        visibility = visibility,
        testonly = testonly,
        tags = tags,
    )

    # The finalized wasm (optimized, versioned, etc)
    final_name = name + ".wasm.gz"
    finalize_wasm(
        name = final_name,
        src_wasm = wasm_name,
        service_file = service_file,
        version_file = "//bazel:version.txt",
        visibility = visibility,
        testonly = testonly,
        keep_name_section = keep_name_section,
    )

    native.alias(
        name = name,
        actual = final_name,
        visibility = visibility,
    )

    # DID service related targets
    native.alias(
        name = name + ".didfile",
        actual = service_file,
        visibility = visibility,
    )
    did_git_test(
        name = name + "_did_git_test",
        did = service_file,
    )

def motoko_canister(name, entry, deps):
    """Defines a Motoko program that builds into a WebAssembly module.

    Args:
      name: the name of the target that produces a Wasm module.
      entry: path to this canister's main Motoko source file.
      deps: list of actor dependencies, e.g., external_actor targets from @rules_motoko.
    """

    raw_wasm = entry.replace(".mo", ".raw")
    raw_did = entry.replace(".mo", ".did")
    final_name = name + ".wasm.gz"

    native.alias(
        name = name + ".didfile",
        actual = raw_did,
    )

    motoko_binary(
        name = name + "_raw",
        entry = entry,
        idl_out = raw_did,
        wasm_out = raw_wasm,
        deps = deps,
    )

    finalize_wasm(
        name = final_name,
        src_wasm = raw_wasm,
        version_file = "//bazel:version.txt",
        testonly = False,
    )

    native.alias(
        name = name,
        actual = final_name,
    )

def finalize_wasm(*, name, src_wasm, service_file = None, version_file, testonly, visibility = ["//visibility:public"], keep_name_section = False):
    """Generates an output file name `name + '.wasm.gz'`.

    The input file is shrunk, annotated with metadata, and gzipped. The canister
    metadata consists of:
        'icp:public git_commit_id': version used in the build
        'icp:public candid:service': the canister's candid service description
    """
    native.genrule(
        name = "_" + name + "_finalize",
        srcs = [src_wasm, version_file] + ([service_file] if not (service_file == None) else []),
        outs = [name],
        visibility = visibility,
        testonly = testonly,
        message = "Finalizing canister " + name,
        tools = ["@crate_index//:ic-wasm__ic-wasm", "@pigz"],
        cmd_bash = " && ".join([
            "{ic_wasm} {input_wasm} -o $@.shrunk shrink {keep_name_section}",
            "{ic_wasm} $@.shrunk -o $@.meta metadata candid:service {keep_name_section} --visibility public --file " + "$(location {})".format(service_file) if not (service_file == None) else "cp $@.shrunk $@.meta",  # if service_file is None, don't include a service file
            "{ic_wasm} $@.meta -o $@.ver metadata git_commit_id {keep_name_section} --visibility public --file {version_file}",
            "{pigz} --processes 16 --no-name $@.ver --stdout > $@",
        ])
            .format(input_wasm = "$(location {})".format(src_wasm), ic_wasm = "$(location @crate_index//:ic-wasm__ic-wasm)", version_file = "$(location {})".format(version_file), pigz = "$(location @pigz)", keep_name_section = "--keep-name-section" if keep_name_section else ""),
    )



================================================
FILE: bazel/cargo.config
================================================
[Empty file]


================================================
FILE: bazel/cc_rs.patch
================================================
# Adjust target naming to match what zig expects.
#
# zig does not plan to change their target naming: https://github.com/ziglang/zig/issues/4911
# and cc-rs is waiting on a 1.0 release to support zig's scheme: https://github.com/rust-lang/cc-rs/pull/986
# Other related links: https://github.com/bazelbuild/rules_rust/issues/2529
#                      https://github.com/Asana/cc-rs
diff --git a/src/lib.rs b/src/lib.rs
index 260b850..83fa53f 100644
--- a/src/lib.rs
+++ b/src/lib.rs
@@ -2262,6 +2262,17 @@ impl Build {
 
                     let clang_target =
                         target.llvm_target(&self.get_raw_target()?, version.as_deref());
+
+                    let clang_target = match &clang_target {
+                        std::borrow::Cow::Borrowed("x86_64-unknown-linux-gnu") => {
+                            "x86_64-linux-gnu"
+                        }
+                        std::borrow::Cow::Borrowed("wasm32-unknown-unknown") => {
+                            "wasm32-freestanding-musl"
+                        }
+                        _other => &clang_target,
+                    };
+
                     cmd.push_cc_arg(format!("--target={clang_target}").into());
                 }
             }



================================================
FILE: bazel/copy_crate_versions.sh
================================================
#!/bin/bash

if [ $# -ne 1 ]; then
    echo "USAGE: $0 [subfolder of ic/rs/]"
    exit 1
fi

print_purple() {
    echo -e "\033[1;35m$*\033[0m"
}

WORKSPACE_DIR=$(bazel info workspace 2>/dev/null)
SCRIPT=$(realpath "$0")
COMPARE_COVERED_TESTS="$(dirname "$SCRIPT")/compare_covered_tests.sh"
echo "**** Checking BUILD.bazel files in $WORKSPACE_DIR/rs/$1/..."
echo "     using $COMPARE_COVERED_TESTS to compare covered tests."
for f in $(find $WORKSPACE_DIR/rs/$1 -name BUILD.bazel); do
    CRATE_DIR="$(dirname "${f}")"
    print_purple "==== checking crate $CRATE_DIR"
    CRATE_NAME=$(grep '^name =' "$CRATE_DIR/Cargo.toml" | head -1 | cut -d ' ' -f 3 | sed -r 's/-/_/g')
    CRATE_VERSION=$(grep '^version =' "$CRATE_DIR/Cargo.toml" | cut -d ' ' -f 3)
    if grep -q "crate_name = $CRATE_NAME," "$CRATE_DIR/BUILD.bazel"; then
        echo "    setting version $CRATE_VERSION for $CRATE_NAME"
        sed -i "s/crate_name = $CRATE_NAME,/crate_name = $CRATE_NAME,\n    version = $CRATE_VERSION,/" "$CRATE_DIR/BUILD.bazel"
        cd $CRATE_DIR
        bazel run //:buildifier
    fi
done



================================================
FILE: bazel/defs.bzl
================================================
"""
Utilities for building IC replica and canisters.
"""

load("@rules_rust//rust:defs.bzl", "rust_binary", "rust_test", "rust_test_suite")
load("@rules_shell//shell:sh_binary.bzl", "sh_binary")
load("@rules_shell//shell:sh_test.bzl", "sh_test")
load("//publish:defs.bzl", "release_nostrip_binary")

_COMPRESS_CONCURRENCY = 16

def _compress_resources(_os, _input_size):
    """ The function returns resource hints to bazel so it can properly schedule actions.

    Check https://bazel.build/rules/lib/actions#run for `resource_set` parameter to find documentation of the function, possible arguments and expected return value.
    """
    return {"cpu": _COMPRESS_CONCURRENCY}

def _gzip_compress(ctx):
    """GZip-compresses source files.
    """
    out = ctx.actions.declare_file(ctx.label.name)
    ctx.actions.run_shell(
        command = "{pigz} --processes {concurrency} --no-name {srcs} --stdout > {out}".format(pigz = ctx.file._pigz.path, concurrency = _COMPRESS_CONCURRENCY, srcs = " ".join([s.path for s in ctx.files.srcs]), out = out.path),
        inputs = ctx.files.srcs,
        outputs = [out],
        tools = [ctx.file._pigz],
        resource_set = _compress_resources,
    )
    return [DefaultInfo(files = depset([out]), runfiles = ctx.runfiles(files = [out]))]

gzip_compress = rule(
    implementation = _gzip_compress,
    attrs = {
        "srcs": attr.label_list(allow_files = True),
        "_pigz": attr.label(allow_single_file = True, default = "@pigz"),
    },
)

def _zstd_compress(ctx):
    """zstd-compresses source files.
    """
    out = ctx.actions.declare_file(ctx.label.name)

    ctx.actions.run(
        executable = "zstd",
        arguments = ["-q", "--threads=0", "-10", "-f", "-z", "-o", out.path] + [s.path for s in ctx.files.srcs],
        inputs = ctx.files.srcs,
        outputs = [out],
        env = {"ZSTDMT_NBWORKERS_MAX": str(_COMPRESS_CONCURRENCY)},
        resource_set = _compress_resources,
    )
    return [DefaultInfo(files = depset([out]), runfiles = ctx.runfiles(files = [out]))]

zstd_compress = rule(
    implementation = _zstd_compress,
    attrs = {
        "srcs": attr.label_list(allow_files = True),
    },
)

def _untar(ctx):
    """Unpacks tar archives.
    """
    out = ctx.actions.declare_directory(ctx.label.name)

    ctx.actions.run(
        executable = "tar",
        arguments = ["-xf", ctx.file.src.path, "-C", out.path],
        inputs = [ctx.file.src],
        outputs = [out],
    )
    return [DefaultInfo(files = depset([out]), runfiles = ctx.runfiles(files = [out]))]

untar = rule(
    implementation = _untar,
    attrs = {
        "src": attr.label(allow_single_file = True),
    },
)

def _mcopy(ctx):
    """Copies Unix files to MSDOS images.
    """
    out = ctx.actions.declare_file(ctx.label.name)

    command = "cp -p {fs} {output} && chmod +w {output} ".format(fs = ctx.file.fs.path, output = out.path)
    for src in ctx.files.srcs:
        command += "&& mcopy -mi {output} -sQ {src_path} ::/{filename} ".format(output = out.path, src_path = src.path, filename = ctx.attr.remap_paths.get(src.basename, src.basename))

    ctx.actions.run_shell(
        command = command,
        inputs = ctx.files.srcs + [ctx.file.fs],
        outputs = [out],
    )
    return [DefaultInfo(files = depset([out]), runfiles = ctx.runfiles(files = [out]))]

mcopy = rule(
    implementation = _mcopy,
    attrs = {
        "srcs": attr.label_list(allow_files = True),
        "fs": attr.label(allow_single_file = True),
        "remap_paths": attr.string_dict(),
    },
)

# Binaries needed for testing with canister_sandbox
_SANDBOX_DATA = [
    "//rs/canister_sandbox",
    "//rs/canister_sandbox:compiler_sandbox",
    "//rs/canister_sandbox:sandbox_launcher",
]

# Env needed for testing with canister_sandbox
_SANDBOX_ENV = {
    "COMPILER_BINARY": "$(rootpath //rs/canister_sandbox:compiler_sandbox)",
    "LAUNCHER_BINARY": "$(rootpath //rs/canister_sandbox:sandbox_launcher)",
    "SANDBOX_BINARY": "$(rootpath //rs/canister_sandbox)",
}

def rust_test_suite_with_extra_srcs(name, srcs, extra_srcs, **kwargs):
    """ A rule for creating a test suite for a set of `rust_test` targets.

    Like `rust_test_suite`, but with ability to deal with integration
    tests that use common utils across various tests.  The sources of
    the common utils should be specified in extra_srcs` argument.

    Args:
      name: see description for `rust_test_suite`
      srcs: see description for `rust_test_suite`
      extra_srcs: list of files that e.g. implement common utils, must be disjoint from `srcs`
      **kwargs: see description for `rust_test_suite`
    """
    tests = []

    for extra_src in extra_srcs:
        if not extra_src.endswith(".rs"):
            fail("Wrong file in extra_srcs: " + extra_src + ". extra_srcs should have `.rs` extensions")

    for src in srcs:
        if not src.endswith(".rs"):
            fail("Wrong file in srcs: " + src + ". srcs should have `.rs` extensions")

        # Prefixed with `name` to allow parameterization with macros
        # The test name should not end with `.rs`
        test_name = name + "_" + src[:-3]
        rust_test(
            name = test_name,
            srcs = [src] + extra_srcs,
            crate_root = src,
            **kwargs
        )
        tests.append(test_name)

    native.test_suite(
        name = name,
        tests = tests,
        tags = kwargs.get("tags", None),
    )

def rust_ic_test_suite_with_extra_srcs(name, srcs, extra_srcs, env = {}, data = [], **kwargs):
    """ A rule for creating a test suite for a set of `rust_test` targets.

    Like `rust_test_suite_with_extra_srcs`, but adds data and env params required for canister sandbox

    Args:
      see description for `rust_test_suite_with_extra_srcs`
    """
    rust_test_suite_with_extra_srcs(
        name,
        srcs,
        extra_srcs,
        env = dict(env.items() + _SANDBOX_ENV.items()),
        data = data + _SANDBOX_DATA,
        **kwargs
    )

def rust_ic_test_suite(env = {}, data = [], **kwargs):
    """ A rule for creating a test suite for a set of `rust_test` targets.

    Like `rust_test_suite`, but adds data and env params required for canister sandbox

    Args:
      see description for `rust_test_suite`
    """
    rust_test_suite(
        env = dict(env.items() + _SANDBOX_ENV.items()),
        data = data + _SANDBOX_DATA,
        **kwargs
    )

def rust_ic_test(env = {}, data = [], **kwargs):
    """ A rule for creating a test suite for a set of `rust_test` targets.

    Like `rust_test`, but adds data and env params required for canister sandbox

    Args:
      see description for `rust_test`
    """
    rust_test(
        env = dict(env.items() + _SANDBOX_ENV.items()),
        data = data + _SANDBOX_DATA,
        **kwargs
    )

def rust_bench(name, env = {}, data = [], pin_cpu = False, test_name = None, test_timeout = None, **kwargs):
    """A rule for defining a rust benchmark.

    Args:
      name: the name of the executable target.
      env: additional environment variables to pass to the benchmark binary.
      data: data dependencies required to run the benchmark.
      pin_cpu: pins the benchmark process to a single CPU if set `True`.
      test_name: generates test with name 'test_name' to test that the benchmark work.
      test_timeout: timeout to apply in the generated test (default: `moderate`).
      **kwargs: see docs for `rust_binary`.
    """

    kwargs.setdefault("testonly", True)

    # The initial binary is a regular rust_binary with rustc flags as in the
    # current build configuration. It is marked as "manual" because it is not
    # meant to be built.
    binary_name_initial = "_" + name + "_bin_default"
    kwargs_initial = dict(kwargs)
    tags_initial = kwargs_initial.pop("tags", [])
    if "manual" not in tags_initial:
        tags_initial.append("manual")
    rust_binary(name = binary_name_initial, tags = tags_initial, **kwargs_initial)

    # The "publish" binary has the same compiler flags applied as for production build.
    binary_name_publish = "_" + name + "_bin_publish"
    release_nostrip_binary(
        name = binary_name_publish,
        binary = binary_name_initial,
        testonly = kwargs.get("testonly"),
    )

    bench_prefix = "taskset -c 0 " if pin_cpu else ""

    # The benchmark binary is a shell script that runs the binary
    # (similar to how `cargo bench` runs the benchmark binary).
    sh_binary(
        srcs = ["//bazel:generic_rust_bench.sh"],
        name = name,
        # Allow benchmark targets to use test-only libraries.
        testonly = kwargs.get("testonly"),
        env = dict(env.items() +
                   [("BAZEL_DEFS_BENCH_PREFIX", bench_prefix)] +
                   {"BAZEL_DEFS_BENCH_BIN": "$(location :%s)" % binary_name_publish}.items()),
        data = data + [":" + binary_name_publish],
        tags = kwargs.get("tags", []) + ["rust_bench"],
    )

    # To test that the benchmarks work.
    if test_name != None:
        test_timeout = test_timeout or "moderate"
        sh_test(
            name = test_name,
            testonly = True,
            timeout = test_timeout,
            env = env,
            srcs = [":" + binary_name_publish],
            data = data,
            tags = kwargs.get("tags", None),
        )

def rust_ic_bench(env = {}, data = [], **kwargs):
    """A rule for defining a rust benchmark.

    Like `rust_bench`, but adds data and env params required for canister sandbox

    Args:
      see description for `rust_bench`
    """
    rust_bench(
        env = dict(env.items() + _SANDBOX_ENV.items()),
        data = data + _SANDBOX_DATA,
        **kwargs
    )

def _symlink_dir_test(ctx):
    """
    Create a symlink to have a stable location for Rust (and maybe other) test binaries

    `rust_test` creates a binary as an output, so you can use that binary in
    other targets, including Rust tests, e.g., as a `data` dependency. But for a
    `rust_test` target `tgt`, the location of the binary in RUNFILES_DIR is
    unpredictable (Bazel will put it in a dir called something like
    `tgt_451223`). This rule creates a symlink to the binary in a stable location.
    """

    # Use the no-op script as the executable
    no_op_output = ctx.actions.declare_file("no_op")
    ctx.actions.write(output = no_op_output, content = ":")

    dirname = ctx.attr.name
    lns = []
    for target, canister_name in ctx.attr.targets.items():
        ln = ctx.actions.declare_file(dirname + "/" + canister_name)
        file = target[DefaultInfo].files.to_list()[0]
        ctx.actions.symlink(
            output = ln,
            target_file = file,
        )
        lns.append(ln)
    return [DefaultInfo(files = depset(direct = lns), executable = no_op_output)]

symlink_dir_test = rule(
    implementation = _symlink_dir_test,
    test = True,
    attrs = {
        "targets": attr.label_keyed_string_dict(allow_files = True),
    },
)

def rust_test_with_binary(name, binary_name, **kwargs):
    """
    A `rust_test` with a stable link to its produced test binary.

    Plain `rust_test` is problematic when one wants to use the produced test binary in
    other Bazel targets (e.g., upgrade/downgrade compatibility tests), as Bazel does not
    provide a stable way to refer to the binary produced by a test. This rule is a thin
    wrapper around `rust_test` that symlinks the test binary to a stable location provided
    by `binary_name`, which can then be used in other tests.

    Usage example:
    ```
    rust_test(
        name = "my_test",
        binary_name = "my_test_binary",
        crate = ":my_crate",
        deps = ["@crate_index//:proptest"]
    )
    ```

    This will generate a rust_test target named `my_test` whose corresponding binary
    will be available as the `my_test_binary` target.
    """
    symlink_dir_test(
        name = binary_name,
        targets = {
            name: binary_name,
        },
    )
    rust_test(
        name = name,
        **kwargs
    )

def _symlink_dir(ctx):
    dirname = ctx.attr.name
    lns = []
    for target, canister_name in ctx.attr.targets.items():
        ln = ctx.actions.declare_file(dirname + "/" + canister_name)
        file = target[DefaultInfo].files.to_list()[0]
        ctx.actions.symlink(
            output = ln,
            target_file = file,
        )
        lns.append(ln)
    return [DefaultInfo(files = depset(direct = lns))]

symlink_dir = rule(
    implementation = _symlink_dir,
    attrs = {
        "targets": attr.label_keyed_string_dict(allow_files = True),
    },
)

def _symlink_dirs(ctx):
    dirname = ctx.attr.name
    lns = []
    for target, childdirname in ctx.attr.targets.items():
        for file in target[DefaultInfo].files.to_list():
            ln = ctx.actions.declare_file(dirname + "/" + childdirname + "/" + file.basename)
            ctx.actions.symlink(
                output = ln,
                target_file = file,
            )
            lns.append(ln)
    return [DefaultInfo(files = depset(direct = lns))]

symlink_dirs = rule(
    implementation = _symlink_dirs,
    attrs = {
        "targets": attr.label_keyed_string_dict(allow_files = True),
    },
)

def _write_info_file_var_impl(ctx):
    """Helper rule that creates a file with the content of the provided var from the info file."""

    output = ctx.actions.declare_file(ctx.label.name)
    ctx.actions.run_shell(
        command = """
            grep <{info_file} -e '{varname}' \\
                    | cut -d' ' -f2 > {out}""".format(varname = ctx.attr.varname, info_file = ctx.info_file.path, out = output.path),
        inputs = [ctx.info_file],
        outputs = [output],
    )
    return [DefaultInfo(files = depset([output]))]

write_info_file_var = rule(
    implementation = _write_info_file_var_impl,
    attrs = {
        "varname": attr.string(mandatory = True),
    },
)

def file_size_check(
        name,
        file,
        max_file_size,
        tags = []):
    """
    A check to make sure the given file is below the specified size.

    Args:
      name: Name of the test.
      file: File to check (label).
      max_file_size: Max accepted size in bytes.
      tags: See Bazel documentation
    """
    sh_test(
        name = name,
        srcs = ["//bazel:file_size_test.sh"],
        data = [file],
        env = {
            "FILE": "$(rootpath %s)" % file,
            "MAX_SIZE": str(max_file_size),
        },
        tags = tags,
    )



================================================
FILE: bazel/dfx.bzl
================================================
"""
The module fetches the dfx binary from the dfinity/sdk repository
"""

DFX_BUILD = """
package(default_visibility = ["//visibility:public"])
exports_files(["dfx"])
"""

VERSION = "0.20.1"
SHA256 = {
    "linux": "011c8a6ff9af0578ee2c8b0a7af3abc5b3d5950cc64421b4c3a7acd1fc18265e",
    "darwin": "4b1c24805f655576fb97f61aba019fbbe7c1bdca0598a5c1a9220a4bfe80f460",
}

URL = "https://github.com/dfinity/sdk/releases/download/{version}/dfx-{version}-{arch}-{platform}.tar.gz"

def _dfx_impl(repository_ctx):
    os_arch = repository_ctx.os.arch

    # even if the macOS version is "x86_64" it runs on ARM chips because of
    # emulation
    if os_arch == "x86_64" or os_arch == "amd64" or os_arch == "aarch64":
        arch = "x86_64"
    else:
        fail("Unsupported architecture: '" + os_arch + "'")

    os_name = repository_ctx.os.name
    if os_name == "linux":
        platform = "linux"
    elif os_name == "mac os x":
        platform = "darwin"
    else:
        fail("Unsupported operating system: " + os_name)

    if platform not in SHA256:
        fail("Unsupported platform: '" + platform + "'")

    repository_ctx.report_progress("Fetching " + repository_ctx.name)
    repository_ctx.download_and_extract(url = URL.format(version = VERSION, platform = platform, arch = arch), sha256 = SHA256[platform])
    repository_ctx.file("BUILD.bazel", DFX_BUILD, executable = True)

_dfx = repository_rule(
    implementation = _dfx_impl,
    attrs = {},
)

def dfx(name):
    _dfx(name = name)



================================================
FILE: bazel/file_size_test.sh
================================================
#!/usr/bin/env bash

set -euo pipefail

file_size=$(wc -c <"$FILE")

if [ "$file_size" -gt "$MAX_SIZE" ]; then
    echo "'$FILE', '$file_size' bytes exceeds the allowed maximum size '$MAX_SIZE'" >&2
    exit 1
else
    echo "'$FILE', '$file_size' bytes is below the allowed maximum size '$MAX_SIZE'"
fi



================================================
FILE: bazel/find_missing_bazel_tests.sh
================================================
#!/bin/bash

if [ $# -ne 1 ]; then
    echo "USAGE: $0 [subfolder of ic/rs/]"
    exit 1
fi

print_purple() {
    echo -e "\033[1;35m$*\033[0m"
}

WORKSPACE_DIR=$(bazel info workspace 2>/dev/null)
SCRIPT=$(realpath "$0")
COMPARE_COVERED_TESTS="$(dirname "$SCRIPT")/compare_covered_tests.sh"
echo "**** Checking BUILD.bazel files in $WORKSPACE_DIR/rs/$1/..."
echo "     using $COMPARE_COVERED_TESTS to compare covered tests."
for f in $(find $WORKSPACE_DIR/rs/$1 -name BUILD.bazel); do
    CRATE_DIR="$(dirname "${f}")"
    print_purple "==== checking crate $CRATE_DIR"
    has_unit_tests="false"
    has_integration_tests="false"
    if grep -q -r -s '#[[]test[]]' $CRATE_DIR/src; then
        has_unit_tests="true"
    fi
    if grep -q -r -s '#[[]tokio::test[]]' $CRATE_DIR/src; then
        has_unit_tests="true"
    fi
    if grep -q -r -s '#[[]test[]]' $CRATE_DIR/tests; then
        has_integration_tests="true"
    fi
    if grep -q -r -s '#[[]tokio::test[]]' $CRATE_DIR/tests; then
        has_integration_tests="true"
    fi

    if [[ ($has_unit_tests == "true") || ($has_integration_tests == "true") ]]; then
        echo "     FOUND tests in the crate, comparing cargo and bazel coverage..."
        cd $CRATE_DIR
        $COMPARE_COVERED_TESTS 2>/dev/null
    else
        echo "     no tests in the crate, skipping."
    fi
done



================================================
FILE: bazel/fuzz_testing.bzl
================================================
"""
This module contains utilities to work with fuzz tests.
"""

load("@rules_rust//rust:defs.bzl", "rust_binary")

# These are rustc flags that can be used with the Rust nightly toolchain to build fuzz tests for libfuzzer.
DEFAULT_RUSTC_FLAGS = [
    "-Cpasses=sancov-module",
    "-Cllvm-args=-sanitizer-coverage-level=4",
    "-Cllvm-args=-sanitizer-coverage-inline-8bit-counters",
    "-Cllvm-args=-sanitizer-coverage-pc-table",
    "-Cllvm-args=-sanitizer-coverage-trace-compares",
    "-Cllvm-args=-sanitizer-coverage-stack-depth",
    "-Cllvm-args=-sanitizer-coverage-prune-blocks=0",
    "-Ctarget-cpu=native",
    "-Coverflow_checks",
    "-Copt-level=3",
    "-Clink-dead-code",
    "-Cdebug-assertions",
    "-Ccodegen-units=1",
    "-Zextra-const-ub-checks",
    "-Zstrict-init-checks",
    # TODO(PSEC): Add configuration to enable only during profiling
    # "-Cinstrument-coverage",
]

DEFAULT_SANITIZERS = [
    "-Zsanitizer=address",
    # zig doesn't like how rustc pushes the sanitizers, so do it ourselves.
    "-Zexternal-clangrt",
    "-Clink-arg=bazel-out/k8-opt/bin/external/rust_linux_x86_64__x86_64-unknown-linux-gnu__stable_tools/rust_toolchain/lib/rustlib/x86_64-unknown-linux-gnu/lib/librustc-stable_rt.asan.a",
]

# This flag will be used by third party crates and internal rust_libraries during fuzzing
DEFAULT_RUSTC_FLAGS_FOR_FUZZING = DEFAULT_RUSTC_FLAGS + DEFAULT_SANITIZERS

def rust_fuzz_test_binary(name, srcs, rustc_flags = [], sanitizers = [], crate_features = [], proc_macro_deps = [], deps = [], allow_main = False, **kwargs):
    """Wrapper for the rust_binary to compile a fuzzing rust_binary

    Args:
      name: name of the fuzzer target.
      srcs: source files for the fuzzer.
      rustc_flags: Additional rustc_flags for rust_binary rule.
      sanitizers: Sanitizers for the fuzzer target. If nothing is provided, address sanitizer is added by default.
      crate_features: Additional crate_features to be used for compilation.
            fuzzing is added by default.
      deps: Fuzzer dependencies.
      allow_main: Allow the fuzzer to export a main function.
      proc_macro_deps: Fuzzer proc_macro dependencies.
      **kwargs: additional arguments to pass a rust_binary rule.
    """

    if not sanitizers:
        sanitizers = DEFAULT_SANITIZERS

    # This would only work inside the devcontainer
    if allow_main:
        TAGS = ["sandbox_libfuzzer"]
    else:
        # default
        TAGS = []

    RUSTC_FLAGS_LIBFUZZER = DEFAULT_RUSTC_FLAGS + ["-Clink-arg=$(location @libfuzzer//:fuzzer)"]

    kwargs.setdefault("testonly", True)

    rust_binary(
        name = name,
        srcs = srcs,
        aliases = {},
        crate_features = crate_features + ["fuzzing"],
        proc_macro_deps = proc_macro_deps,
        deps = deps,
        compile_data = ["@libfuzzer//:fuzzer"],
        rustc_flags = rustc_flags + RUSTC_FLAGS_LIBFUZZER + sanitizers,
        tags = [
            # Makes sure this target is not run in normal CI builds. It would fail due to non-nightly Rust toolchain.
            "fuzz_test",
            "libfuzzer",
        ] + TAGS,
        **kwargs
    )

# TODO(PSEC): Enable allow_main for AFL fuzzers
def rust_fuzz_test_binary_afl(name, srcs, rustc_flags = [], crate_features = [], proc_macro_deps = [], deps = [], **kwargs):
    """Wrapper for the rust_binary to compile a fuzzing rust_binary compatible with AFL

    Args:
      name: name of the fuzzer target.
      srcs: source files for the fuzzer.
      rustc_flags: Additional rustc_flags for rust_binary rule.
      crate_features: Additional crate_features to be used for compilation.
            fuzzing is added by default.
      deps: Fuzzer dependencies.
      proc_macro_deps: Fuzzer proc_macro dependencies.
      **kwargs: additional arguments to pass a rust_binary rule.
    """

    RUSTC_FLAGS_AFL = DEFAULT_RUSTC_FLAGS + [
        "-Cllvm-args=-sanitizer-coverage-trace-pc-guard",
        "-Clink-arg=-fuse-ld=gold",
        "-Clink-arg=-fsanitize=fuzzer",
        "-Clink-arg=-fsanitize=address",
    ]

    kwargs.setdefault("testonly", True)

    rust_binary(
        name = name,
        srcs = srcs,
        aliases = {},
        rustc_env = {
            "AFL_LLVM_LAF_ALL": "1",
            "AFL_USE_ASAN": "1",
            "AFL_USE_LSAN": "1",
        },
        crate_features = crate_features + ["fuzzing"],
        proc_macro_deps = proc_macro_deps,
        deps = deps,
        rustc_flags = rustc_flags + RUSTC_FLAGS_AFL,
        tags = [
            # Makes sure this target is not run in normal CI builds. It would fail due to non-nightly Rust toolchain.
            "fuzz_test",
            "afl",
        ],
        **kwargs
    )



================================================
FILE: bazel/gazelle_test.sh
================================================
#!/usr/bin/env bash

set -euo pipefail

BUILD_WORKSPACE_DIRECTORY="$(dirname $(readlink $WORKSPACE))"
export BUILD_WORKSPACE_DIRECTORY

if ! "$GAZELLE_BIN" update -mode=diff; then
    echo "Some gazelle managed build files need to be regenerated" >&2
    echo "run bazel run //:gazelle" >&2
    exit 1
fi



================================================
FILE: bazel/generic_rust_bench.sh
================================================
#!/usr/bin/env bash

set -euo pipefail

# When Cargo runs benchmarks, it passes the --bench or --test command-line arguments to
# the benchmark executables. Criterion.rs looks for these arguments and tries to either
# run benchmarks or run in test mode. In particular, when you run cargo test --benches
# (run tests, including testing benchmarks) Cargo does not pass either of these
# arguments. This is perhaps strange, since cargo bench --test passes both --bench and
# --test. In any case, Criterion.rs benchmarks run in test mode when --bench is not
# present, or when --bench and --test are both present.
#
# https://bheisler.github.io/criterion.rs/book/faq.html#when-i-run-benchmark-executables-directly-without-using-cargo-they-just-print-success-why
CMD="${BAZEL_DEFS_BENCH_PREFIX}${BAZEL_DEFS_BENCH_BIN} --bench $@"

echo "running ${CMD}"
${CMD}



================================================
FILE: bazel/hermetic_cc_toolchain.patch
================================================
# Extend hermetic_cc_toolchain with a wasm target that does not use wasi
# (https://github.com/uber/hermetic_cc_toolchain/pull/214), and add a path to
# put additional restrictions on the generated toolchains so that we can
# restrict which targets pick up the toolchains
# (https://github.com/uber/hermetic_cc_toolchain/pull/213).
# Strip debug_info off every target for determinism
# (https://github.com/ziglang/zig/issues/23821, https://github.com/ziglang/zig/issues/23823).
diff --git a/MODULE.bazel b/MODULE.bazel
index 409d626..15b70b3 100644
--- a/MODULE.bazel
+++ b/MODULE.bazel
@@ -49,7 +49,25 @@ register_toolchains(
     "@zig_sdk//libc_aware/toolchain:linux_arm64_musl",
     # wasm/wasi toolchains
     "@zig_sdk//toolchain:wasip1_wasm",
+    "@zig_sdk//toolchain:none_wasm",
 
     # These toolchains are only registered locally.
     dev_dependency = True,
 )
+
+bazel_dep(name = "bazel_skylib", version = "1.7.1")
+
+new_local_repository = use_repo_rule("@bazel_tools//tools/build_defs/repo:local.bzl", "new_local_repository")
+new_local_repository(
+    name = "strip",
+    build_file_content = """
+load("@bazel_skylib//rules:native_binary.bzl", "native_binary")
+
+native_binary(
+name = "strip_bin",
+src = ":strip",
+visibility = ["//visibility:public"],
+)
+""",
+    path = "/usr/bin",
+)
diff --git a/toolchain/defs.bzl b/toolchain/defs.bzl
index f6f613e..4b36565 100644
--- a/toolchain/defs.bzl
+++ b/toolchain/defs.bzl
@@ -60,7 +60,8 @@ def toolchains(
         version = VERSION,
         url_formats = [],
         host_platform_sha256 = HOST_PLATFORM_SHA256,
-        host_platform_ext = _HOST_PLATFORM_EXT):
+        host_platform_ext = _HOST_PLATFORM_EXT,
+        extra_settings = []):
     """
         Download zig toolchain and declare bazel toolchains.
         The platforms are not registered automatically, that should be done by
@@ -83,6 +84,7 @@ def toolchains(
         url_formats = url_formats,
         host_platform_sha256 = host_platform_sha256,
         host_platform_ext = host_platform_ext,
+        extra_settings = extra_settings
     )
 
 def _quote(s):
@@ -110,6 +112,8 @@ def _zig_repository_impl(repository_ctx):
         "host_platform": host_platform,
     }
 
+    extra_settings = "[" + " ".join([_quote(str(setting)) for setting in repository_ctx.attr.extra_settings]) + "]"
+
     # Fetch Label dependencies before doing download/extract.
     # The Bazel docs are not very clear about this behavior but see:
     # https://bazel.build/extending/repo#when_is_the_implementation_function_executed
@@ -117,7 +121,6 @@ def _zig_repository_impl(repository_ctx):
     # https://github.com/bazelbuild/bazel-gazelle/pull/1206
     for dest, src in {
         "platform/BUILD": "//toolchain/platform:BUILD",
-        "toolchain/BUILD": "//toolchain/toolchain:BUILD",
         "libc/BUILD": "//toolchain/libc:BUILD",
         "libc_aware/platform/BUILD": "//toolchain/libc_aware/platform:BUILD",
         "libc_aware/toolchain/BUILD": "//toolchain/libc_aware/toolchain:BUILD",
@@ -126,6 +129,7 @@ def _zig_repository_impl(repository_ctx):
 
     for dest, src in {
         "BUILD": "//toolchain:BUILD.sdk.bazel",
+        "toolchain/BUILD": "//toolchain/toolchain:BUILD",
     }.items():
         repository_ctx.template(
             dest,
@@ -134,6 +138,7 @@ def _zig_repository_impl(repository_ctx):
             substitutions = {
                 "{zig_sdk_path}": _quote("external/zig_sdk"),
                 "{os}": _quote(os),
+                "{extra_settings}": extra_settings,
             },
         )
 
@@ -148,7 +153,7 @@ def _zig_repository_impl(repository_ctx):
     cache_prefix = repository_ctx.os.environ.get("HERMETIC_CC_TOOLCHAIN_CACHE_PREFIX", "")
     if cache_prefix == "":
         if os == "windows":
-            cache_prefix = "C:\\\\Temp\\\\zig-cache"
+            fail("windows is not supported")
         elif os == "macos":
             cache_prefix = "/var/tmp/zig-cache"
         elif os == "linux":
@@ -156,6 +161,10 @@ def _zig_repository_impl(repository_ctx):
         else:
             fail("unknown os: {}".format(os))
 
+    # use the catchall config to ensure all builds use a `zig-cache/config-FOO/`
+    # subdirectory, including the zig-wrapper itself
+    cache_prefix = cache_prefix + "/config-catchall"
+
     repository_ctx.template(
         "tools/zig-wrapper.zig",
         Label("//toolchain:zig-wrapper.zig"),
@@ -230,6 +239,7 @@ zig_repository = repository_rule(
         "host_platform_sha256": attr.string_dict(),
         "url_formats": attr.string_list(allow_empty = False),
         "host_platform_ext": attr.string_dict(),
+        "extra_settings": attr.label_list(),
     },
     environ = ["HERMETIC_CC_TOOLCHAIN_CACHE_PREFIX"],
     implementation = _zig_repository_impl,
@@ -272,6 +282,7 @@ def declare_files(os):
                 ":zig",
                 ":{}_includes".format(target_config.zigtarget),
                 cxx_tool_label,
+                "@strip//:strip_bin",
             ],
         )
 
@@ -281,6 +292,7 @@ def declare_files(os):
                 ":zig",
                 ":{}_includes".format(target_config.zigtarget),
                 cxx_tool_label,
+                "@strip//:strip_bin",
             ] + native.glob([
                 "lib/libc/{}/**".format(target_config.libc),
                 "lib/libcxx/**",
diff --git a/toolchain/ext.bzl b/toolchain/ext.bzl
index ebf0ff8..c2e8af3 100644
--- a/toolchain/ext.bzl
+++ b/toolchain/ext.bzl
@@ -1,6 +1,11 @@
 load("@hermetic_cc_toolchain//toolchain:defs.bzl", zig_toolchains = "toolchains")
 
 def _toolchains_impl(ctx):
-    zig_toolchains()
+    extra_settings = []
+    for mod in ctx.modules:
+        for tag in mod.tags.extra_settings:
+            extra_settings += tag.settings
 
-toolchains = module_extension(implementation = _toolchains_impl)
+    zig_toolchains(extra_settings = extra_settings)
+
+toolchains = module_extension(implementation = _toolchains_impl, tag_classes = { "extra_settings": tag_class(attrs = { "settings": attr.label_list(doc = "Each setting is added to every toolchain to make them more restrictive.")})})
diff --git a/toolchain/platform/defs.bzl b/toolchain/platform/defs.bzl
index d4a8344..faafc5b 100644
--- a/toolchain/platform/defs.bzl
+++ b/toolchain/platform/defs.bzl
@@ -16,6 +16,7 @@ def declare_platforms():
 
     # We can support GOARCH=wasm32 after https://github.com/golang/go/issues/63131
     declare_platform("wasm", "wasm32", "wasi", "wasip1")
+    declare_platform("wasm", "wasm32", "none", "none")
 
 def declare_libc_aware_platforms():
     # create @zig_sdk//{os}_{arch}_platform entries with zig and go conventions
diff --git a/toolchain/private/defs.bzl b/toolchain/private/defs.bzl
index 716a3a3..b88d082 100644
--- a/toolchain/private/defs.bzl
+++ b/toolchain/private/defs.bzl
@@ -49,6 +49,7 @@ def target_structs():
         for glibc in _GLIBCS:
             ret.append(_target_linux_gnu(gocpu, zigcpu, glibc))
     ret.append(_target_wasm())
+    ret.append(_target_wasm_no_wasi())
     return ret
 
 def _target_macos(gocpu, zigcpu):
@@ -222,3 +223,22 @@ def _target_wasm():
         ld_zig_subcmd = "wasm-ld",
         artifact_name_patterns = [],
     )
+
+def _target_wasm_no_wasi():
+    return struct(
+        gotarget = "none_wasm",
+        zigtarget = "wasm32-freestanding-musl",
+        includes = [] + _INCLUDE_TAIL,
+        linkopts = [],
+        dynamic_library_linkopts = [],
+        supports_dynamic_linker = False,
+        copts = [],
+        libc = "musl",
+        bazel_target_cpu = "wasm32",
+        constraint_values = [
+            "@platforms//os:none",
+            "@platforms//cpu:wasm32",
+        ],
+        ld_zig_subcmd = "wasm-ld",
+        artifact_name_patterns = [],
+    )
diff --git a/toolchain/toolchain/BUILD b/toolchain/toolchain/BUILD
index 552fcaa..8f7dba5 100644
--- a/toolchain/toolchain/BUILD
+++ b/toolchain/toolchain/BUILD
@@ -4,4 +4,4 @@ package(
     default_visibility = ["//visibility:public"],
 )
 
-declare_toolchains()
+declare_toolchains(extra_settings = {extra_settings})
diff --git a/toolchain/toolchain/defs.bzl b/toolchain/toolchain/defs.bzl
index 50cc881..0549c26 100644
--- a/toolchain/toolchain/defs.bzl
+++ b/toolchain/toolchain/defs.bzl
@@ -1,6 +1,6 @@
 load("@hermetic_cc_toolchain//toolchain/private:defs.bzl", "target_structs")
 
-def declare_toolchains():
+def declare_toolchains(extra_settings = []):
     for target_config in target_structs():
         gotarget = target_config.gotarget
         zigtarget = target_config.zigtarget
@@ -12,7 +12,7 @@ def declare_toolchains():
         if hasattr(target_config, "libc_constraint"):
             extra_constraints = ["@zig_sdk//libc:unconstrained"]
 
-        _declare_toolchain(gotarget, zigtarget, target_config.constraint_values + extra_constraints)
+        _declare_toolchain(gotarget, zigtarget, target_config.constraint_values + extra_constraints, extra_settings)
 
 def declare_libc_aware_toolchains():
     for target_config in target_structs():
@@ -25,13 +25,14 @@ def declare_libc_aware_toolchains():
         if hasattr(target_config, "libc_constraint"):
             _declare_toolchain(gotarget, zigtarget, target_config.constraint_values + [target_config.libc_constraint])
 
-def _declare_toolchain(gotarget, zigtarget, target_compatible_with):
+def _declare_toolchain(gotarget, zigtarget, target_compatible_with, extra_settings):
     # register two kinds of toolchain targets: Go and Zig conventions.
     # Go convention: amd64/arm64, linux/darwin
     native.toolchain(
         name = gotarget,
         exec_compatible_with = None,
         target_compatible_with = target_compatible_with,
+        target_settings = extra_settings,
         toolchain = "@zig_sdk//:%s_cc" % zigtarget,
         toolchain_type = "@bazel_tools//tools/cpp:toolchain_type",
     )
@@ -41,6 +42,7 @@ def _declare_toolchain(gotarget, zigtarget, target_compatible_with):
         name = zigtarget,
         exec_compatible_with = None,
         target_compatible_with = target_compatible_with,
+        target_settings = extra_settings,
         toolchain = "@zig_sdk//:%s_cc" % zigtarget,
         toolchain_type = "@bazel_tools//tools/cpp:toolchain_type",
     )
diff --git a/toolchain/zig-wrapper.zig b/toolchain/zig-wrapper.zig
index d1d59f9..4f471ee 100644
--- a/toolchain/zig-wrapper.zig
+++ b/toolchain/zig-wrapper.zig
@@ -132,7 +132,7 @@ pub fn main() u8 {
             if (builtin.os.tag == .windows)
                 return spawnWindows(arena, params)
             else
-                return execUnix(arena, params);
+                return spawnAndStripUnix(arena, params);
         },
     }
 }
@@ -161,6 +161,71 @@ fn execUnix(arena: mem.Allocator, params: ExecParams) u8 {
     return 1;
 }
 
+fn makeSuffix(allocator: std.mem.Allocator, pwd: []const u8) ![]const u8 {
+    var it = std.mem.tokenize(u8, pwd, "/");
+
+    while (it.next()) |segment| {
+        if (std.mem.startsWith(u8, segment, "k8-opt-")) {
+            var hasher = std.hash.Wyhash.init(0);
+            hasher.update(segment);
+            const hash_value = hasher.final();
+            return std.fmt.allocPrint(allocator, "config-{x}", .{hash_value});
+        }
+    }
+
+    // no "k8-opt-" found
+    return std.fmt.allocPrint(allocator, "config-catchall", .{});
+}
+
+fn spawnAndStripUnix(arena: mem.Allocator, params: ExecParams) u8 {
+    // Build a strip command
+    const strip_cmd = blk: {
+        var list = ArrayListUnmanaged([]const u8){};
+        list.appendSlice(arena, &[_][]const u8{ "strip", "-S" }) catch |err| {
+            return fatal("error building strip cmd: {s}\n", .{@errorName(err)});
+        };
+
+        // Find any output targets
+        var next = false;
+        for (params.args.items) |param| {
+            if (mem.eql(u8, param, "-o")) {
+                next = true;
+            } else if (next) {
+                list.append(arena, param) catch |err| {
+                    return fatal("error adding output target: {s}\n", .{@errorName(err)});
+                };
+                break;
+            }
+        }
+        break :blk list;
+    };
+
+    // Run the intended zig process
+    var proc = ChildProcess.init(params.args.items, arena);
+    proc.env_map = &params.env;
+
+    const ret = proc.spawnAndWait() catch |err| {
+        return fatal("error spawning {s}: {s}\n", .{ params.args.items[0], @errorName(err) });
+    };
+
+    const code = switch (ret) {
+        .Exited => |code| code,
+        else => |other| return fatal("abnormal exit: {any}\n", .{other}),
+    };
+
+    // Run strip command, ignore output
+    var strip_proc = ChildProcess.init(strip_cmd.items, arena);
+    strip_proc.env_map = &params.env;
+    strip_proc.stdout_behavior = .Ignore;
+    strip_proc.stderr_behavior = .Ignore;
+
+    _ = strip_proc.spawnAndWait() catch |err| {
+        return fatal("running strip: {s}\n", .{@errorName(err)});
+    };
+
+    return code;
+}
+
 // argv_it is an object that has such method:
 //     fn next(self: *Self) ?[]const u8
 // in non-testing code it is *process.ArgIterator.
@@ -217,9 +282,19 @@ fn parseArgs(
     var env = process.getEnvMap(arena) catch |err|
         return parseFatal(arena, "error getting env: {s}", .{@errorName(err)});
 
+    // Get the current working directory (PWD)
+    const allocator = std.heap.page_allocator;
+    const pwd = std.fs.cwd().realpathAlloc(allocator, ".") catch {
+        std.process.exit(1);
+    };
+    defer allocator.free(pwd);
+
+    const suffix = try makeSuffix(arena, pwd);
+    const cache_dir = try std.fmt.allocPrint(arena, "{s}/{s}", .{ CACHE_DIR, suffix });
+
     try env.put("ZIG_LIB_DIR", zig_lib_dir);
-    try env.put("ZIG_LOCAL_CACHE_DIR", CACHE_DIR);
-    try env.put("ZIG_GLOBAL_CACHE_DIR", CACHE_DIR);
+    try env.put("ZIG_LOCAL_CACHE_DIR", cache_dir);
+    try env.put("ZIG_GLOBAL_CACHE_DIR", cache_dir);
 
     // args is the path to the zig binary and args to it.
     var args = ArrayListUnmanaged([]const u8){};
@@ -283,7 +358,7 @@ fn getRunMode(self_exe: []const u8, self_base_noexe: []const u8) error{BadParent
         return error.BadParent;
 
     const got_os = it.next() orelse return error.BadParent;
-    if (mem.indexOf(u8, "linux,macos,windows,wasi", got_os) == null)
+    if (mem.indexOf(u8, "linux,macos,windows,wasi,freestanding", got_os) == null)
         return error.BadParent;
 
     // ABI triple is too much of a moving target



================================================
FILE: bazel/idl2json.bzl
================================================
"""
The module fetches the idl2json binary from the dfinity/idl2json repository
"""

IDL2JSON_BUILD = """
package(default_visibility = ["//visibility:public"])
exports_files(["idl2json"])
"""

VERSION = "0.9.2"
SHA256 = {
    "unknown-linux-musl.tar.gz": "c91863c56f94bcafa70ca6aa5aa3d937324075afdd0c0c222f27b5d11372fcfb",
    "apple-darwin.zip": "c7356d96ae091a47e180631492d1c45462e85042dcc3a7b68da60722fc87edba",
}

URL = "https://github.com/dfinity/idl2json/releases/download/v{version}/idl2json_cli-{arch}-{file}"

def _idl_to_json_impl(repository_ctx):
    os_arch = repository_ctx.os.arch

    # even if the macOS version is "x86_64" it runs on ARM chips because of
    # emulation
    if os_arch == "x86_64" or os_arch == "amd64" or os_arch == "aarch64":
        arch = "x86_64"
    else:
        fail("Unsupported architecture: '" + os_arch + "'")

    os_name = repository_ctx.os.name
    if os_name == "linux":
        file = "unknown-linux-musl.tar.gz"
    elif os_name == "mac os x":
        file = "apple-darwin.zip"
    else:
        fail("Unsupported operating system: " + os_name)

    if file not in SHA256:
        fail("Unsupported file: '" + file + "'")

    repository_ctx.report_progress("Fetching " + repository_ctx.name)
    repository_ctx.download_and_extract(url = URL.format(version = VERSION, file = file, arch = arch), sha256 = SHA256[file])
    repository_ctx.file("BUILD.bazel", IDL2JSON_BUILD, executable = True)

_idl_to_json = repository_rule(
    implementation = _idl_to_json_impl,
    attrs = {},
)

def idl_to_json(name):
    _idl_to_json(name = name)



================================================
FILE: bazel/libssh2-sys.patch
================================================
# Patch for determinism issues
# https://github.com/alexcrichton/ssh2-rs/issues/340
diff --git a/build.rs b/build.rs
index c4425ee..f99f132 100644
--- a/build.rs
+++ b/build.rs
@@ -173,6 +173,7 @@ fn main() {
         .unwrap();
     let version = &version_line[version_line.find('"').unwrap() + 1..version_line.len() - 1];

+    /*
     let pkgconfig = dst.join("lib/pkgconfig");
     fs::create_dir_all(&pkgconfig).unwrap();
     fs::write(
@@ -188,6 +189,7 @@ fn main() {
             .replace("@LIBSSH2VER@", version),
     )
     .unwrap();
+    */

     cfg.warnings(false);
     cfg.compile("ssh2");



================================================
FILE: bazel/lmdb_rkv_sys.patch
================================================
# patch our fork of the lmdb-rkv-sys to allow specifying the path
# to the built static archive
diff --git a/build.rs b/build.rs
index c422d52..b779ee0 100644
--- a/build.rs
+++ b/build.rs
@@ -57,7 +57,21 @@ fn main() {
         warn!("Building with `-fsanitize=fuzzer`.");
     }

-    if let Err(_) = std::env::var("LMDB_NO_BUILD") {
+    if let Ok(lmdb) = std::env::var("LMDB_OVERRIDE") {
+        let lmdb = PathBuf::from(lmdb);
+        assert!(
+            lmdb.exists(),
+            "Path to `lmdb` '{}' does not exist",
+            lmdb.display()
+        );
+        println!(
+            "cargo:rustc-link-search=native={}",
+            lmdb.parent().unwrap().display()
+        );
+        let stem = lmdb.file_stem().unwrap().to_str().unwrap();
+        println!("cargo:rustc-link-lib=static={}", &stem[3..]);
+        return;
+    } else {
         if pkg_config::probe_library("lmdb").is_err() {
             let mut lmdb = PathBuf::from(&env::var("CARGO_MANIFEST_DIR").unwrap());
             lmdb.push("lmdb");



================================================
FILE: bazel/mainnet-canisters.bzl
================================================
"""Mainnet canister definitions.

This creates a Bazel repository which exports 'canister_deps'. This macro can be
called to create one Bazel repository for each canister in the mainnet canister list.
The repository contains the canister module.
"""

def _canisters_impl(repository_ctx):
    reponames = dict(repository_ctx.attr.reponames)
    repositories = dict(repository_ctx.attr.repositories)
    filenames = dict(repository_ctx.attr.filenames)

    # Read and decode mainnet canister data
    cans = json.decode(repository_ctx.read(repository_ctx.attr.path))
    canister_keys = cans.keys()

    content = '''

load("@bazel_tools//tools/build_defs/repo:http.bzl", "http_file")

def canister_deps():
    '''

    # Iterate over all the keys defined in the mainnet canister data

    for canister_key in canister_keys:
        canisterinfo = cans.pop(canister_key, None)

        rev = canisterinfo.get("rev", None)
        if rev == None:
            repository = repositories.pop(canister_key, None)
            if repository == None:
                fail("no rev and repository for canister: " + canister_key)
            tag = canisterinfo.get("tag", None)
            if tag == None:
                fail("no rev and tag for canister: " + canister_key)
        else:
            repository = None
            tag = None

        sha256 = canisterinfo.get("sha256", None)
        if sha256 == None:
            fail("no sha256 for canister: " + canister_key)

        filename = filenames.pop(canister_key, None)
        if filename == None:
            fail("no filename for canister: " + canister_key)

        reponame = reponames.pop(canister_key, None)
        if reponame == None:
            fail("no reponame for canister: " + canister_key)

        if rev == None:
            url = "https://github.com/{repository}/releases/download/{tag}/{filename}".format(repository = repository, tag = tag, filename = filename)
        else:
            url = "https://download.dfinity.systems/ic/{rev}/canisters/{filename}".format(rev = rev, filename = filename)

        content += '''

    http_file(
        name = "{reponame}",
        downloaded_file_path = "{filename}",
        sha256 = "{sha256}",
        url = "{url}",
)

        '''.format(rev = rev, filename = filename, sha256 = sha256, reponame = reponame, url = url)

    if len(cans.keys()) != 0:
        fail("unused canisters: " + ", ".join(cans.keys()))

    if len(reponames.keys()) != 0:
        fail("unused reponames: " + ", ".join(reponames.keys()))

    if len(repositories.keys()) != 0:
        fail("unused repositories: " + ", ".join(repositories.keys()))

    if len(filenames.keys()) != 0:
        fail("unused filenames: " + ", ".join(filenames.keys()))

    repository_ctx.file("BUILD.bazel", content = "\n", executable = False)
    repository_ctx.file(
        "defs.bzl",
        content = content,
        executable = False,
    )

_canisters = repository_rule(
    implementation = _canisters_impl,
    attrs = {
        "path": attr.label(mandatory = True, doc = "path to mainnet canister data"),
        "reponames": attr.string_dict(mandatory = True, doc = "mapping from canister key to generated Bazel repository name"),
        "repositories": attr.string_dict(mandatory = True, doc = "mapping from canister key to GitHub repository name"),
        "filenames": attr.string_dict(mandatory = True, doc = "mapping from canister key to filename as per the DFINITY CDN"),
    },
)

def canisters(name, path, reponames, repositories, filenames):
    _canisters(name = name, path = path, reponames = reponames, repositories = repositories, filenames = filenames)



================================================
FILE: bazel/mainnet-icos-images.bzl
================================================
"""
This module defines Bazel targets for the mainnet versions of ICOS images
"""

load("@bazel_tools//tools/build_defs/repo:http.bzl", "http_file")
load("@mainnet_icos_versions//:defs.bzl", "mainnet_icos_versions")

MAINNET_LATEST = {
    "version": mainnet_icos_versions["guestos"]["latest_release"]["version"],
    "hash": mainnet_icos_versions["guestos"]["latest_release"]["update_img_hash"],
    "dev_hash": mainnet_icos_versions["guestos"]["latest_release"]["update_img_hash_dev"],
    "launch_measurements": mainnet_icos_versions["guestos"]["latest_release"]["launch_measurements"],
    "dev_launch_measurements": mainnet_icos_versions["guestos"]["latest_release"]["launch_measurements_dev"],
}
MAINNET_NNS = {
    "version": mainnet_icos_versions["guestos"]["subnets"]["tdb26-jop6k-aogll-7ltgs-eruif-6kk7m-qpktf-gdiqx-mxtrf-vb5e6-eqe"]["version"],
    "hash": mainnet_icos_versions["guestos"]["subnets"]["tdb26-jop6k-aogll-7ltgs-eruif-6kk7m-qpktf-gdiqx-mxtrf-vb5e6-eqe"]["update_img_hash"],
    "dev_hash": mainnet_icos_versions["guestos"]["subnets"]["tdb26-jop6k-aogll-7ltgs-eruif-6kk7m-qpktf-gdiqx-mxtrf-vb5e6-eqe"]["update_img_hash_dev"],
    "launch_measurements": mainnet_icos_versions["guestos"]["subnets"]["tdb26-jop6k-aogll-7ltgs-eruif-6kk7m-qpktf-gdiqx-mxtrf-vb5e6-eqe"]["launch_measurements"],
    "dev_launch_measurements": mainnet_icos_versions["guestos"]["subnets"]["tdb26-jop6k-aogll-7ltgs-eruif-6kk7m-qpktf-gdiqx-mxtrf-vb5e6-eqe"]["launch_measurements_dev"],
}
MAINNET_APP = {
    "version": mainnet_icos_versions["guestos"]["subnets"]["io67a-2jmkw-zup3h-snbwi-g6a5n-rm5dn-b6png-lvdpl-nqnto-yih6l-gqe"]["version"],
    "hash": mainnet_icos_versions["guestos"]["subnets"]["io67a-2jmkw-zup3h-snbwi-g6a5n-rm5dn-b6png-lvdpl-nqnto-yih6l-gqe"]["update_img_hash"],
    "dev_hash": mainnet_icos_versions["guestos"]["subnets"]["io67a-2jmkw-zup3h-snbwi-g6a5n-rm5dn-b6png-lvdpl-nqnto-yih6l-gqe"]["update_img_hash_dev"],
    "launch_measurements": mainnet_icos_versions["guestos"]["subnets"]["io67a-2jmkw-zup3h-snbwi-g6a5n-rm5dn-b6png-lvdpl-nqnto-yih6l-gqe"]["launch_measurements"],
    "dev_launch_measurements": mainnet_icos_versions["guestos"]["subnets"]["io67a-2jmkw-zup3h-snbwi-g6a5n-rm5dn-b6png-lvdpl-nqnto-yih6l-gqe"]["launch_measurements_dev"],
}
MAINNET_LATEST_HOSTOS = {
    "version": mainnet_icos_versions["hostos"]["latest_release"]["version"],
    "hash": mainnet_icos_versions["hostos"]["latest_release"]["update_img_hash"],
    "dev_hash": mainnet_icos_versions["hostos"]["latest_release"]["update_img_hash_dev"],
    "launch_measurements": mainnet_icos_versions["hostos"]["latest_release"]["launch_measurements"],
    "dev_launch_measurements": mainnet_icos_versions["hostos"]["latest_release"]["launch_measurements_dev"],
}

def icos_image_download_url(git_commit_id, variant, update):
    return "https://download.dfinity.systems/ic/{git_commit_id}/{variant}/{component}/{component}.tar.zst".format(
        git_commit_id = git_commit_id,
        variant = variant,
        component = "update-img" if update else "disk-img",
    )

def icos_dev_image_download_url(git_commit_id, variant, update):
    return "https://download.dfinity.systems/ic/{git_commit_id}/{variant}/{component}-dev/{component}.tar.zst".format(
        git_commit_id = git_commit_id,
        variant = variant,
        component = "update-img" if update else "disk-img",
    )

def get_mainnet_setupos_images():
    """
    Pull the requested SetupOS mainnet images, and their measurements.
    """

    versions = {
        "mainnet_latest_setupos_disk_image": MAINNET_LATEST_HOSTOS,
    }

    for (name, info) in versions.items():
        http_file(
            name = name,
            downloaded_file_path = "disk-img.tar.zst",
            url = icos_image_download_url(info["version"], "setup-os", False),
        )

        # TODO: This could live in the same repo as above
        _mainnet_measurements(
            name = name + "_launch_measurements",
            measurements = json.encode(info["launch_measurements"]),
        )

        http_file(
            name = name + "_dev",
            downloaded_file_path = "disk-img.tar.zst",
            url = icos_dev_image_download_url(info["version"], "setup-os", False),
        )

        # TODO: This could live in the same repo as above
        _mainnet_measurements(
            name = name + "_dev_launch_measurements",
            measurements = json.encode(info["dev_launch_measurements"]),
        )

def get_mainnet_guestos_images():
    versions = {
        "mainnet_latest_guest_img": MAINNET_LATEST,
        "mainnet_nns_guest_img": MAINNET_NNS,
        "mainnet_app_guest_img": MAINNET_APP,
    }

    for (name, info) in versions.items():
        _get_mainnet_guestos_image(
            name = name,
            setupos_url = icos_image_download_url(info["version"], "setup-os", False),
            measurements = json.encode(info["launch_measurements"]),
        )

        _get_mainnet_guestos_image(
            name = name + "_dev",
            setupos_url = icos_dev_image_download_url(info["version"], "setup-os", False),
            measurements = json.encode(info["dev_launch_measurements"]),
        )

_DEFS_CONTENTS = '''\
def extract_image(name, **kwargs):
    native.genrule(
        name = name,
        srcs = ["disk-img.tar.zst"],
        outs = [name + ".tar.zst"],
        cmd = """#!/bin/bash
            $(location @@//rs/ic_os/build_tools/partition_tools:extract-guestos) --image $< $@
        """,
        target_compatible_with = ["@platforms//os:linux"],
        tools = ["@@//rs/ic_os/build_tools/partition_tools:extract-guestos"],
        **kwargs
    )
'''

_BUILD_CONTENTS = """\
load(":defs.bzl", "extract_image")
load("@bazel_skylib//rules:write_file.bzl", "write_file")

package(default_visibility = ["//visibility:public"])

extract_image("{name}")

write_file(
    name = "launch_measurements",
    out = "launch-measurements.json",
    content = ['''{measurements}'''],
)
"""

_attrs = {
    "setupos_url": attr.string(mandatory = True, doc = "URL to the SetupOS image to extract from"),
    "setupos_integrity": attr.string(doc = "Optional integrity for the image. If unset, it will be set after the image is downloaded."),
    "measurements": attr.string(mandatory = True, doc = "Launch measurements for the GuestOS version extracted."),
}

def _copy_attrs(repository_ctx, attrs):
    orig = repository_ctx.attr
    keys = attrs.keys()

    result = {}
    for key in keys:
        if hasattr(orig, key):
            result[key] = getattr(orig, key)
    result["name"] = orig.name

    return result

# Implementation based on https://github.com/bazelbuild/bazel/blob/a0bcf66154b37882d9ea2fd6be6137c949dea43b/tools/build_defs/repo/http.bzl#L524,
# simplified, to allow multiple files in the same created repo.
def _get_mainnet_guestos_image_impl(repository_ctx):
    download_info = repository_ctx.download(
        repository_ctx.attr.setupos_url,
        "disk-img.tar.zst",
        integrity = repository_ctx.attr.setupos_integrity,
    )

    repository_ctx.file("defs.bzl", content = _DEFS_CONTENTS)
    repository_ctx.file("BUILD.bazel", content = _BUILD_CONTENTS.format(name = repository_ctx.name, measurements = repository_ctx.attr.measurements))

    new_attrs = _copy_attrs(repository_ctx, _attrs)
    new_attrs.update({"setupos_integrity": download_info.integrity})

    return new_attrs

_get_mainnet_guestos_image = repository_rule(
    implementation = _get_mainnet_guestos_image_impl,
    attrs = _attrs,
)

def _mainnet_measurements_impl(repository_ctx):
    repository_ctx.file("BUILD.bazel", content = """\
load("@bazel_skylib//rules:write_file.bzl", "write_file")

package(default_visibility = ["//visibility:public"])

write_file(
    name = "{name}",
    out = "launch-measurements.json",
    content = ['''{measurements}'''],
)
""".format(name = repository_ctx.name, measurements = repository_ctx.attr.measurements))

_mainnet_measurements = repository_rule(
    implementation = _mainnet_measurements_impl,
    attrs = {
        "measurements": attr.string(mandatory = True, doc = "Launch measurements to expose as file."),
    },
)



================================================
FILE: bazel/mainnet-icos-versions.bzl
================================================
"""Mainnet version definitions.

This creates a Bazel repository which exports 'mainnet_icos_versions'. This macro can be
called to create one Bazel repository for the entire mainnet ICOS versions list.
"""

def _mainnet_icos_versions_impl(repository_ctx):
    # Read and decode mainnet version data
    versions = json.decode(repository_ctx.read(repository_ctx.attr.path))

    # Create a minimal BUILD.bazel file (Bazel requires it)
    repository_ctx.file("BUILD.bazel", content = "\n")

    content = "mainnet_icos_versions = %s" % versions
    repository_ctx.file("defs.bzl", content = content)

mainnet_icos_versions = repository_rule(
    implementation = _mainnet_icos_versions_impl,
    attrs = {
        "path": attr.label(mandatory = True, doc = "path to mainnet ICOS versions data"),
    },
)



================================================
FILE: bazel/noble.yaml
================================================
# Packages used by our ubuntu base, adapted from:
#  https://github.com/GoogleContainerTools/rules_distroless/blob/2ce7b477def75579c49bab25266f953f30275c88/examples/ubuntu_snapshot/BUILD.bazel
#
#  Anytime this file is changed, the lockfile needs to be regenerated. See MODULE
#  for instructions.
version: 1

# Various channels used to pull packages from
sources:
  - channel: noble main
    url: https://snapshot.ubuntu.com/ubuntu/20250804T000000Z
  - channel: noble universe
    url: https://snapshot.ubuntu.com/ubuntu/20250804T000000Z
  - channel: noble-security main
    url: https://snapshot.ubuntu.com/ubuntu/20250804T000000Z
  - channel: noble-updates main
    url: https://snapshot.ubuntu.com/ubuntu/20250804T000000Z

archs:
  - "amd64"

packages:
  - "apt"
  - "bash"
  - "ca-certificates"
  - "coreutils" # for chmod
  - "dmsetup"
  - "dosfstools"
  - "dpkg" # for apt list --installed
  - "gawk" # for build-bootstrap-config-image
  - "gzip" # for tar-ing up ic regsitry store in systests
  - "libcryptsetup-dev"
  - "libssl3t64"
  - "libunwind8"
  - "mtools"
  - "openssh-client" # used to SSH into image
  - "perl"
  - "rsync"
  - "tar"
  - "udev" # for device-mapper tests
  - "xz-utils"
  - "zstd"



================================================
FILE: bazel/os_info.bzl
================================================
"""
The module exports information on the OS in like "x86_64-linux"
"""

def _os_info_impl(repository_ctx):
    # Figure out the arch
    os_arch = repository_ctx.os.arch
    is_x86_64 = os_arch == "x86" or os_arch == "x86_64" or os_arch == "amd64"
    is_arm_64 = os_arch == "arm" or os_arch == "arm64" or os_arch == "aarch64"

    # Figure out the OS
    os_name = repository_ctx.os.name
    is_linux = os_name.lower().startswith("linux")
    is_darwin = os_name.lower().startswith("mac")

    os_arch = "x86_64" if is_x86_64 else "arm64" if is_arm_64 else "unknown"
    os_name = "linux" if is_linux else "darwin" if is_darwin else "unknown"

    os_info = os_arch + "-" + os_name

    # Create a minimal BUILD.bazel file (Bazel requires it)
    repository_ctx.file("BUILD.bazel", content = "\n")
    repository_ctx.file("defs.bzl", "os_info = \"{}\"".format(os_info))

os_info = repository_rule(
    implementation = _os_info_impl,
    attrs = {},
)



================================================
FILE: bazel/pocket-ic-tests.bzl
================================================
"""Build helpers for using the pocket-ic"""

load("@bazel_skylib//lib:paths.bzl", "paths")

# This defines a transition used in the pocket-ic declaration.
# This allows tests to depend on an arbitrary version of pocket-ic
# and for dependents to override that version.
def _pocket_ic_mainnet_transition_impl(_settings, _attr):
    return {
        "//:pocket-ic-server-variant": "mainnet",
    }

pocket_ic_mainnet_transition = transition(
    implementation = _pocket_ic_mainnet_transition_impl,
    inputs = [],
    outputs = [
        "//:pocket-ic-server-variant",
    ],
)

# Provider that allows wrapping/transitioning a test executable. This
# ensure all the test data & env is forwarded.
# Adapted from github.com/aherrman/bazel-transitions-demo:
# https://github.com/aherrmann/bazel-transitions-demo/blob/f22cf40a62131eace14829f262e8d7c00b0a9a19/flag/defs.bzl#L124
TestAspectInfo = provider("some descr", fields = ["args", "env"])

def _test_aspect_impl(_target, ctx):
    data = getattr(ctx.rule.attr, "data", [])
    args = getattr(ctx.rule.attr, "args", [])
    env = getattr(ctx.rule.attr, "env", [])
    args = [ctx.expand_location(arg, data) for arg in args]
    env = {k: ctx.expand_location(v, data) for (k, v) in env.items()}
    return [TestAspectInfo(
        args = args,
        env = env,
    )]

_test_aspect = aspect(_test_aspect_impl)

def _pocket_ic_mainnet_test_impl(ctx):
    test_aspect_info = ctx.attr.test[TestAspectInfo]
    (_, extension) = paths.split_extension(ctx.executable.test.path)
    executable = ctx.actions.declare_file(
        ctx.label.name + extension,
    )
    ctx.actions.write(
        output = executable,
        content = """\
#!/usr/bin/env bash
set -euo pipefail
{commands}
""".format(
            commands = "\n".join([
                " \\\n".join([
                    '{}="{}"'.format(k, v)
                    for k, v in test_aspect_info.env.items()
                ] + [
                    ctx.executable.test.short_path,
                ] + test_aspect_info.args),
            ]),
        ),
        is_executable = True,
    )

    runfiles = ctx.runfiles(files = [executable, ctx.executable.test] + ctx.files.data)
    runfiles = runfiles.merge(ctx.attr.test[DefaultInfo].default_runfiles)
    for data_dep in ctx.attr.data:
        runfiles = runfiles.merge(data_dep[DefaultInfo].default_runfiles)

    return [DefaultInfo(
        executable = executable,
        files = depset(direct = [executable]),
        runfiles = runfiles,
    )]

# Rule to override another (test) rule. The resulting rule
# will use the mainnet pocket-ic variant.
pocket_ic_mainnet_test = rule(
    _pocket_ic_mainnet_test_impl,
    attrs = {
        "_allowlist_function_transition": attr.label(
            default = "@bazel_tools//tools/allowlists/function_transition_allowlist",
        ),
        "data": attr.label_list(allow_files = True),
        "test": attr.label(
            aspects = [_test_aspect],
            cfg = "target",
            executable = True,
        ),
    },
    cfg = pocket_ic_mainnet_transition,
    executable = True,
    test = True,
)



================================================
FILE: bazel/prost.bzl
================================================
"""
This module contains utilities to work with code generated by prost-build.
"""

load("@rules_rust//rust:defs.bzl", "rust_binary", "rust_test")
load("@rules_shell//shell:sh_binary.bzl", "sh_binary")

# the prost crate expects PROTOC_INCLUDE to point to a directory containing
# various .proto files. Creating a directory is not straightforward in Bazel
# so as a workaround we simply point to the sources.
PROTOC_INCLUDE = "external/protobuf~/src"

def generated_files_check(name, srcs, deps, data, manifest_dir):
    rust_test(
        name = name,
        srcs = srcs,
        data = data + [
            "@rules_rust//rust/toolchain:current_rustfmt_files",
            "@com_google_protobuf//:protoc",
            "@com_google_protobuf//:descriptor_proto_srcs",
            "@com_google_protobuf//:protobuf_headers",
            "@com_google_protobuf//:well_known_type_protos",
        ],
        env = {
            "PROTOC": "$(rootpath @com_google_protobuf//:protoc)",
            "PROTOC_INCLUDE": PROTOC_INCLUDE,
            "CARGO_MANIFEST_DIR": manifest_dir,
            "RUSTFMT": "$(rootpath @rules_rust//rust/toolchain:current_rustfmt_files)",
        },
        deps = deps,
    )

def protobuf_generator(name, srcs, manifest_dir, deps = [], data = []):
    binary_name = "_%s_bin" % name
    rust_binary(
        name = binary_name,
        srcs = srcs,
        data = data,
        deps = deps,
    )
    sh_binary(
        name = name,
        data = data + [
            ":" + binary_name,
            "@com_google_protobuf//:well_known_type_protos",
            "@com_google_protobuf//:descriptor_proto_srcs",
            "@com_google_protobuf//:protoc",
            "@com_google_protobuf//:protobuf_headers",
            "@rules_rust//rust/toolchain:current_rustfmt_files",
        ],
        srcs = ["//bazel:prost_generator.sh"],
        env = {
            "PROTOC": "$(location @com_google_protobuf//:protoc)",
            "PROTOC_INCLUDE": PROTOC_INCLUDE,
            "CARGO_MANIFEST_DIR": manifest_dir,
            "GENERATOR": "$(location :%s)" % binary_name,
            "RUSTFMT": "$(rootpath @rules_rust//rust/toolchain:current_rustfmt_files)",
        },
        tags = ["local", "manual", "pb-generator"],
    )



================================================
FILE: bazel/prost_generator.sh
================================================
#!/bin/bash
set -e
CARGO_MANIFEST_DIR=$BUILD_WORKSPACE_DIRECTORY/$CARGO_MANIFEST_DIR
$GENERATOR



================================================
FILE: bazel/rosetta_cli.bzl
================================================
"""
This module defines a repository rule for accessing the rosetta-cli tool.
See https://github.com/coinbase/rosetta-cli for more detail.
"""

ROSETTA_CLI_BUILD = """
package(default_visibility = ["//visibility:public"])
exports_files(["rosetta-cli"])
"""

BINARIES = {
    "darwin-amd64": {
        "url": "https://github.com/coinbase/mesh-cli/releases/download/v0.10.3/rosetta-cli-0.10.3-darwin-amd64.tar.gz",
        "sha256": "6426d69d8ce6851a00eb47c4b3bb5a5f2f792daca58a58d077ac5ade7c07f42b",
    },
    "darwin-arm64": {
        "url": "https://github.com/coinbase/mesh-cli/releases/download/v0.10.3/rosetta-cli-0.10.3-darwin-arm64.tar.gz",
        "sha256": "41eaa23bc2a34568549e7c0d99c3471fd0c20959ffb7a1e9fd2acc658f500b20",
    },
    "linux-amd64": {
        "url": "https://github.com/coinbase/mesh-cli/releases/download/v0.10.3/rosetta-cli-0.10.3-linux-amd64.tar.gz",
        "sha256": "1ea96b427dfa69a93d2915bc57669014b58d66a9ee7d761509d50b66486d42f8",
    },
}

def _rosetta_cli_impl(repository_ctx):
    os_name = repository_ctx.os.name
    if os_name == "linux":
        os = "linux"
    elif os_name == "mac os x":
        os = "darwin"
    else:
        fail("Unsupported operating system: " + os_name)

    os_arch = repository_ctx.os.arch
    if os_arch == "x86_64":
        platform = os + "-amd64"
    elif os_arch == "amd64":
        platform = os + "-amd64"
    elif os_arch == "aarch64":
        platform = os + "-arm64"
    else:
        fail("Unsupported architecture: '" + os_arch + "'")

    if platform not in BINARIES:
        fail("Unsupported platform: '" + platform + "'")

    bin = BINARIES[platform]

    repository_ctx.report_progress("Fetching rosetta-cli")
    repository_ctx.download_and_extract(url = bin["url"], sha256 = bin["sha256"])
    repository_ctx.symlink("rosetta-cli-0.10.3-" + platform, "rosetta-cli")
    repository_ctx.file("BUILD.bazel", ROSETTA_CLI_BUILD, executable = False)

_rosetta_cli = repository_rule(
    implementation = _rosetta_cli_impl,
    attrs = {},
)

def rosetta_cli_repository(name):
    _rosetta_cli(name = name)



================================================
FILE: bazel/rules_foreign_cc.patch
================================================
# Avoid build logs that create sources of non-determinism
diff --git a/foreign_cc/private/framework.bzl b/foreign_cc/private/framework.bzl
index 33129b8..7326107 100644
--- a/foreign_cc/private/framework.bzl
+++ b/foreign_cc/private/framework.bzl
@@ -616,7 +616,7 @@ def wrap_outputs(ctx, lib_name, configure_name, script_text, env_prelude, build_
     cleanup_on_success_function = create_function(
         ctx,
         "cleanup_on_success",
-        "rm -rf $$BUILD_TMPDIR$$ $$EXT_BUILD_DEPS$$",
+        "rm -rf $$BUILD_TMPDIR$$ $$EXT_BUILD_DEPS$$ && echo > $$BUILD_LOG$$",
     )
     cleanup_on_failure_function = create_function(
         ctx,



================================================
FILE: bazel/rules_haskell.patch
================================================
# workaround for https://github.com/tweag/rules_haskell/issues/2254
diff --git a/haskell/private/workspace_utils.bzl b/haskell/private/workspace_utils.bzl
index df851329..980b3e16 100644
--- a/haskell/private/workspace_utils.bzl
+++ b/haskell/private/workspace_utils.bzl
@@ -19,6 +19,8 @@ def execute_or_fail_loudly(
         environment = environment,
         quiet = True,
         working_directory = working_directory,
+        # triple the default timeout because e.g. copying GHC is slow (1.5G)
+        timeout = 3 * 600,
     )
     if exec_result.return_code != 0:
         arguments = [_as_string(x) for x in arguments]



================================================
FILE: bazel/shfmt.bzl
================================================
"""
The module fetches shfmt binary to be used by bazel smoke test
"""

SHFMT_BUILD = """
package(default_visibility = ["//visibility:public"])
exports_files(["shfmt"])
"""

VERSION = "3.6.0"
SHA256 = {
    "darwin_amd64": "b8c9c025b498e2816b62f0b717f6032e9ab49e725a45b8205f52f66318f17185",
    "darwin_arm64": "633f242246ee0a866c5f5df25cbf61b6af0d5e143555aca32950059cf13d91e0",
    "linux_amd64": "5741a02a641de7e56b8da170e71a97e58050d66a3cf485fb268d6a5a8bb74afb",
}

URL = "https://github.com/mvdan/sh/releases/download/v{version}/shfmt_v{version}_{platform}"

FILE_SYM = "shfmt_v{version}_{platform}"

def _shfmt_impl(repository_ctx):
    os_arch = repository_ctx.os.arch

    if os_arch == "x86_64" or os_arch == "amd64":
        arch = "amd64"
    elif os_arch == "aarch64":
        arch = "arm64"
    else:
        fail("Unsupported architecture: '" + os_arch + "'")

    os_name = repository_ctx.os.name
    if os_name == "linux":
        platform = "linux_" + arch
    elif os_name == "mac os x":
        platform = "darwin_" + arch

    else:
        fail("Unsupported operating system: " + os_name)

    if platform not in SHA256:
        fail("Unsupported platform: '" + platform + "'")

    repository_ctx.report_progress("Fetching " + repository_ctx.name)
    repository_ctx.download(url = URL.format(version = VERSION, platform = platform), sha256 = SHA256[platform], executable = True, output = "shfmt")
    repository_ctx.file("BUILD.bazel", SHFMT_BUILD, executable = True)

_shfmt = repository_rule(
    implementation = _shfmt_impl,
    attrs = {},
)

def shfmt(name):
    _shfmt(name = name)



================================================
FILE: bazel/sns_quill.bzl
================================================
"""
The module fetches the sns-quill binary from the dfinity/sns-quill repository
"""

SNS_QUILL_BUILD = """
package(default_visibility = ["//visibility:public"])
exports_files(["sns-quill"])
"""

VERSION = "0.4.2-beta.1"
SHA256 = {
    "linux": "88c00f0da9c4963922c67514e9321401560e2eaa56b6f505a8f5356c5ffe53bf",
    "macos": "6f9363ea9aecbeebb2f079ea42f65d602282d19550506207f8b8538c37466906",
}

URL = "https://github.com/dfinity/sns-quill/releases/download/v{version}/sns-quill-{platform}-{arch}"

def _sns_quill_impl(repository_ctx):
    os_arch = repository_ctx.os.arch

    # even if the macOS version is "x86_64" it runs on ARM chips because of
    # emulation
    if os_arch == "x86_64" or os_arch == "amd64" or os_arch == "aarch64":
        arch = "x86_64"
    else:
        fail("Unsupported architecture: '" + os_arch + "'")

    os_name = repository_ctx.os.name
    if os_name == "linux":
        platform = "linux"
    elif os_name == "mac os x":
        platform = "macos"
    else:
        fail("Unsupported operating system: " + os_name)

    if platform not in SHA256:
        fail("Unsupported platform: '" + platform + "'")

    repository_ctx.report_progress("Fetching " + repository_ctx.name)
    repository_ctx.download(
        url = URL.format(version = VERSION, platform = platform, arch = arch),
        sha256 = SHA256[platform],
        executable = True,
        output = "sns-quill",
    )
    repository_ctx.file("BUILD.bazel", SNS_QUILL_BUILD, executable = True)

_sns_quill = repository_rule(
    implementation = _sns_quill_impl,
    attrs = {},
)

def sns_quill(name):
    _sns_quill(name = name)



================================================
FILE: bazel/trivy.bzl
================================================
"""
The module fetches trivy binary to be used by docker_vulnerabilities_scanning
"""

TRIVY_BUILD = """
package(default_visibility = ["//visibility:public"])
exports_files(["trivy"])
"""

VERSION = "0.35.0"
SHA256 = {
    "macOS-64bit": "165c501451ab91a486678107c34bfb7b0b9e117c8ed5bba746ebda31923d9330",
    "macOS-ARM64": "343fb01737e886bde797f30e5e6359fc1a7b7e1e9807f0e62ae3aea3f9508a1a",
    "Linux-64bit": "ebc1dd4d4c0594028d6a501dfc1a73d56add20b29d3dee5ab6e64aac94b1d526",
}

URL = "https://github.com/aquasecurity/trivy/releases/download/v{version}/trivy_{version}_{platform}.tar.gz"

def _trivy_impl(repository_ctx):
    os_arch = repository_ctx.os.arch

    if os_arch == "x86_64" or os_arch == "amd64":
        arch = "64bit"
    elif os_arch == "aarch64":
        arch = "ARM64"
    else:
        fail("Unsupported architecture: '" + os_arch + "'")

    os_name = repository_ctx.os.name
    if os_name == "linux":
        platform = "Linux-" + arch
    elif os_name == "mac os x":
        platform = "macOS-" + arch
    else:
        fail("Unsupported operating system: " + os_name)

    if platform not in SHA256:
        fail("Unsupported platform: '" + platform + "'")

    repository_ctx.report_progress("Fetching " + repository_ctx.name)
    repository_ctx.download_and_extract(url = URL.format(version = VERSION, platform = platform), sha256 = SHA256[platform])
    repository_ctx.file("BUILD.bazel", TRIVY_BUILD, executable = False)

_trivy = repository_rule(
    implementation = _trivy_impl,
    attrs = {},
)

def trivy_scan(name):
    _trivy(name = name)



================================================
FILE: bazel/upload_systest_dep.sh
================================================
#!/usr/bin/env bash

set -euo pipefail

# Uploads a dependency to shared storage and returns the download URL.
#
# The path to the dependency should be specified as the first (and only) argument.
#
# The download URL is printed to stdout.

# NOTE: This script uses bazel-remote as the CAS storage (implementation detail).

# Look up a CAS key (provided as $1) through the redirect server.
# If the key exists, then then download URL is returned (through stdout).
# If the key does not exist, the empty string is returned.
lookup_dep_url() {
    REDIRECT_SERVER_URL="https://artifacts.idx.dfinity.network"
    local redirect_url="$REDIRECT_SERVER_URL/cas/$1"
    local result
    result=$(curl --silent --head \
        -w '%{http_code} %{redirect_url}' \
        "$redirect_url" \
        | tail -n1)

    local result_code
    result_code=$(cut -d' ' -f1 <<<"$result")
    if [ "$result_code" == "404" ]; then
        # The key was not found
        return
    fi

    if [ "$result_code" != "307" ]; then
        echo "Expected 404 or 307 when looking up dependency '$1', got '$result_code'" >&2
        exit 1
    fi

    local result_url
    result_url=$(cut -d' ' -f2 <<<"$result")
    if [ -z "$result_url" ]; then
        echo "Looking up dependency '$1' did not return a URL, got: '$result'" >&2
        exit 1
    fi

    echo "$result_url"
}

dep_filename="${1:?Dependency not specified}"
dep_sha256=$(sha256sum "$dep_filename" | cut -d' ' -f1)

echo "Found dep to upload $dep_filename ($dep_sha256)" >&2
result_url=$(lookup_dep_url "$dep_sha256")

# First, figure out _if_ the dep should be uploaded (no point re-uploading several GBs
# if it's been uploaded already)
if [ -n "$result_url" ]; then
    echo "dep '$dep_filename': already uploaded" >&2
else
    echo "dep '$dep_filename': not uploaded yet" >&2

    # We use bazel-remote as a CAS storage
    UPLOAD_URL="http://server.bazel-remote.svc.cluster.local:8080/cas"

    # Upload the dep
    dep_upload_url="$UPLOAD_URL/$dep_sha256"
    echo "Using upload URL: '$dep_upload_url'" >&2
    curl_out=$(mktemp)
    curl --silent --show-error --fail --retry 3 "$dep_upload_url" --upload-file "$dep_filename" -w '%{size_upload} %{time_total} %{speed_upload}\n' | tee "$curl_out" >&2
    # read & pretty print 3 metrics: upload size, upload time & upload speed
    if read -ra metrics <"$curl_out"; then
        echo "Uploaded $(numfmt --to=iec-i --suffix=B "${metrics[0]}") in ${metrics[1]}s ($(numfmt --to=iec-i --suffix=B "${metrics[2]}")/s)" >&2
    fi

    rm "$curl_out"

    # Check that it was actually uploaded and can be served (this sometimes takes a minute)
    attempt=1
    result_url=
    while true; do
        result_url=$(lookup_dep_url "$dep_sha256")

        if [ -n "$result_url" ]; then
            break
        fi

        echo "attempt $attempt failed" >&2
        if [ "$attempt" -ge 10 ]; then
            echo "  giving up" >&2
            exit 1
        fi

        echo "  will retry in 1s" >&2
        sleep 1

        attempt=$((attempt + 1))
    done
fi

# extract cluster
# NOTE: this assumes the result URL is https://artifacts.<CLUSTER>.dfinity.network/...
cluster=$(sed <<<"$result_url" -n -E 's$^https://artifacts.([^.]+).*$\1$p')
if [ -z "$cluster" ]; then
    echo "could not read cluster from '$result_url'" >&2
    exit 1
fi

echo "dep '$dep_filename': cluster is '$cluster'" >&2

# Use the direct URL, without going through the redirect server
dep_download_url="http://$cluster.artifacts.proxy-global.dfinity.network:8080/cas/$dep_sha256"
echo "dep '$dep_filename': download_url: '$dep_download_url'" >&2
echo "$dep_download_url"



================================================
FILE: bazel/workspace_status.sh
================================================
#!/usr/bin/env bash

set -euo pipefail

# By default, we set a hardcoded, constant version to avoid rebuilds. Only when
# --stamp is provided do we write a meaningful version.
if [ "$#" == "0" ]; then
    echo "STABLE_VERSION 0000000000000000000000000000000000000000"
    echo "STABLE_COMMIT_TIMESTAMP 4000000000" # arbitrary (constant) timestamp
    echo "STABLE_COMMIT_DATE_ISO_8601 0000-00-00T00:00:00+00:00"
elif [ "$#" == "1" ] && [ "$1" == "--stamp" ]; then
    version="$(git rev-parse HEAD)"
    # If the checkout is not clean, mark the version as dirty
    if [ -n "$(git status --porcelain)" ]; then
        version="$version-dirty"
    fi
    echo "STABLE_VERSION $version"
    echo "STABLE_COMMIT_TIMESTAMP $(git show -s --format=%ct)"
    echo "STABLE_COMMIT_DATE_ISO_8601 $(git show -s --format=%cI)"
else
    exit 1
fi

# Used to read credentials for S3 upload
echo "HOME ${HOME}"

# Used as farm metadata
test -n "${CI_JOB_NAME:-}" && echo "STABLE_FARM_JOB_NAME ${CI_JOB_NAME}"
if [[ -n "${USER:-}" ]]; then
    echo "STABLE_FARM_USER ${USER}"
elif [[ -n "${HOSTUSER:-}" ]]; then
    echo "STABLE_FARM_USER ${HOSTUSER}"
fi



================================================
FILE: bazel/candid_integration_tests/BUILD.bazel
================================================
load("@python_deps//:requirements.bzl", "requirement")
load("@rules_python//python:defs.bzl", "py_test")
load("//bazel:candid.bzl", "did_git_test")

did_git_test(
    name = "example_did_git_test",
    did = ":example.did",
    tags = ["manual"],  # only executed as part of candid_integration_tests
)

did_git_test(
    name = "example_did_git_test_also_reverse",
    did = ":example.did",
    enable_also_reverse = True,
    tags = ["manual"],  # only executed as part of candid_integration_tests
)

genrule(
    name = "create_new_did_file",
    srcs = [":example.did"],
    outs = ["new.did"],
    cmd_bash = "cat $(location example.did) > $@",
)

did_git_test(
    name = "new_did_git_test",
    did = ":new.did",
    tags = ["manual"],  # only executed as part of candid_integration_tests
)

py_test(
    name = "candid_integration_tests",
    srcs = ["candid_integration_tests.py"],
    data = [
        ":example.did",
        ":example_backup.did",
        ":example_did_git_test",
        ":example_did_git_test_also_reverse",
        ":new_did_git_test",
        "//:WORKSPACE.bazel",
    ],
    env = {
        "DID_FILE_PATH": "$(location :example.did)",
        "BACKUP_DID_FILE_PATH": "$(location :example_backup.did)",
        "TEST_BIN": "$(location :example_did_git_test)",
        "TEST_BIN_ALSO_REVERSE": "$(location :example_did_git_test_also_reverse)",
        "NEW_DID_TEST": "$(location :new_did_git_test)",
        "WORKSPACE": "$(rootpath //:WORKSPACE.bazel)",
    },
    tags = ["local"],
    deps = [requirement("pytest")],
)



================================================
FILE: bazel/candid_integration_tests/candid_integration_tests.py
================================================
#!/usr/bin/env python3

import os
import shutil
import subprocess
import sys

import pytest

workspace_dir = os.path.dirname(os.path.realpath(os.environ["WORKSPACE"]))
if not os.path.isdir(workspace_dir):
    sys.exit("WORKSPACE path '{}' is not directory".format(workspace_dir))
did_file_path = os.path.realpath(os.environ["DID_FILE_PATH"])
backup_did_file_path = os.path.realpath(os.environ["BACKUP_DID_FILE_PATH"])


def modify_file_contents(path, find, replacement):
    with open(path) as f:
        contents = f.read()

    new_contents = contents.replace(find, replacement)
    if new_contents == contents:
        raise ValueError(f"No change: {contents}")

    with open(path, "w") as f:
        f.write(new_contents)


def run_example_did_git_test(test_bin="TEST_BIN"):
    return subprocess.run(
        [os.environ[test_bin]],
        env={
            "BUILD_WORKSPACE_DIRECTORY": workspace_dir,
            "TEST_TMPDIR": os.environ["TEST_TMPDIR"],
        },
        capture_output=True,
    )


@pytest.fixture(autouse=True)
def setup_example_did():
    # setup
    shutil.copyfile(backup_did_file_path, did_file_path)

    # run test
    yield

    # tear down
    shutil.copyfile(backup_did_file_path, did_file_path)


def test_did_check_succeeds():
    res = run_example_did_git_test()

    message = "bazel/candid_integration_tests/example.did passed candid checks"
    assert message in res.stdout.decode("utf-8")
    assert res.returncode == 0


def test_remove_variants_check_fails():
    modify_file_contents(path=did_file_path, find="happy; sad", replacement="happy")

    res = run_example_did_git_test()

    error_message = "Method do_stuff: func (Request) -> () is not a subtype of func (Request/1) -> ()"

    assert error_message in res.stderr.decode("utf-8")
    assert res.returncode == 101


def test_adding_new_did_file_succeeds():
    res = run_example_did_git_test(test_bin="NEW_DID_TEST")

    message = "is a new file, skipping backwards compatibility check"
    assert message in res.stdout.decode("utf-8")
    assert res.returncode == 0


def test_add_variants_succeeds():
    modify_file_contents(path=did_file_path, find="sad", replacement="sad; stunned")

    res = run_example_did_git_test()

    message = "bazel/candid_integration_tests/example.did passed candid checks"
    assert message in res.stdout.decode("utf-8")
    assert res.returncode == 0


def test_remove_required_field_from_input_check_fails():
    modify_file_contents(
        path=did_file_path,
        find="existing_required_request_field : int;",
        replacement="// Blank.",
    )

    res = run_example_did_git_test(test_bin="TEST_BIN_ALSO_REVERSE")

    error_message = "Method dance: func (DanceRequest) -> (DanceResponse) is not a subtype of func (DanceRequest/1) -> (DanceResponse/1)"
    assert error_message in res.stderr.decode("utf-8")
    assert res.returncode == 101
    assert "running also-reverse check" in res.stdout.decode("utf-8")


def test_remove_required_field_from_output_check_fails():
    modify_file_contents(
        path=did_file_path,
        find="existing_required_response_field : int;",
        replacement="// Blank.",
    )

    res = run_example_did_git_test(test_bin="TEST_BIN_ALSO_REVERSE")

    error_message = "Method dance: func (DanceRequest) -> (DanceResponse) is not a subtype of func (DanceRequest/1) -> (DanceResponse/1)"
    assert error_message in res.stderr.decode("utf-8")
    assert res.returncode == 101


def test_adding_a_required_field_to_input_check_fails():
    modify_file_contents(
        path=did_file_path,
        find="// Insert DanceRequest fields here.",
        replacement="new_required_int : int;",
    )

    res = run_example_did_git_test(test_bin="TEST_BIN_ALSO_REVERSE")

    error_message = "Method dance: func (DanceRequest) -> (DanceResponse) is not a subtype of func (DanceRequest/1) -> (DanceResponse/1)"
    assert error_message in res.stderr.decode("utf-8")
    assert res.returncode == 101


def test_adding_optional_field_succeeds():
    modify_file_contents(
        path=did_file_path,
        find="// Insert DanceRequest fields here.",
        replacement="new_optional_int : opt int;",
    )

    res = run_example_did_git_test(test_bin="TEST_BIN_ALSO_REVERSE")

    message = "bazel/candid_integration_tests/example.did passed candid checks"
    assert message in res.stdout.decode("utf-8")
    assert res.returncode == 0


def test_adding_optional_field_reverse_succeeds():
    modify_file_contents(
        path=did_file_path,
        find="// Insert DanceResponse fields here.",
        replacement="new_optional_int : opt int;",
    )

    res = run_example_did_git_test(test_bin="TEST_BIN_ALSO_REVERSE")

    message = "bazel/candid_integration_tests/example.did passed candid checks"
    assert message in res.stdout.decode("utf-8")
    assert res.returncode == 0


if __name__ == "__main__":
    sys.exit(pytest.main([__file__]))



================================================
FILE: bazel/candid_integration_tests/example.did
================================================
// Test data for candid_integration_tests

type Request = record {
  emotion : variant { malaise; happy; sad };
};

type DanceRequest = record {
  existing_required_request_field : int;
  existing_optional_request_field : opt int;
  // Insert DanceRequest fields here.
};

type DanceResponse = record {
  existing_required_response_field : int;
  existing_optional_response_field : opt int;
  // Insert DanceResponse fields here.
};

service : {
  // Comment within service.
  do_stuff : (Request) -> ();
  dance : (DanceRequest) -> (DanceResponse);
}



================================================
FILE: bazel/candid_integration_tests/example_backup.did
================================================
// Test data for candid_integration_tests

type Request = record {
  emotion : variant { malaise; happy; sad };
};

type DanceRequest = record {
  existing_required_request_field : int;
  existing_optional_request_field : opt int;
  // Insert DanceRequest fields here.
};

type DanceResponse = record {
  existing_required_response_field : int;
  existing_optional_response_field : opt int;
  // Insert DanceResponse fields here.
};

service : {
  // Comment within service.
  do_stuff : (Request) -> ();
  dance : (DanceRequest) -> (DanceResponse);
}



================================================
FILE: bazel/conf/README.md
================================================
# Bazel configuration

This directory contains some bazelrc fragments that are used in different scenarios. The fragments
can be used like this:

```
bazel build --noworkspace_rc --bazelrc <fragment> --bazelrc <fragment>
```

The default `.bazelrc` loads all fragments. See the individual fragments for more information.



================================================
FILE: bazel/conf/.bazelrc.build
================================================
# A .bazelrc needing build configuration. Without this configuration, the
# build will most likely fail.

# To require no rustfmt issues, pass --config=fmt.
# To require no clippy issues, pass --config=clippy. Without this, warnings will still be generated.
# To enable both of the above, pass --config=lint.
# --config=lint implies both --config=fmt and --config=clippy.
build:lint --config=fmt
build:lint --config=clippy
# rust-clippy
build:clippy --aspects=@rules_rust//rust:defs.bzl%rust_clippy_aspect
build:clippy --output_groups=+clippy_checks
build --@rules_rust//:clippy.toml=//:clippy.toml --@rules_rust//:clippy_flags=-D,warnings,-D,clippy::all,-D,clippy::mem_forget,-C,debug-assertions=off,-A,clippy::uninlined_format_args
# rustfmt
build:fmt --aspects=@rules_rust//rust:defs.bzl%rustfmt_aspect
build:fmt --output_groups=+rustfmt_checks
build --@rules_rust//:rustfmt.toml=//:rustfmt.toml

# Until the lockfile format has settled, don't use a
# lockfile for MODULE.bazel
common --lockfile_mode=off

# Use hermetic JDK
# See https://bazel.build/docs/bazel-and-java#hermetic-testing
build --java_runtime_version=remotejdk_17

common --experimental_allow_tags_propagation
build --nosandbox_default_allow_network
build --incompatible_strict_action_env # use an environment with a static value for PATH and do not inherit LD_LIBRARY_PATH

# default to optimized and unstripped binaries.
build --compilation_mode=opt
build --@rules_rust//:extra_rustc_flags=-Cdebug-assertions=on
build --@rules_rust//:extra_rustc_flag=-Dbindings_with_variant_name
build --strip=never

# Build everything ic-os without sandbox
build --strategy_regexp=ic-os[:/].*=local

build --workspace_status_command='$(pwd)/bazel/workspace_status.sh'

build --experimental_repository_downloader_retries=3 # https://bazel.build/reference/command-line-reference#flag--experimental_repository_downloader_retries

common --flag_alias=release_build=//bazel:release_build
common --flag_alias=s3_endpoint=//ci/src/artifacts:s3_endpoint
common --flag_alias=timeout_value=//bazel:timeout_value
common --flag_alias=hermetic_cc=//bazel:hermetic_cc

common:stamped --workspace_status_command='$(pwd)/bazel/workspace_status.sh --stamp'

# configure some tests to retry automatically, best used unattened (not locally).
# default all tests to fail ...
#   ... after three attempts for tests marked as flaky
#   ... after three attempts for all tests in //rs/tests
#   ... after the first attempt for other tests
#   see also:
#     https://bazel.build/reference/command-line-reference#build-flag--flaky_test_attempts
#
# (NOTE: for convenience, applied to 'common' instead of just 'test' so that it doesn't
# fail on 'bazel build')
common:flaky_retry --flaky_test_attempts=default --flaky_test_attempts=//rs/tests/.*@3

# Exclude fuzz tests by default
# https://github.com/bazelbuild/bazel/issues/8439
build --build_tag_filters="-fuzz_test"
test --test_tag_filters="-fuzz_test"

test --test_output=errors

test:precommit --build_tests_only --test_tag_filters="smoke"

build:testnet --build_tag_filters=
test:testnet --test_output=streamed --test_tag_filters=

# Set all tests (including those marked as flaky) to fail explicitly on the
# first try (can be overriden). This is useful when developing locally to
# spot flakiness.
#
# see also:
#   https://bazel.build/reference/command-line-reference#flag--flaky_test_attempts
test --flaky_test_attempts=1

# So that developers can build in debug mode.
build:dev --compilation_mode=fastbuild

# Fuzzing configuration
# fuzzers use nightly features so we tell rustc to allow nightly features
build:fuzzing --@rules_rust//rust/settings:extra_rustc_env=RUSTC_BOOTSTRAP=1 --@rules_rust//rust/settings:extra_exec_rustc_env=RUSTC_BOOTSTRAP=1
build:fuzzing --build_tag_filters=fuzz_test
# Ignoring transitions for now since it doesn't add any additional improvement to current setup
build:fuzzing --//bazel:enable_fuzzing_code=True

# AFL configuration
build:afl --action_env="AR=llvm-ar-18"
build:afl --action_env="AS=llvm-as-18"
build:afl --action_env="CC=afl-clang-lto"
build:afl --action_env="CXX=afl-clang-lto++"
build:afl --action_env="LD=afl-clang-lto++"
build:afl --action_env="LLVM_CONFIG=llvm-config-18"
build:afl --action_env="RANLIB=llvm-ranlib-18"
build:afl --config=fuzzing
# Note: Instrumenting with AFL is done by overriding the above variables, but
# these are not respected by the hermetic toolchains. Instead, we use
# toolchains from the host system.
# For more context see: https://github.com/dfinity/ic/pull/3508
build:afl --hermetic_cc=false
build:afl --build_tag_filters=afl
run:afl --run_under="//bin/fuzzing:afl_wrapper"

# Fuzzing w/ Canister Sandbox configuration
# NOTE: This is only for --config=fuzzing
# AFL handles this differently in afl_wrapper.sh
build:sandbox_fuzzing --config=fuzzing
run:sandbox_fuzzing --run_under="ASAN_OPTIONS=detect_leaks=0:allow_user_segv_handler=1:handle_segv=1:handle_sigfpe=1:handle_sigill=0:quarantine_size_mb=16 LSAN_OPTIONS=handle_sigill=0 RUST_MIN_STACK=8192000"

# Suppress all additional output to make it more convenient in scripts
query --ui_event_filters=-info,-debug --noshow_progress
cquery --ui_event_filters=-info,-debug --noshow_progress

# This is disabled by default on bazel 7+ some of our targets choke
# on this (not yet clear why)
common --remote_download_all

# This option (in conjunction with remote cache issues) creates build failures
#   https://github.com/bazelbuild/bazel/issues/22387
common --noexperimental_inmemory_dotd_files

# This is particularly helpful for canbench, but other tests that follow this
# convention would also benefit. If the test does not support this, this "almost
# certainly" does no harm.
common --test_env=CLICOLOR_FORCE=true

# Show full backtrack on failure
common --test_env=RUST_BACKTRACE=full

# Give canceled actions some more time to cleanup
common --local_termination_grace_seconds=90

# Speed up compilation with zig cc
# zig cc compiles the standard library on demand, so a shared cache makes a big
# difference by cutting out duplicated work.

# Set the cache path for all environments
build --repo_env=HERMETIC_CC_TOOLCHAIN_CACHE_PREFIX=/tmp/zig-cache
# Share /tmp/zig-cache across targets
build --sandbox_add_mount_pair=/tmp/zig-cache
# Allow writes to the shared cache
build --sandbox_writable_path=/tmp/zig-cache



================================================
FILE: bazel/conf/.bazelrc.internal
================================================
# A .bazelrc used internally by DFINITY for metrics, cache, etc

# Build event upload configuration
build:bes --bes_results_url=https://dash.idx.dfinity.network/invocation/
build:bes --bes_backend=bes.idx.dfinity.network
build:bes --bes_timeout=180s # Default is no timeout.
build:bes --remote_build_event_upload=minimal

# DFINITY internal remote cache setup
build --remote_cache=bazel-remote.idx.dfinity.network
build --experimental_remote_cache_async
build --experimental_remote_cache_compression # If enabled, compress/decompress cache blobs with zstd.
# TODO: re-enable after fixing the error like this:
# `Failed to fetch file with hash 'xxx' because it does not exist remotely. --remote_download_outputs=minimal does not work if your remote cache evicts files during builds.`
# Probably disabling `--experimental_remote_cache_async` will help
#build --remote_download_minimal # https://bazel.build/reference/command-line-reference#flag--remote_download_minimal
#build --remote_download_outputs=toplevel # Still download outputs from top level targets.

build --experimental_remote_downloader=bazel-remote.idx.dfinity.network --experimental_remote_downloader_local_fallback
build:local --experimental_remote_downloader=
build --remote_local_fallback
build    --remote_upload_local_results=false

# Run `bazel build ... --config=local` to build targets without cache
build:local --remote_cache=



================================================
FILE: bazel/inject_version_into_wasm_tests/BUILD.bazel
================================================
load("@rules_python//python:defs.bzl", "py_test")
load("//bazel:canisters.bzl", "finalize_wasm")

# Test subject
finalize_wasm(
    name = "stamped_trivial",
    testonly = True,
    service_file = "service.did",
    src_wasm = "trivial.wasm",
    version_file = "//bazel:version.txt",
)

# Inspects test subject.
py_test(
    name = "inspect_stamped_trivial_wasm",
    srcs = ["inspect_stamped_trivial_wasm.py"],
    data = [
        ":stamped_trivial",
        "//bazel:version.txt",
        "@crate_index//:ic-wasm__ic-wasm",
    ],
    env = {
        "IC_WASM_PATH": "$(location @crate_index//:ic-wasm__ic-wasm)",
        "STAMPED_WASM_PATH": "$(location :stamped_trivial)",
        "VERSION_TXT_PATH": "$(location //bazel:version.txt)",
    },
)



================================================
FILE: bazel/inject_version_into_wasm_tests/inspect_stamped_trivial_wasm.py
================================================
"""
Test(s) for the inject_version_into_wasm function defined in ../canisters.bzl.

Requires the following environment varianbles to be set:

  1. IC_WASM_PATH: Path to the ic-wasm tool, which can be installed via
     `cargo install ic-wasm`.
  2. STAMPED_WASM_PATH: Path to .wasm file generated by a inject_version_into_wasm
     rule. Such a wasm is supposed to have a WASM custom section named
     'icp:public git_commit_id'.
  3. VERSION_TXT_PATH: The contents of this file (with whitespace
     stripped from both ends) contains the value that's supposed to be
     associated with the aforementioned WASM custom section.
"""

import os
import subprocess
import unittest


class InjectVersionIntoWasmTest(unittest.TestCase):
    def test_wasm_custom_section_(self):
        # See the module docstring for a description of these inputs.
        ic_wasm_path = os.getenv("IC_WASM_PATH")
        stamped_wasm_path = os.getenv("STAMPED_WASM_PATH")
        version_txt_path = os.getenv("VERSION_TXT_PATH")

        with open(version_txt_path) as f:
            expected_git_commit_id = f.read().strip()

        list_metadata_process = subprocess.run(
            [ic_wasm_path, stamped_wasm_path, "metadata"],
            capture_output=True,
            check=True,
            text=True,
        )
        self.assertIn("icp:public git_commit_id", list_metadata_process.stdout, f"{repr(list_metadata_process)}")

        show_git_commit_id_process = subprocess.run(
            [ic_wasm_path, stamped_wasm_path, "metadata", "git_commit_id"],
            capture_output=True,
            check=True,
            text=True,
        )
        self.assertIn(expected_git_commit_id, show_git_commit_id_process.stdout, f"{repr(show_git_commit_id_process)}")


if __name__ == "__main__":
    unittest.main()



================================================
FILE: bazel/inject_version_into_wasm_tests/service.did
================================================
[Empty file]


================================================
FILE: bazel/inject_version_into_wasm_tests/trivial.wasm
================================================
 asm   


================================================
FILE: bazel/tlaplus/BUILD.bazel
================================================
# See https://lamport.azurewebsites.net/tla/tools.html for the
# description of the TLA+ command-line tools.

load("@rules_java//java:java_binary.bzl", "java_binary")

COMMON_DEPS = [
    # Keep sorted.
    "//third_party/tlaplus-1.8.0:tla2tools",
    "@tlaplus_community_modules//jar",
    "@tlaplus_community_modules_deps//jar",
]

java_binary(
    name = "sany",
    main_class = "tla2sany.SANY",
    visibility = ["//visibility:public"],
    runtime_deps = COMMON_DEPS,
)

java_binary(
    name = "tlc",
    main_class = "tlc2.TLC",
    visibility = ["//visibility:public"],
    runtime_deps = COMMON_DEPS,
)

java_binary(
    name = "tla2latex",
    main_class = "tla2tex.TLA",
    visibility = ["//visibility:public"],
    runtime_deps = COMMON_DEPS,
)



================================================
FILE: bazel/tlaplus/defs.bzl
================================================
"""
This module defines rules to check TLA+ specifications.
"""

TLA_FILE_TYPES = [".tla"]

TlaModuleInfo = provider(
    doc = "Provides information about a TLA+ module.",
    fields = {
        "src": "File: the module path.",
        "deps": "depset: transitive dependencies of this module.",
    },
)

def _tla_module_impl(ctx):
    src = ctx.file.src

    tla_deps = depset(
        direct = [src],
        transitive = [dep[TlaModuleInfo].deps for dep in ctx.attr.deps if TlaModuleInfo in dep],
    )

    all_runfiles = []
    for dep in ctx.attr.deps:
        all_runfiles.append(dep[DefaultInfo].default_runfiles)
    runfiles = ctx.runfiles(files = tla_deps.to_list()).merge_all(all_runfiles)

    return [
        DefaultInfo(runfiles = runfiles),
        TlaModuleInfo(src = src, deps = tla_deps),
    ]

tla_module = rule(
    implementation = _tla_module_impl,
    attrs = {
        "src": attr.label(allow_single_file = TLA_FILE_TYPES),
        
